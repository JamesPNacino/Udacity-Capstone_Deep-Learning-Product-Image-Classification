{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Capstone Project: Product Image Classification\n",
    "\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this udacity course, I have been highly fascinated with image classification. Luckily for me I have found a project, hosted on Kaggle.com in partnership with Cdiscount, in which the goal is to classify images.\n",
    "\n",
    "The company Cdiscount, is the largest non-food e-commerce company in France with over 30 million different products for sale. They want to be able to classify their product images to predict its respective product category. This is similar to how Amazon has a bunch of different categories for each of their products.\n",
    "\n",
    "Currently Cdiscount classifies their products by analyzing the description of the products that the seller describes. Through a text classification algorithm, Cdiscount is able to classify the product into its respective product category (with some accuracy). However, now they want to go even further beyond, to improve their product classification accuracy. In order to do this, they are looking into image classification. Cdiscount wants to read the picture that the seller inputs to automatically classify the product into its correct category. Having the correct category for the product will make the experience easier for Cdiscount customers to find products from various sellers, and would make it easier for the sellers to acknowledge the correct category for the product they are selling.\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Cdiscount provided no ordinary image classification problem. I have been given a little over seven million images from Cdiscount which I will use to train my model, and they want to be able to classify their products correctly into one of 5270 different categories. In this problem, I will implement a convolutional neural network to classify the image's categories.\n",
    "\n",
    "### Solution 1 - The Solution that I backed away from, and my original thought to solving this problem\n",
    "\n",
    "When I first started this problem, my goal was to implement a solution that would break this problem up into different sets of neural networks. I am going to explain my first thought process when I first went out to solve this problem.\n",
    "\n",
    "I felt that 5270 different classes is a lot of labels to correctly predict which the image is associated with, which is why I thought of breaking this problem into mutliple sets of neural networks. Cdiscount also provided a file which nests each of the 5270 categories into more general categories. So as a concrete example, a backpack and a purse would be in the same general category as provided by Cdiscount, but their specific category would be different. I was going to create a convolutional neural network which predicted the general category of the product. Then after running the products through my first neural network, I wanted to create many neural networks that are specific to the general categories. \n",
    "\n",
    "So my machine learning pipeline would've looked like this:\n",
    "\n",
    "1. Create and train a convolutional neural network which predicts the product's general category. There are 49 different general categories, so 49 different output nodes\n",
    "2. Run each product through this neural network to predict its general category\n",
    "3. Create and train new convolutional neural networks with products that have been predicted with a general category from the first neural network. This means that I will have 49 different neural networks that are specifically trained with products that within a general category. Each of these neural networks will have around 100 different output nodes. The logic for this is because 49 x 100 equals around 5000.\n",
    "\n",
    "I decided not to follow up with this solution for many reasons, and will further explain why I ditched this solution later on when I test this out on a neural network in the solution statement section:\n",
    "\n",
    "1. It was much more complicated to maintain and compile several neural networks and to train each one indiviually.\n",
    "2. It would have taken longer to run this algorithm on the test data, which again was to predict the correct class of product using the first neural network, then the second neural network \n",
    "3. The accuracy of the highest validation was around 55%, which is not as high as I would've hoped. This is because it still has to go through a second neural network, which will for sure bring the accuracy down when testing the final prediction\n",
    "\n",
    "### Solution 2 - Revised and implemented solution\n",
    "\n",
    "This solution maintains the use of one convolutional neural network, but predicts the class of a product into one of 5270 specific product categories. The benefits of this is as follows:\n",
    "\n",
    "1. It is much easier to maintain and implement one convolutional neural network \n",
    "2. Testing the accuracy on the test dataset will be faster\n",
    "3. Due to only having one convolutional neural network, it will be easier to add new layers, nodes, or a new structure in general to validate different results\n",
    "4. Transfer learning with one CNN is more efficient\n",
    "\n",
    "\n",
    "### Various Miscellaneous Problems That Will Be Addressed Later in this Report\n",
    "\n",
    "1. Storing and pre-processing the data\n",
    "2. Memory and Storage Issues\n",
    "2. Setting up Google Cloud Computing\n",
    "3. Setting up CUDA GPU Computing\n",
    "\n",
    "### Goals for this project\n",
    "\n",
    "Each Kaggle competition has a leaderboard. My goal entering my first Kaggle competition is to be in the top 50th percentile. This means that I want to achieve an accuracy score on the test dataset that is higher than the median participant's accuracy score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets and Inputs\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "In order to process the data, Cdiscount provided a '.bson' file which contained all of the image data. This file is a mongoDB database data extract. It takes the form of a JSON-like structure. With this '.bson' file format, it is not that easy to access images. In the Kaggle competition, they gave us starter code to read the '.bson' file. I have edited the code to store the data in the format that I want, however, using this code is not what I initially wanted to do.\n",
    "\n",
    "I initially tried many things to process the data to where it could be properly loaded into a neural network. I first tried to convert the 'train.bson' file using a mongoDB tool called 'bsondump' which was to convert the '.bson' file into a '.json' file. I did this because there were various Python modules where you can read in JSON files pretty easily. However, it turned out that the converted JSON file was in an incorrect file format for Python, and R programming languages to properly read in.\n",
    "\n",
    "So I resulted to the code that they gave to read in a '.bson' file. However, the starter code that they gave us had no information on how to store the images. I wanted to store the images into an easily accessible file type that can store vasts amounts of data. I researched various file formats and saw that the 'H5' file format was able to store python arrays, and was able to handle vasts amount of data efficiently.\n",
    "\n",
    "Below is the code for processing the data into '.H5' files. The result was 140 different H5 files which stored the image arrays. Each of these H5 files contained 50000 images. I used separate H5 files as well to story the productID as well as the categoryID, however, these files did not nearly need as much memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the required packages for proccessing and storing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import h5py # Used to store images and data\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "\n",
    "\n",
    "import io\n",
    "import bson                       # this is installed with the pymongo package\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.data import imread   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the training dataset\n",
    "\n",
    "I added a lot of things to the starter code that they gave us to process the BSON files. When I first started I ran into a problem. The problem was that the image arrays took a very long time to be appended together. The work around was to use pandas dataframes. I would temporarily use the pandas dataframes to append each image array in each cell under one column; this turned out to be a lot faster then appending two numpy arrays together. Then I converted the pandas column into a numpy array of image data. Each iteration of the loop contains 50,000 images and its respective category which is saved in H5 files.\n",
    "\n",
    "The H5 files were too large to fit into my computer's storage, in total the images consumed around 800 GB. To solve this, I had to buy an external hard drive which I could store all the image data into. I then had to figure how to connect my iPython notebook to an external storage device which I was able to do with Windows symbolic links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Simple data processing\n",
    "from bson.json_util import dumps\n",
    "\n",
    "# Read in the BSON file\n",
    "data = bson.decode_file_iter(open('train.bson', 'rb'))\n",
    "\n",
    "prod_to_category = dict()\n",
    "\n",
    "# Initialize counters for the for loop to process and store image data\n",
    "i = 0\n",
    "j = 200000\n",
    "k = 0\n",
    "l = 0\n",
    "dat = 1\n",
    "\n",
    "for c, d in enumerate(data):\n",
    "    product_id = d['_id']\n",
    "    category_id = d['category_id'] # This won't be in Test data\n",
    "    prod_to_category[product_id] = category_id\n",
    "    i+=1\n",
    "    k+=1\n",
    "    l+=1\n",
    "    \n",
    "    #  print out the number of iterations the loop has went through\n",
    "    if (i == j):\n",
    "        print (i, \"records loaded\")\n",
    "        print(picture_1.shape)\n",
    "        j+=200000\n",
    "        \n",
    "        \n",
    "    for e, pic in enumerate(d['imgs']):\n",
    "        \n",
    "        # It was much faster to append a single numpy picture array into a new cell in a pandas dataframe.\n",
    "        # I initially tried to append a single numpy picture array into an existing numpy array with all the\n",
    "        # previous image data, but this turned out to take a very long time\n",
    "        if (l == 1):\n",
    "            picture_1 = np.reshape(imread(io.BytesIO(pic['picture'])), (1,180,180,3))\n",
    "            get = pd.DataFrame({'A': [product_id], 'B': [category_id], 'C':[picture_1]})\n",
    "            frames = get\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            picture_2 = np.reshape(imread(io.BytesIO(pic['picture'])), (1,180,180,3))\n",
    "            get2 = pd.DataFrame({'A': [product_id], 'B': [category_id], 'C':[picture_2]})\n",
    "            frames = frames.append(get2)\n",
    "            break\n",
    "            \n",
    "    # Once 'l' is equal to 50000, then convert the pandas columns into numpy arrays and write the data to an H5 file. \n",
    "    # This proved to be the fastest method to store the images and write them onto a file type that I can access later\n",
    "    if (l == 50000):\n",
    "        c = np.array(frames.C.tolist())\n",
    "        c = c.reshape(l,180,180,3)\n",
    "        #c = np.array(frames.C)\n",
    "        b = np.array(frames.B)\n",
    "        a = np.array(frames.A)\n",
    "        \n",
    "\n",
    "        h5f = h5py.File('Product_ID' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=a)\n",
    "        \n",
    "        h5f = h5py.File('Category_ID' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=b)\n",
    "        \n",
    "        h5f = h5py.File('Product_Image' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=c)\n",
    "        \n",
    "        h5f.close()            \n",
    "        \n",
    "        # Have to make sure that you set these arrays back to its original value or the arrays will get too big and the computer\n",
    "        # will get slow and run out of memory. These variables will be appended with new values in the next iterations of the loop\n",
    "        # until 'l' equals 50000 then these arrarys reset.\n",
    "        a = None\n",
    "        b = None\n",
    "        c = None\n",
    "        \n",
    "        l = 0\n",
    "        dat += 1\n",
    "        \n",
    "        # My computer auto-updated istelf before it finished this last itertion. 'k' was equal to the total number of pictures\n",
    "        # in the dataset. I only was able to write 7000000 pictures onto H5 files. But this should be fine for the classification\n",
    "        # problem\n",
    "    elif (k == 7069894):\n",
    "        c = np.array(frames.C.tolist())\n",
    "        c = c.reshape(i,180,180,3)\n",
    "        #c = np.array(frames.C)\n",
    "        b = np.array(frames.B)\n",
    "        a = np.array(frames.A)\n",
    "        \n",
    "        h5f = h5py.File('Product_ID' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=a)\n",
    "        \n",
    "        h5f = h5py.File('Category_ID' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=b)\n",
    "        \n",
    "        h5f = h5py.File('Product_Image' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=c)\n",
    "        \n",
    "        h5f.close()\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing the testing dataset\n",
    "\n",
    "The test dataset for this problem consisted of 1,768,182 images. I did the same process here as in processing the training dataset by storing 50,000 images and the categoryId into multiple H5 files, with the last H5 file containing less than 50,000 images to evenly add up to the total amount of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200000 records loaded\n",
      "(1, 180, 180, 3)\n",
      "400000 records loaded\n",
      "(1, 180, 180, 3)\n",
      "600000 records loaded\n",
      "(1, 180, 180, 3)\n",
      "800000 records loaded\n",
      "(1, 180, 180, 3)\n",
      "1000000 records loaded\n",
      "(1, 180, 180, 3)\n",
      "1200000 records loaded\n",
      "(1, 180, 180, 3)\n",
      "1400000 records loaded\n",
      "(1, 180, 180, 3)\n",
      "1600000 records loaded\n",
      "(1, 180, 180, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read in the BSON file\n",
    "data = bson.decode_file_iter(open('test.bson', 'rb'))\n",
    "\n",
    "prod_to_category = dict()\n",
    "\n",
    "# Initialize counters for the for loop to process and store image data\n",
    "i = 0\n",
    "j = 200000\n",
    "k = 0\n",
    "l = 0\n",
    "dat = 1\n",
    "\n",
    "for c, d in enumerate(data):\n",
    "    product_id = d['_id']\n",
    "    i+=1\n",
    "    k+=1\n",
    "    l+=1\n",
    "    \n",
    "    #  print out the number of iterations the loop has went through\n",
    "    if (i == j):\n",
    "        print (i, \"records loaded\")\n",
    "        print(picture_1.shape)\n",
    "        j+=200000\n",
    "        \n",
    "        \n",
    "    for e, pic in enumerate(d['imgs']):\n",
    "        \n",
    "        # It was much faster to append a single numpy picture array into a new cell in a pandas dataframe.\n",
    "        # I initially tried to append a single numpy picture array into an existing numpy array with all the\n",
    "        # previous image data, but this turned out to take a very long time\n",
    "        if (l == 1):\n",
    "            picture_1 = np.reshape(imread(io.BytesIO(pic['picture'])), (1,180,180,3))\n",
    "            get = pd.DataFrame({'A': [product_id], 'C':[picture_1]})\n",
    "            frames = get\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            picture_2 = np.reshape(imread(io.BytesIO(pic['picture'])), (1,180,180,3))\n",
    "            get2 = pd.DataFrame({'A': [product_id], 'C':[picture_2]})\n",
    "            frames = frames.append(get2)\n",
    "            break\n",
    "            \n",
    "    # Once 'l' is equal to 50000, then convert the pandas columns into numpy arrays and write the data to an H5 file. \n",
    "    # This proved to be the fastest method to store the images and write them onto a file type that I can access later\n",
    "    if (l == 50000):\n",
    "        c = np.array(frames.C.tolist())\n",
    "        c = c.reshape(l,180,180,3)\n",
    "        #c = np.array(frames.C)\n",
    "        a = np.array(frames.A)\n",
    "        \n",
    "\n",
    "        h5f = h5py.File('./test/Product_IDtest' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=a)\n",
    "        \n",
    "        h5f = h5py.File('./test/Product_Imagetest' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=c)\n",
    "        \n",
    "        h5f.close()            \n",
    "        \n",
    "        # Have to make sure that you set these arrays back to its original value or the arrays will get too big and the computer\n",
    "        # will get slow and run out of memory. These variables will be appended with new values in the next iterations of the loop\n",
    "        # until 'l' equals 50000 then these arrarys reset.\n",
    "        a = None\n",
    "        c = None\n",
    "        \n",
    "        l = 0\n",
    "        dat += 1\n",
    "        \n",
    "        # My computer auto-updated istelf before it finished this last itertion. 'k' was equal to the total number of pictures\n",
    "        # in the dataset. I only was able to write 7000000 pictures onto H5 files. But this should be fine for the classification\n",
    "        # problem\n",
    "    elif (k == 1768182):\n",
    "        c = np.array(frames.C.tolist())\n",
    "        c = c.reshape(len(frames),180,180,3)\n",
    "        #c = np.array(frames.C)\n",
    "        a = np.array(frames.A)\n",
    "        \n",
    "        h5f = h5py.File('./test/Product_IDtest' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=a)\n",
    "        \n",
    "        h5f = h5py.File('./test/Product_Imagetest' + str(dat) + '.h5', 'w')\n",
    "        h5f.create_dataset('dataset', data=c)\n",
    "        \n",
    "        h5f.close()\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization - Pictures\n",
    "\n",
    "Below are examples of the pictures that Cdiscount has provided for us. There are many different types of products, ranging from phone cases to batteries to CD albums to many more thousands of different objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzsvXmQZMl93/fJzHfVXd3Vx/T03Nfu\nYncB7IHFSexiQYgEQZMSRdGmaEpyMIL+w1bYYTtMyX/5DzsshR22FA6FJYRNmZZJgXLQDPEAuQRx\nLA4C2At7z+7M7NzdPX1Vd53vzEz/ka+6Zxa7BKDdNWYY9Z3oqKmsrHz5sl7+8nf/hLWWKaaYYooJ\n5I97AlNMMcXthSlRmGKKKW7BlChMMcUUt2BKFKaYYopbMCUKU0wxxS2YEoUpppjiFrxnREEI8dNC\niNeFEBeEEP/gvbrOFFNM8e5CvBd+CkIIBZwDPgNcB54Gftla++q7frEpppjiXcV7xSk8Alyw1l60\n1mbAF4Cff4+uNcUUU7yL8N6jcZeBaze9vw58+O06z83N2WPHjt3S9lb8i3jLb9u3efvWvX/AYLcO\nZ+1+R2Fv+o7FAhaBuGmgt7+i610gUHt9BRYwOMos7N4H6HIS6k33Y0XZz5RNk8H4/vW6ZS72rT94\nm+a3mf/kf/Yt+1r2b0BMVVW3JZ599tkta+38D+r3XhGFt35ubu4gxK8Dvw5w5MgRnnnqmb1vaQH6\npi+om16FnXxg9geTZu8C+aQJiShfjSn3dzm2kW78m0ZAAh4WD4Nnzf6MM9/NyzOgdNlsyu9LJApV\nbgJhbpq0AERR/t/1joFKVt5N7kEgSXyI9E0TVzDwDTmaBhpfu/5WCWI8qhaIy74VSiICN9OKyVop\ngKx4040KkFB4ElMuuAGit6UqBovG3DKIe2fRN62f3COQigpT3H4QQlz5Yfq9V0ThOnD4pveHgNWb\nO1hrPw98HuDhhx92x89NpESy/yyLN73eQnKE623LBzXd25UaNTmP5f4DO9m3N+/fSbsuJ2GE6ysF\neFZjhUEbU5IC0HKyRSTSGIT0ARxxuGVz2b1Xi6GiPcjKU7QAFESqvPgtp7/EYNy9WEctEiQxgkxA\nS01uXYEFVRKGm9duD553K1FwlJLJZd+8nLeuyORzgSpJ84SHcb+Pf8vVbPlPMcWdjPeKz3saOC2E\nOC6ECID/APiD9+haU0wxxbuI94RTsNYWQoj/FHgCdyj9prX2lR/4vTe9l+wdbG+BSasp2WeJxeKT\nlp9KsALP/Y+bBXBr3dffLD7sqwz2x84qjhF3IsNkDFvOSyCV3JuLFSBu4WImb5QbXYW36AFQUEjw\n7E23IyaMg0TiIb2bRSODBHToxvXGGXgSqyRWWEy5gtpalJBIsc8h3XLzxmLkvh5AC9A6RwiBmMzZ\ngrXleyFwBiW3RpPf6ebfxQDWgNbg+0xxB+O9Eh+w1n4R+OIP3f9NosPNEG/xZp+AyP3NAES3fFu4\nga11TyyAEYjCgBGoWwY2b3p1GNZubjd7U5A4UiGAoijKmSikpdxIBj1RZAiDsZbYg1a54h4wEjDE\nqQaa5WWy3JB4BboUfdQew25QZEjsvlLSa0JJiDzB3v3sKS8nCzW5T4vrLN5876DUm7a6AGFvWsvy\nY2vdny7nK+X+qxROWpnizsZt8RO+WUl2y6n9Nv0n0ADWaeYlgjhxnIIxTimplEeggv0TUxuMtsjo\nzcfZ/mbYg4Bobz5y7yMFqH11wfdTMQsIif+msXaASnlZ4e8rCIUCyvZASFoEGKCiDWinU6gFFovG\nI0UlpQZBN/fnrICgvBOxv3ltdhNzMqFm5XE/YQokYHKBVGr/XjQYDYUxSClREz2GdN+Tyq2xLnWp\nRk4Jwl8V3DY/o+ZWznpfi38ruZg0T1plSVGsdUQg8Jv73c3EnChuMWVIAYy5lbqIN/2VqE40kpZ9\nbd6bqViRlZMpbYayFCmM3O9vQTU1MimnYQQqtPiBIMwEJPsXjWpg0hQGo/0LdTzq9dJmUjjCR+Lm\nlFswHsioHMCHwoDJIPQcSw9gtdvIBjc1M1lwCaQGT6lbNra2IIREeiXhAna2M6IooFJxtzkhLFo7\nZky+mUBOccfhtiEKN2N/e5hb2F9nYSjNf/ucubP4TY7db58HYBzH9IcDhkmM1hZbEgirNTa3NCt1\nR1DK8YwALaR7LR9sLUB2d1FGoozEKze5p4W7vtAYWRA13G4slMFIgRESrEIad3RbIxFWctVu0E/d\nPAJhGVdgHEpqRtJJS72EVGStiN3RDsEoZrbpWIjamQ7ieJ3A06jRGIDf+ZdPEecZI5NjQw9Zc/MQ\nvkdW5ORpRrNWx2SO20jjhDRNyXWBlQKryrVUkkAbvDAgDEMAlFIIYVFKEUQ+QegelZmZNvNLHY4e\nPcz8Qg1RrpXn79POH+AhMsVtjildn2KKKW7Bbckp7OFNx442GiGdNWE8dHx4LYpAw+7rK7z2yms8\n8PUbZd8CaTQRBuOVWjBASR9PSOphTi2qs7O1BcBur8/M/ByVVotR7tjzQliK7jae9AkJ8LVbLs9I\nPM9DRgoqASROsO7Fu2RGM+gNsFoRyBoAg+6YRrXF3R1FMHCeR9LkxDM+ulOh7oUEIzdGmuVsRz0O\ntGsk3V0unXsdgEce+DlyMhIzpuY5caW7MmSYxhD5eM2ALHbt1tdYT5KnkI9jilLPkoxjTKERSiKU\npCh9IAqtCaSiVgvIfSeuaJ0DBj8KEDImy928DZdZWb3MBx94Hx984F4+9ugjrt3kSAVFnuP51Xf8\n00/x48NtRRT22BbLvrVgz7bg3H9L4x61SilAZzB89TprL5+nYxRhsARAOOzTyEfYeoRo1Z16HkiS\nhDzJ2b56g53CYHIncIdBxM7VG7yy/QqNzgwA7//wIzAuILPYxCKKiTIydEK5tVAkrF+97uYXwvnL\nF5idWWD92jZ65PrPRh38SCO6migoN8zCLIEnyXsj/MADv+HmoTIOLixACL3emK2SECEE1pdkIsPq\nEQBNbwZ0BJ6PEBHWunsprETgITxNliSY3E6WEoFBWglaTJYEIS0zjc6eyACgPI1U4CkFIkMETqmQ\nF2MeeuAhwqrAWsu3v/lNAD76Ex8GCjzvZjeqKe5E3DZE4S3lGGH27PYAQpbTde6HDjfGrL/8BsXV\nLkcOHoaFUgWvUjbHCXKmQvPoPKZU+6+sr7B2fZOZuzrMVlu0Km6TKukxiEeI69e5OnKbbnvzHA9X\n6+wOe3RvdMl67iT2RUgQhRhfk3gxO3kXgMXji9z1yYfoNDtk332Vhx76jJtLGvD6l/+CC93LNHJ3\nNzODNfptyQ07ohVVOdM+BIBXa8PMjNMKCo9cT5QnFuNDLi1FXlokvDrWeBjPQ0uftDz5nVu3h0CR\nJmPkRElifefhaQVSSKR0Gz1QkihsUBQFprQ1SukjtCUtMhAglVu/5cUjLB9doDe4wW63x07PcWZ3\n3X2C2U5rqmn8K4DbhigIbgoKuskqYG96W5ATEGJyjRyUpGKzj7nRQ60PGe5c59LqJgCxNHRVxtjr\n0opz5g8fcdc5fBjv7hbzh45Q9QIoOYU0LaiEEfd4H2C7P3BD73bJezH5Voiejyj6pZtzIRgLQ2zG\nDGzC537pVwDwD83CTIs//sf/lI3+Jg996C43x+9dpecZXg9STrRm3RidJno+oFANLlxaY2PlHACH\nD55kubuF9X16cUGcT8yPFisFQoHw3IpEIiI1mlxLrPHwS1NAYQ1FZhmnKboQe1YYa0FqixACT/r4\nvlMqBkHgLBPFvrwmpSTLU8bjGIQmrDoCcuTwSVZXLnHy9BG+8vUnePzTHwfgz5/4Gr/0K7/kTKhT\nP+c7GlOyPsUUU9yC24JTEJTOQG8RqWduisXLTYEnQzwUE2+aYrtP/41VxPU+3eQGZr4OQLPdxEQw\nVEP6KqXVduzvzKEOgWnz+sp1inFCWB5r7eYMzRqEtTqNBXea+0WLpL9FXS8zo0OCotRj5IpxMqI7\n2GI7W+dfvfinANzv3cWDC/cjTjb49EcegXQDgG89/RV6WY/sgQO8/9NOpGicOgZtCYFm9TvPc/5P\nnwFgddxj88I5OrPzkGZO/gfQ4GlNoPZ9BkKh8AzkuUZaUCXrLoQly1KGwyGB8vZCsfc8qYXA8zwi\n34laYRCS6hjlgaeCvT5ZZimKwpkdSxHk+tU1KrU6L790nvvufoCnn3oRgMUDs1w+t8KxU0d/tB9/\nitsOtwVRAG6VE+AWd+ZJsy9DClsQaLXXYevSCluvXeH9zUMszc/AiQX3PZWzGRYcWDpE9X3L5Asu\nnPfF86/w1Pee5qXvPUcFQTNyOoWZeptjJ0/w0CMfpnPwAAC9Xh/REQRWULEhUento0yA1YqqVUgZ\nIs+4ZXzltefwrsKnf/5TqNWCL/yP/ysAlW2PjjfLwoMP0FhyYzPs0e1uMXvXEQ5+4mN0uu5+nvyD\nbzAYbdJSDRomoG5LQmQ8/CInVAZZ6hl8AVI4V2hrDaJUzlpdoHXu4hk8VXp4gbAW5QkCzyPwFd7E\n11kYrB6jlMKWzmJFYcnyGGs0WEFRuPbXzp7n4PIiCwcWWL1+A506InLulat8/MOPOX3PVHy4o3H7\nEIW3wc06RZiQAgFj95BuXV0hyAxLx06DrMHOCgDdpEd+oMbBxcOo+SbfWXkNgN//4u/xpa99idGg\nR1X6kDjTZn+7x6FDh/g76a/xs7/w1wGYWayzobpkWUEcZ6jUOQ35JkBKgfQLjJdx4swyAKtrL6HT\nXcKZKlxeJxoPAThaWaZtI44tnuDGk98F4PIbr3Il3WTp3uN88jN/g3D2oBu7O6bSqNLKFA0NaVHu\nsEKAVoRG7BtmbIEvLFpBanKSwt1LbHOM1Xi+wFLsJZOQShJ6PpUoxPO8vZUt8gKhMoyV5GN3j0mS\nkeYGIRRSKtLUKTGHw5i4XdDbGROPDWnmxs4zD1OEkLHnsj3FnYnbiyjcnKDkLTC2CQ1RdbM2rnMR\npyw0O9BowyuX4LTjCFpNRef0QTh8gPMb1/n9P/9jAP78mW+wnu5w4NA8MsupSecWrQN48eIV/vnv\n/O9kTbcRP/u5n6HwU3wsfuBTke5UVIVHkozp9rcYZhsMB461/uynHmVwbYXr3/oWhw59mL/+i78E\nwCu/+QQHO4uQeWw9cxYAP+lxsB2x+fzr2OYLiMhZHyrDnEbdp6kVldziZyUF0C7IQgi5xxGYPAMM\nQgiKIiOOHRGKbYaNfDxPYooCWSoglVIEoUcY+kgpyUsrRpalVJqCItfksbO8xHGCFT7VagOBIkmc\n5aXTmWc4HKNNTlYkzC+4RD5JkuF7tTfHk01xB+I2IQrWZSkqD0UtIHeBwwSFJCiJhWcVaSBJgHYp\nWKe7KSePnaTfWyO5p4ldcBtgI0toz0C7Dmevb/KtF91mXFsds7x0Pw1bxeYGkTrHntBEeLLgwqVN\nnnrBRXl/8NFP8Al9isxo/GpArN3G2I432dzd4uSZk5y/OGL9WskRdGZ5/bUVvvKHz/Abf/+TRJ96\nPwCVuubPXzrLz5xZo/N33wfAV7/+JKM85lTnbkR0N7vfcRzOjqzQnLNoPUCNFghUKW7UE/ozVzAq\npDo+CUBNb5AqSdeD6yZnsxQHgkqLuhTY/gCpM4JamQSmKdBRwQ5jyCS+cO1BVKUbKjZ3Bijj/Dy8\nShVRCLRVZFkfa5zzUuDPoGQFz6uQFQOurztrT9CUbNlzLDQOIm1tPwT77X5xa39gnyl+PJhaH6aY\nYopb8O/MKQghDgP/F3AAxzR+3lr7T4UQs8DvAseAy8AvWWt33vlU2TtZpGRP1CiKAhtYoiii8Fxa\nNoCF5gxhJaK/tcnK5TfI+n0AKkpSUQIfQ6tRY3voTvlkp0czCMg9GG9tAzBc3yS//yFWV67x3Hf+\ngk988hMAdIcJm6M+9e0uHgKVOdk8SuBU6wC9AyeJYh9Gjgs5llUIKwcwccHSgx8B4JdnFilkgW/n\n4KkBr7/4EgAzzQbVVhU/qoIIsXHpjBUol0hGg82dS7RSRxiPthmmY4pIoctEj6O4j1U5gRjTbkX4\nZTCT9QVWeggdIWwVSmuK0SE78dNEXgOvdF7K85h0HONXPNpNj7R05cYO0UVKvRYSVZuMM8c99eMu\n/e6ILLN8X1T6FHcU3on4UAD/pbX2OSFEA3hWCPEl4O8BX7bW/qOyCMw/AH7jLx9qEui/FxD91r2E\ny4Ac3DTroigwtiCohASBIEl7AITVKq2wgvQFkdGoUgkXmowKGhEPmV1s4zVdfILuRYjhkPVun/6K\nMyW2RcBK0eeaHnBud433e27T2VpIrHM8z6MR1EA7wuL3MpqxINiIufT7XyZbd3NJtgfEvSEXvKs8\n+mnnQi3mj+BnA3hhhde/dX6PEJ3+2EnyKCapAElBv3AEZ8k3aCXwtCAsrSCZVuwOEvoqxlR8gsjp\nU7IiQ0lDLaghpcaY0qKQ5xRotLVII13aJ8DmgtC3jHpdisTR79NH7yKpZFy7+hq1SpuFjlundjvg\n6tUdtjfHhJUWwnfz82xERbSoeI2bMjtPcSfi35koWGvXgLXy/wMhxFlcavefBx4ru/0W8DV+IFGY\n4C+XZkSZoFTAnvujEBatNQQBvm+wpTJwNBhRGQxoLs1zcK5DqzwtV7IRIh9Bbrh07lWG246DkEYT\nCkHd8zg4OwfATKWGp+DgwjyPf/InEGVigqoXMOoN6W/tsnNtjTnP+UZkwwyRGHRiuPj6JRpF6Rrc\nXsCrL/DEiy/zrRt/BMDRpUVG/W0ufvc1mtuzvO/YPQCENZ9s0UMGEYlWDCcntzQUwhJZxeRnG4vz\n5HKDTBoKZjHWeShKsYDCRxaWKPKwwhEzgXa5okSOYYtiYsbQmmbFY/nQPBfPX3XrdPllWnWPB+4/\nwPFjHV58yflR9HcvAG0OzJ1CelW6u46IDHbH5EPljoopp3BH411RNAohjgEPAN8FFkuCgbV2TQix\n8G5cw13n+/2barUaptBQZKgoxA/cxihK5x0ZN2g1qxw96hRoV9ev0N1ZIxIhST8h8Nypm8QZ670N\nFhcWeOihhwCYn1sk748JpWRp+Si7u7sApEnBcrND0h9iC0tn0Wngo0YLaap0jhyBfs6ZBefI0z55\nNxj4wHOCG+ddOYznX76C1QPapsY9B48ye8CZJLfsGgvHj6BllZEWmMz9RLlnKaxB5wbi0gworqHq\nYwIRMjQjksSJA75qo0QdkxqisInFcUmCIcaOsWKEZYQ2jsOxOqXYlZxcOkTllHMHj0dDdnauoZMR\np46f5OQJJzo9/71XGPQCsnzAbi9lHDuxouLVECZED0DOvvPfeoofH94xURBC1IHfA/5za23/h9Uo\nv7nuw9t3NDclUr253b3MzLTQNxKSwQBV8xiMnYwbNeoEUQ2tNUtLSzz66KMA9NIh33nmWS5fW0Wn\n0CizjCZpjkTx4U89xsce/zQAKqoz2tklz3OGvV2arRYAhRXcf++9bKysc2LhGDORi3BUqkJicqrt\neYSXIptlFqg0gyTnWGWZ+QOOq+h5q3hhwulT98GqgsT5B7QON8lFSmE9EgupcRta28J5LVqxtx5Z\n4qFMFT8K8JTFE06H0ahlLM8o2iqgu3ppLzNUXmRkMiPXBTovUCXBUVqh7ZDL599gvuNo+Ec/9CC9\n3QVW18+iyDj28AcAqEYVzr++zfkLQ3q7XdJclusdUY1qqOD7CfcUdxbekfVBCOHjCMJvW2v/37J5\nXQixVH6+BGy81XettZ+31j5srX14fv4HFq2ZYoop/n/CO7E+COD/AM5aa//nmz76A+DvAv+ofP23\n72iGN8FaV6hNuwkA0JmbYbR6jV5/l/ZiE1NWVFLaJ4sLilFGZ3mOjz7yUQBSURC1Wnz9yW+xtbrD\nYOhO0YOHj/K5z/17/PJ/+CscP3MCgNX1ddqdOr6QdLe2qZQJCEToY6SiXmtSDaqIzLV3u5uMrnWp\nm4BKWMEbOpk96d5AaovcKFiUTtF48HibXPagWgNGUHFij18XXN9YBVHF7FSwseMgVG7wrECi9lJf\n23SRIh8gPajWBEY6caDVMpw4UeOeQwtceHm0l216J87Y6hfs9ixm7CFTpzz0CInaNxgOYtbLeI3z\nXsiJk4v85GOfpd4qWH3BJXv53gtnsbrNkcPHqVYtr190OogbW6v0ht29vBBT3Ll4J+LDx4FfBV4S\nQjxftv03OGLwb4QQvwZcBf7WO5virRC4fAETRWO9XmeIZjwe0jSGxdoiAKu7N9je2SBMqixUDrO4\n5ATdxx7+JKdP3MsnPvQ4vldFWrcEc51Fjhw/QRAEdNdd6HQlaLC2tcbS4gGiaoX+jtMpLLTnGKz3\nOL10hGxjSLV0Rd5aH5Bc22ReNZlVEXrHbehxt08jrBHQINlyFglVadKLE8a751kIDhKVItTa9jPk\nYUxF+HhZQKNMkFK1gtzgLAmFUxxWPYVIckSREyqfzDpvRJEPaEQzHD14iKNHP1wmgIVrN7q8dnmT\nNy7tsjMu0KVFxjNgrAIRUG84rm0w0Dz13bNcuhTxwMMnueejHwPgW39xgQsXL5Okm9TqB6g1HGHp\nBE0I8qnny18BvBPrwzd5+xydn/7RR7zZJPk210QDnmMSSjObELC8vMTa5hYvvvg8H77ncQAqAwhz\njfATklqXWhlEtNhuMb+0wL0H70N5VbLSjThLNUoqdKqplCdxKCOORwa7a7E6RAkXPGU3M5ojxe7T\n5zCDnD974kkAzhw4zl0HjuBRoVjdZnfNpXrTw5hU7DAn2kT18iQdp+iqj5qpIP1ZzNgRHBspfAHV\nwlDJYFByIYxyilGCJyvQdHqJgwcNu2tD+rqPzQMWF93YOhtw8dXz3NPuMDs3A7NOT9DY7nHf6aN8\n9OGPsXp5m42rznLQvdHj2a0ezdl55MSSUgia7Ta9YZcn/vQ5VlYdoTR2BmMzvKDGzmBEYp2no1/P\n8GoZRk0Tt97puE3cnH8Q3Mb1lIekTLcYudNZNuswq5nROb2h5fLXnOns2H0nmVk8xWb/CqvfeZ3G\nmjMzzp0+gt9q4IUVlFcQFBOHKJ8ojEjHI3ZKM2WSrbO4eYPxKGEwGDAs80JmSU6RW2xmsFpwJHNm\n0NpuSlxsYnPD7nqX3rbLyJSnGZ7wEN0+QccpK/WhkKIlqS0tENSXIXbmzspwm9iMqeUKpRUdWzov\nGUG82+fK9jpcci7Ry+1TfPL4QfRilSvpJle7jpXfXN/GMwaT7bjc7+uO4Lz6wtM8d/YiYWWRRuUA\nM5HjqjrtFvcdWmA4yIkHbj3SkaGggpDOGerseSdWjMcJu31No10hrIm9cO2t4RpxsYtVMWIS2TnF\nHYnbjCjsl2t7O+ydQpPw3CCAMCQMQ4Lcoxo7DXx+bQO/yJiv1QilYbBaRkMOrxG023iVOsoLyTIn\nb5ssZ7fI6W9usHljDYAsGRNdW8dYAcqjXqYkU36A8iKs8rBClIVWQeZjsp2ULMtI4zGyjJWIfIFS\nFpMkZAO35PEwJy0q1CIf6rV9C4F2AUpxH+obBdl4sLc2c50Z/FoHax1H8L3fvoGar9B83xy1EzM8\n9lgZVBXcz/aVF8jjizAbQVk8Zn7RI39hRK+/Q1qZ48qGS6Vm0m2y90lM4dOoOkKhjUd/EON5GhlU\nyEtHp+E4I6rWCSs1doe7hE3XnuyMSNIehhjJlCjcyZhKgFNMMcUtuM04hR8Eg0U4r7yitIaP+rDb\nZTDokxcZC4fcSTfsbrLbW6dzdJ7m8jzNkgvPC02xa1FjDSIn6zq5ur92g2S3y3hnCy91irlWGNBM\nPDwvwPdC8JyFAK/i0h+VBZsm6eNJC7IsJtMFTSGQvjsxXZZkgdepk/qOxRkWGYNegtps0TQDZGmp\nyMcFjXabMJVYI0jKPAboAr8aMFNv4OnS47I1x/p4m6svXWXj7BrtF92a3Htvh5MLAUtHTsD2OkTO\nX2L50BFmO2skaz71xkEi69pr3hw7i5fp7aYEYVly3vMIpKASeSTrOatrLmN1q9VGU7C9dZ3d4YAP\nnHB5KKszd3HsyFG86Tlzx+MOIAqTcqlgrMEK6ao6lhr4uLeD3tpinMSuDoPvxITqYpV8rLm6eo1K\nusviCRdu7C8s4lsFSYHuj8ivOWVgfn2dKB0xYwwNr8zJENVIGwprLJkRiDKHg0xTlPBAea4o5MS5\nKssJYk1gjJtyWU0Om0ChodoiKGVwZQvyQcx4o0uqe1RwWvyKjOh21wiKJpWwholKQqQEsc7J8hFy\nXIog9DB+ytzSLCoIWR84ceCbT17nmXzAkVaVD9x1hiPHnKIxbLSptzbpX76K2L1BRboJNtsRO4Mt\ndCExobv3eJyw3u9SDTW56VJruHXNdczBg6e4//6P8P4HTu+VqvsX/+e/ZO3qBnfffXzqvHSH4w4g\nCvvQWiM8VVZTdo9ekiSYNMb3fTqdFmNVBi1ZiS0qRIVHYSxr3XUAomRMFDapRA1G3QG9VadAK3Z3\nmI8C5oIAkTl7P7sp16sVAt8n8iPCwOkOKtJH4ZfJEiUMSrk/y0FnQFESikkoZw4mh8AiSw/Khh8y\nNCneIEUw3qvOKjxJZiWJ7yEjj2E2qctmyXyLEh5RWfNi9vAuK9fWGPUGmNl5gui4u5w+xGBzizdu\n5Lz2zFWWj5YJUo7M0R3XmD14mEajgVfGcvTyi3hSowKDEM6MKmSfWn1MtZIRBBnz805B+r573s99\nDz/OcFPzvWdf5ew5V6YvG2YcXzoFWQX8KVm4k3FnEAUxMT/eXC6qTLeuNVhLJYqozsxgF90Dee3a\nNfI85/DBw4T1JttrLgpxdWWLPOlyYvkYqc4JwnKj16q0Aokw2V6lZ4qM+twCUVA6I5V1EsgteZpR\nZDEUGltyLYgCAo2UFuUVeJMK08pz5Zql2CtKEylJqA1ikCJ1DGW69SJIOPPQPXi2QqY148RZGkY2\nZ2Ryqp7ZW484EyDrRLV5BqLGzq47zU2iqPiLNEOfoJEzLtPIrZ5dxdYUVKsQSypl4tZ+fwcRjDEa\n/LLt7gdO8Mgj9zAzJ9nZeBU/dNesHznDynPf4Y/+8GlOnniYil9yLTsjNq5tc+zMoXf0U0/x48dU\nAJxiiiluwZ3BKZSQUmLRaAT5LDtHAAAgAElEQVSUrrtaaxSgPIHwfeKm4yDUYoNxr8eWHbHcWqCz\n6NKgVWY2eOPsZV6/fJEmPn55cnueoshigmSwzylUAtoihByKZERSmL1rCly+Q78aMSojBbWUFLZA\niwKrNDJ0NNcPPTxP0h6avbF1nmFMgsoVUuTsFWEJFMZYCmFJhCEv52dDDy8MkBZkGfKcJgsUhSTP\nm4w0DMpakpEQFFIzyvrERYwqfTqEL2nMzjPShuFIs3D4NADtmmJXfYMPfuBBHri/TCEXQb93hS//\n2RO8+sqTLC23AXj8kz/N8v2P8HeaR/GDZf7N734RgEatyVx7EXIgeLd+8Sl+HLgtiIKLZQCciwFK\ngAqMC/hVFq/0MFRCljkBPCjZ1l3Ppy4r1Ace5twu+axrn08jZr2QQSHYWtlgrudyCVZ7BffX6lwM\nxmybAQ8dcEq44ttbBITEzTq6tGzUC8H1mSq1Wg0lBLVSCdes1xltbnHt0hVsMqJdLUvPWaioJhQ5\n2Tihbl175DcgNpCEmJZzaBLhLkt9hdipk4xHJAcvAZCHY/LzQ2rxHP4gwwaO9acqCLKIZuajyqpR\nN3id5umUi7td4uw0YebCr228y+xMTjYaUfUW6JWJU9TSKuLAZcg9Hn3oczz+aUcoX3jqD1hUd3Pg\ng0sgnQPUb/4P/xO7Wz614AyvPneMo5/7LACzjccAeO7s1/jzr/427dljbv2CiM1hyolpLoU7HrcF\nUfhRIHFVp22paCyKgjzPKVSBVAVp38nVgQxQSMLEoBJNmVIA4hjymJmZiAgfM3AfyFrEzkaXWtSh\nEpYqdU9y5OAyUgqM1niilLakpNZqcfDoYfJxwo01J/fbrKASRTSiKl6zji0Vk4U0ZEVGVVWwpcQm\njcQYQT5OSJMBdrb0ipQ+xWAMaYLJDEJNkskoBAorFHhujEPRMisbF7CpwKuA9Z3L8TDdYS3bJqxo\nRiajccDdjzfv88EP3cXDDz5OJZjBlg5ds3mTcxeu0Otvc+SYM3f+tc98jmp4hCsXUjbXNXHs+q6s\n3GB57gBznUWa9bm9NG26cDUlpqaHOx93FFGw1mKF21airBDl+z5BEOAJDyklaakLjOohgRQwHpJl\nGWNRavermlGs8Y1HQ3rsZM5y0Do2x+aoi5Wa2VL5SGG4fPZ1Di4foLa8DGVK9P76DZIsJYoi/HaV\nTuDqPmxtbDLIMlKbIArwy2CrqogQnqCqFbqMwbDCoqyi0JpsPMQfOZNkzTTJshiMIRQGv/SB8IQk\nE4oC8EpFIzcMca/Aa1QZMCZVZWaoZYOqaKJQM+5tcPfHHwTgI4/9BLWGRIwtT/yrP2HrtSsAZLtr\nXLdjHv/JR7jrEWfBODSrWX29y3a3T6XS4MKFiwBUWh69cY+19S7Kq5GXC57rIeKOepqmeDvcUT+j\n1hozKQ4VOm19u92m1QkIcwXaIA+6gB5m29hCo6/EJEnBeMb1t7N1AjNLemWDMNMMWu6Erp48ggwL\n1q9sIWtuI+a5oSUU/dUb5IMhURkRWK9GNE8cB18xXF2h1nRiQjjXpt2Zh04HhiPiDWfuHPT7JEkC\nhSQt5X4rM5o2wg8CkrTAjt0pb3qKigpBBnjSEpQJKT3pY40kNxZp3ekceXVq4SJxmHM93qRx1IVl\n/8RPPciJE1WW5it888k/o95ysRwzHY8sK7h+9SIXzr7Ax0+7JLLPPrkDYUCrfhR8FyWZb13nj/7w\nS1y7FHNo+V5C49Z1dWWTazdWGY1yvHAWpdy6pukO2qRTfcJfAUytD1NMMcUtuKM4BXAihBE4bSQu\nR2PY8mGQo/sDROBOc1EJ8ApDRflYIxgFZQbkuRp1GWFWdxGDIcFCWSFqqUUnPMOVtS5ZqfHXvs+B\nXNAd7jAY3KBXuijXOm3m5jrQmaNezPPGRefAU6s3YaYBC7NO7h+5E9qTlrqpQy8lLyst6TyHKojI\nx08LemPXV68OWTp2wsWEK40o71MYBUZhrUGX7hqX4xVW0z7jiqJzqMVHfvI+AB577H5EPgBvyE/9\n9Id56dtfB+B3futf8JGP/DSHj57m3/97f5O50vqwtTVm4+Iq33vuMotlrcv508e5664H2Fp/BS+o\nUCmdrmI9IhkPSNKASFX3vc3jlKwYgyzATotJ3sm4o4iClBIxkadL2Vxr7cyTacpoNII3nEKs6KVI\nK6lujKkOCwgce55FAmN95kbAwBC3ynyHGz1mZIWB9gh3XN+wWSPZWKcZ+NSiGpvlJr924Q02treY\nPbSMV6/QqJYZjIIAsgw2tyl2djBlEczZmTnnMPT62b206iKz2MAiVEEeakal6SWODUtCg5djTUZa\ntmttUdrHVxBEbozgjCK+UnB9tMm9S0d58EHnOJR313jqiSfJu9f41C8/xv333AvA8986x0J9Gb/R\nwuvEfOPLXwHgXH+LdBzy6kurtNuvAvCpqE2rcYCZ2W3SNGc4cmsifGi1FvECRa9vGMROJ1MUBpdI\nOwWq78KvPcWPC+9G4lYFPAOsWGt/VghxHPgCMAs8B/yqtTb7y8b4UaAmddjLIiSDwYBgmOEnzgpR\nWXEuymZrRGqhOtZgoeW5zZXkCTozMBDgV6kkpRXj6jaEDY7UOgwuu9DpyKtAVANTUPSHlKoGGnMH\niIVh69IVhrqgOuNkeb9SpeiNqFSqjIcxeeKuuWMtaZJzdOwRlDUbUHUKA6lKGFZzZKM0X86G2Egi\nfENfp4ytU24aK/BNSMUHQjeRn/2PP8WpVzf53T/5EkQDdFlz5+K5Nd548Sp+2uONJ77NyZ935sRH\nP/Q58lENPTbsjnb4k6+6dPOROU6j+T5UKrhy1Xl+/uv/+9+yvTNC2BqNep1KmS6uO9gmiFrEYxiN\nDEI4y4ZUgStmOyUKdzzeDU7hPwPOAmXqYv4x8L9Ya78ghPjnwK8B/9u7cB2MMQjp4vBs6bwUxzFx\nnCG1xPM8RMn6p0lMmqUY6VGv15Bl2aKqzsjygq6yzC7NMbGh2SzGZini2EF2Vp2J0fMtlRvbEIX4\nWYEpS7r7BrQ2tEXAwcUD5KUzUTxK6Q9W6WuL1VALJinXA4rRiCTzUWXwlO9XKBTsqpjdGgTLLl1c\nY/kAQyyBZ+nnBXFZ1RmpCKx0YoVXtjXH3P3h03xo7QJifkiz4+7xutAMtcSOFb28AiN3zbRo8sUv\nfJlf/o9+lmPH7+bUkktBf/nlhF3dQ3oSifPFKPKURn2OwGswHqdY4da7VqsxHo/ZWE/wgjnas27e\n+WiNJBlza43wKe5EvCOiIIQ4BHwO+O+B/6JM5vo48LfLLr8F/Le8S0TB8zxMGTU5iYMQQrgU7MOc\nmdkOXRe3w8blARk5qxXLqTNztJvu4S0unGckFeOlGfzD8zRwJ6Ba3WR3GOP5GvWQi6h8+eIVPoQA\nbfCCcM8bMRuMqUUBjUodkRnKNIpUCs8FLIUhWEtepps3RUJL+KSyQJTythfVuda9zGDZJ1tuEJ5w\nEx82Qg60ZwgbEbsXX6C74Tif3BpCIihyzn3TpX+LbmiO/Nx/ws989jM88fS/JitTo1UOtBlEEmVq\n3NAW2k5P8NrVyzz/4jUevTigFXS4a9aFPffTy4yqlpnODKoM7bSE+CJk0E9AGGZbjuZfXb2MtgH1\negcpa/R6TnzIdUG1WiU3ySS8Y4o7FO/U+vBPgP+a/VRJHWDX2tJmBtdxVaO+D0KIXxdCPCOEeGZ7\nc/MdTmOKKaZ4t/BOUrz/LLBhrX1WCPHYpPktur6lj5u19vPA5wEeePjhH9oPLi9ylLc/7UqlQqWi\niXdiut0uqxNPR2EJZ5rknQrZQgOEM6C3VYT2Fes1CS1FpEs/hSCEQJPXQpTn/PxnTYG9cQ5hNQiJ\nV+Y28ILQuVlbAanBn+g5ZORyLFgFaY5KS1Y602A18axiXEYyilQRzrWoffAAyd1t6scOu/vxG4SH\nj0GyTchlioknppIQ1ln/3rd59UWXPLv/7U1+KribxUceYWl+kbR0rppZ6jCzPMuNi9dpHz4KpvSs\nVBFB0OaZb7/K+e8OWXl+1U17VKOykCNUjC61o0mcM9Oe4cEPnKbX3+S7z30ZgNpMiBKCghStc2x5\ni0JIAhXgy2gS1T7FHYp3muL954QQPwNEOJ3CPwHaQgiv5BYOAavvfJr78DyPmw1eWmvyPCfLMkSh\n8Tcc+9vILVIXdBkS+xtQOt8EPcNSM2K3lyJ2x/gTFehmQm0Qk9Y1FemWZUHW6UtLUeTYIqVScfJ2\njcDxRsY4K0jpdowxTgE6jiHZD3IiCED5jMMC7bu+nvKIFmdpve8w6b1NqiVRIK1AKKFV4UT9FFcy\np/Tc6e1SP7DEwvw8j376JwD4iyvnWNsasBgGVGstXn3RmUY/9vCn+Nt/87PEm2MWDi/Svew4sZWL\nK2iteePyOnbYJSiTtRyYW6LvvYalsqc4vPuuezm4eBcvvvgK5954gROnHMPXHV1HeT7GpOR54lyb\nAc94oCUwNUfe6XgnKd7/IfAPAUpO4b+y1v6KEOL/AX4RZ4F4V4vBAPhlktS9Aic7O6TdHjaOqTea\nzJZJSdpe5FyKhxmVrTHOuQHIJeQeM/0YVnpQFlthfQiDEd6uQpY5BUQO3pFFitGYPEkRZT4F6UPF\nUyAVCA9K3wOES7ekbYYWBq/kaKQyQMKVvM9SywVgVdsLeCdrcHqJ6KAgF44lWL90jee+/i3u+9AJ\nTnzyHj7y6IfdfV4dwDBGLMwzU3Pp4tbzK6xdvMgHfUGrvcjXn3D+CB88/AANf55GpwVDn1e/6src\n37hwmWrQILWawgvxFlwC2LhqCbwNlpdPcc+ZDwGQjds8+90X2Njo0m41uHbNFYPpHAzw/ITxWGBM\nhUrguCprAtCe+5u6xN3ReC/8FH4D+IIQ4r8DvoerIvWuIM9zCl8R4Dn/hBJSSowQaK3p5aWyrd1E\n+ZKaF7jEIaVfA1XAjKkFEXGSM47LwiqhBRtgjMGUBMc3gnyxjcmqmOGY4cht3FQbEpsSaYVnc7yJ\ngxE5hGBViDGaYZm+LUnHJEnCsBPh1zsAeI0FWK7BkTlkZ0jSdRv93EsvsnrhDVqdnOX3t2iUSsLt\nKzsMNzepn2qzsubSrtWXj7Edr2NCS55I3njJcQTP1l/mg8snaLaP8/wXv8lzT78GQKg8anMRq6Nd\ncsxeJua0WfDQsRYPP3Av1y46s+Y3nnwaikWa1SYb3VWMdMTz5MllCmOJk4w8jQl8Z45F1PBlCKX1\nYoo7F+8KUbDWfg1Xch5r7UXgkXdj3DdDKYU2Gis9wsCd5gcOHGBWz7BTrDIaxwyXnJiQHJrF5hk6\njhGBoldx/SutGsYYdBxTjMfslOxvcqBC1XrYfoIuS8lpBPF8SKg8glFCse3qJ+SjlEB4pKlmMBwh\ny82fpilJlqIFWE9iSrHCa4cE0QwnPnUPs7GLLUhWxpjdLapdCU2o1p314VRngeDQURYaVaROmcST\nF1nO5mCL+t1Nmocdt+HZDRbnq8iqQe2G+NIlrX3qLy7wveELtE0H0kW6204XEh72SOkjvAHHji1x\n8rTb0J3ZjMceeZDv/OnXOPuCI6rYZXa2u+SZYGaxTnPOhWU/9PBp0qwgS29wKZNEwSR4zEPYAKzP\nNFTyzsaU0ZtiiiluwR3n5izLU0iUWZHDMMSrKqIoIk5SmkeWAGicOkK224ONHTxtSNqll93dy0TC\nJ/3uy4SJZlQpS9HPVVHGQyU5unRG0kpSP36QWhiRD8bYCbdRQKPehrTArG+ytuKcncZkJEpB4FEo\ngy7jLWYWZ5k7eJDivhNEK+7U3lq5SHdzDfPCFcbrOQ8+6CrtHbn3/cRnL3OgWsNv1CiGjjuZaTe5\ndmmV7Rsr+Afcz/YLv/A3oL4DpLz83EvkqdMRbOcFgbYYZvDVMqbhuJA82GKQXKUzV+WjDx7n7tNu\nfvOdHbKdc5w5s0B/y4kPr22MqYYdUmtJxlscqJWxD+MdVKgIQ00QKPygFJ0K0KlxvktTXeMdjTuK\nKGit9x64NHHyfbfbRXUL4jhGSkl327npLhw6iAJqWmJGKd3IOQHFoz4NLankIBOwNbcEphrh5xKZ\nG0SZOMT6ithXCCWITUGvrOKk/BA6bbASaTW1MjFqlRkqzTrRTJ1+FrMxcBs6aNTwF9pcH/XwxUSn\nUMXPQy6uXOLi1Uvc1TkFQG3mQ9z1sU/AwQQUXLlwDoBO8y40lsvXrnLfgWNuEYSFZgQrV3jl+RdB\nOf0DymNsdkkTHz0QqMhds9LSqFzSqApmKpqacfOTdo2gYphtdPiEOuPuRRa8/mKGNQm5MZx7/SwA\nY/sqi0sL3FiXxPEigSrXylqsFdNCkn8FIOxtYFR+6OGH7XefeWbvgBEYKE/rmx+yNEsJAw/I6a26\npB9f/ZPfI++v4+Uj0DHttY8D4Pse0oLOUqzWhKUvgY/Et4rd9W2aQYNKWbhA5h6eVlS8Kr4qrQ8G\nshNVfOUhEVhdKiAFBNLDak0Rp3uWDWEFCh8pApQMkF6ZXED5ID1y6+NX3Wmebe/gBRbZsaxtPYuc\nd7L84q/+JFSMM+8FM3zrS98E4MyJ4zQiePHZrzJb5oA49cDfJ17b4WtPfofnXzqHKetOCq9GWijC\nqE6BJEvdvL1AUW9UMSYnL2I6HeflefTYYR5/5CuE1RZI57l47UbCd5+6wAsvXafIGsyVadfOnLyP\ni5euoXyPYTJC1t266qDg4U88zE986u4fqWjczc+fEKIkLmV9jZIbRI+de/fk76ZM3kIIpFKu/Ye6\nntzjMgF0UeyP8cMN4JTRxuCp8kw1BmuM87K9aWysde2+vzdnACbXMmavVAHKWbJsOb56i/lYa2/N\naP4jQgjxrLX24R/U7w7hFEqFnee5ClFZTmveKdX+2md+mmpVQNaHdAT6nv2vFTmkqavHkJbFYUcJ\nyWDE4fgA8WBMPiyTtGaSfJzTHW3hScfi1ytVLo9vECgPKSy2dHMW2iJ0QZ6kpKOERs0pN6X1kVKh\nZIjnhfimdHbyIpd0Vkpa1in3skqGtRo5TtgtdqmXOSfJY+f3EDT4xp99mfMXnGjy/vs+QDRX4d77\n38+zT30VgJf+2R/TH47Z2NomiwV+VGr+C4XNC0bxDnGecXDZKSZn5mqk2ZD+YBNP5hjtXJQ3bmzg\neWe4fHmTC5cuAHDx6g7XVkcMxzVajYNU607RuDMw5CYkSQpyq5jEcY/HKRpFbiH6EZ/bCRGYPPST\nvz0of28zCgFiUtNT/bskhJTcfNII6e1xOdZa8tIBzPf9WwgUOKKllEIqhVTs61OVQrwNTdlrV29S\n31mLNeyZuSfPuBBinxj+mDBVNE4xxRS34PblFATfZ9myRiCkdFWgJ8GD0oM8cx6EfsiOfRGAarVK\nEARkWUJRFIRlZZagViMIZyDOaKcF6JJSexEYBaMM8onoIlmi5q6Vp1AmLzVpjElzsnFMMo7p77oT\n1xhAg9EWozPGlNWqzBCQ5KM1uqUdXyMoCsjiMdvxDcSau6HNrwx58Bf/FuOtHk8//zKeKnM1RBEY\nQ21+lvvud1mYnzobMO6NmLEN2jWPQpf1KI2h7nsUaJoLs2SFyyCdbW8QBAUNs4PnaR444/IsPPLx\nj/LCN85y+XrMxStOjNnuSXLmaUSztP0lasJxZt2rXaQN0AaajQirynTz45jZyiyh4UdSNE5EBuD7\nxIbJe+compMkyZ7IMPmuMcal6TNvX6n8luspD8/zbjmNJ6eztXbP/0WpbI9TeLOIPeFk4tF4b75v\nxdZP+v1/7L13rGVJft/3qTr53Phy7NyTQ++Ejaa5JJegvZBFyjZlCgacKEMADcuA/xNEApING5AB\nA4Zg2P7HgmTABEWLpgnKskVK5C4JLjdydidP93R8r1++7+Z7T67yH1X3dvfsDnd2J7Ah9w+Yed3n\nnT6hTtWvfvH7DYLggeeb3VtKSRhaUF3x4PO831h9EvLQKIUf+LoCjDFjBqyoTJWgwGPWoqtKTG9B\nISCu0dqwef0yYVINkDWHIAgA41cfDW7T3euysLiMcgW5ZXcqpprIj2iuLhAFJlOhlcDPS6hMQZOw\nMQUpJNKLcakTI1m0qEQoAUqaQqlK3GsT07M/r8LAVj9OPNAxVIJJNqJnYdiPpkPUdEjhBXzui18k\nte6NFhpUQpn2WHzRdDdW//OfMto/oFLgh3Uyi/OoXZdWu43je/jJlLZv6yimY0gzXnjqAs899zjT\nZADA137zN3nzGxF5WSExMY9lL0J6IU4RU5vE1K0WVmNJvdEkKUe4BQymViEeHeMdj/BPl2D1R/jw\nPDjZZ0oiz3ODawnsH5+gtaYsS/I8ny/S2bllWc5L0H+YTJLpA+7J7M+zhT27dlEUJtt134Kf+ftl\nWaKUom7jQ7PzZr+//12klA8osvvv4zjOXCmEYcjqYgvPe9Al+rBxhB9HHhql8L6i5VxjhBZ6vaoU\n0nJBuIQQeiYwOa2o+mbR+YGPg2A8mNLLBmB7DuJane3tiyjpcHh8wvGxIZhN8gJH+oRhzyAlAQsL\nC9RESkmJorLlyoZ/wtECifkv6VpUIuHgSBfp+TjSwwlM4E8Kx0C0R2MIZ8hRAcgIVEDNXcCfmLRh\nsn+XN29d57nPfYkvbF6mf3Bsr51BmCKLCfTMM//cl5c5PFRU2kHIgANLjXfaT9BiRFkJFhYWOTw0\nFZCh6/LE45dZXVzg9W98h9dffQWAu3fvcrn582gh0TPfV0Fl+0kiVRCVJnvjVxntqs64GKLGFSIz\niiXNMy64/o+Ufbg/hjATYStTp9Mp/b7JjiRpThzHNOI6Sqn5wnVdd87o/UEthe04fsCyuN8auP9Z\niqL4vvjG/coBoLDNY+9VGvcrrdnx+wOHs34dpRSp3ZSySUkzDpBSPnDuI6Uw31pnP41CmBlvaVrh\nSonnxZCZSbq/d8RyLKiHDpQ53tTGvhsNHN8lHJzS72ZzIBS5IKgtRiR5RtH32KybVODZCxeRwuXO\n7l0mA1NynGuXpdWYyimRPjjW7BNKk6cZeZahlKLWaM/f4B4/g0TbFVIqjdYFatADZdwB3wGKnCLP\nDYybrQNYOrPG9Vt7DLv7DHs5myum7qLKBiTjPaKWouybJinvC5c4M/XAq0HY4NyMjLaS4NbBiakO\nT3BsFmTcH7C/v0+vOubslXU2n/k5ADqdY5ydNtM0ZZrbgKyaoKUNvuUVRZbb9ysZjAVJlpKWGYl1\nkZRKmYx3aJebH/xrvyfKfv/kz7KM4dDA37UXV2m328RxZNizZmXonjfjBCIvPlgWrarK+b+HB3fu\n+xfkTDkVRTE/X9kMg+M45t+5zvz492VMuKckHMeZ98HMrjvLYMwCm3meM51O8X3/+5TCJy0PhVIQ\n7/n5fhIGlnFaOZzumzr//+f//j3cbMRSKBh2jlhumFx9EIasbK5z+anHOffEF8BCpR/fus4rr7xN\nKaHWbrJ80eTwPVYAlxWnoJyYRTfojoiHCqUrPMelVrMLOogIRZ3Qd8Hz5tBwc1dHa/M2M46H2QRw\nV/G9lj2zTl45VNr4xK7duWq1kMWlOn/27T+hTAVbXzRFTUpNqYoxZBPcRfMcFA60G5RJSZp3kRb2\nXuGATAj8CO/lJW68+l0ADpK7rD65xPkzG7gO5FYBLMstPHXRYF3OyXIxEfNKo8ZTMtv3EbkRg+6Q\nUgm6gzGJfceT0RB9VkM8AFo/5EvaW7xnB7x/t77fv1dVSZFnJBi3YrZIZwuoqqoP7D5IVeFyb+f2\nbbn8zCVwrAWKBoHGcZ259XR/fEEIQWr7bMqyNPSFjvPAoq5UhSoLsqR4QNnAPSunsm6trjKSrDBA\nxPY7vneMPimr4VH24ZE8kkfygDwUlgK8v3a633iqKsgSRS2SLC2ZxqIrz13hiY0VlpebHF9/h52b\nRwCsbWxw5tIF2FyHWgQWDGp1bRMZxXixT2t9HWxDT2orIRsbazQ2bWXgcMgf/8bvkE6mFEU1p6L3\nXQ/fdYmiiDiMaNhmJsdzCcMYPwoJwhDH1g0Iuys4oc8c1LRw8XHwax64E7Qy91c6I5aawfFdPv3C\nFxGRdYcGFfWtiwxufYvWosVinAharTaZzMncgnqzMR+rcTLGc3KOrn8bf9HsLpfPnyeuOUzLLlk6\nxImNdRJFAclwhIxdpDWJK63RAlzpEGwFRMLuXqrC6yhazVXa5TIyNFbLUX+CW69TNlLcD2gpvDcf\nPzO1Zyb6bFf0pUaqApVX6KLAt7tu7Ds4jkNRKKQqPtA98yx5IPBXZA5FUTAajUiSZG4JZJlhAKvX\n6/csxPusivuraytrvQjPQ7oCd2b96xKlS6L43s4/L3yS2pxncT9zXTLN8we6f98rn5Sl8NAohXtx\nhPf/zTgpaddcqkmJzs3gbW9ss3zxDEx7rF54jKPc+KErT6zCk9uow32ku3DvKouCxfVl3rn2NoEI\nCT2zSGWrMlQSTp8ZNruKS37yL/3HfOs3f4u9oz22zhkgFIlg3BsxPJoyqMaMG8a0TjOTNsurklJV\n80CUlAZUNoyDeat2rbnAIJ3QWA1ZP1vn6RfMtd06bK6u8uWf+GmCcJ3khgkqShHCqU8ozoItrlrU\nDeI8JlZQUBEUtqLRcai7NaRULK02UHoG35TDNKMsNX6pcGZAtNrhSI2J3Bg3mMVBTCBMCUlVJZCb\n4yqt8IMaSX5CRUA2MbGdSaUgK/AbIU3WfoTvzvcFCd8bsKsFLkvLC5RZhlL+fSnMkjAISMcD/A/o\nPvQ7x1y7do3HHzfl3OfPn+f27SP+0T/8hxwfH88Xfq9nskG/9Eu/xBe/+EUAHIyrVyRTgiBA2C7Y\nIiv56h/+Cz772c+ytrJIt2tJhIUgDEPKMnsg7aqUwgtcsnRCo24K3zrHBzhO9IArAg8qzk+qqOkh\nUgo/XKLIPK7jusxCRclkiup0kFVC2jnmoDSWwmQv43OPL5I0cuJgSmpR5l0/5KC/R17PmQRDhE1t\nFkGKCyjGKGnTgJ4mlPcgtmMAACAASURBVBd4+cqLiFLwqRcM4AnLyyYrcnxC9+4+i+fPm+P1OvT7\nHO3cwnFdlrcNWlGVJFy9/i7TgyGrdTN5N86c43jU4Xi8y2vf+DrPPLVkX9KHaU6wcIF3v/o2r75i\n+B6Ho5xa3cOLKhAm7XouDqjVGvhBhOeHBJYVWwsX1wuo15sEYYiojGUhhI/w23ieNKaZJbfB89hs\nnJo6DQt5T5mgdYFwJEhxXzkukJXoJAcHYpsR8lSFJ+tEP1KR8wcTVZWoPGMyHuE4zjxtV+Q5riPx\nPRfxAdu1tzc2qUcxCxaWH8flzOYW//mv/GdEUTRfvHme0+/32d7envv44/HYpBE9H9/1yCvzHXxH\n8u/+lV+g1+uhipzlBRN47nQ6OAREcUSWZfNnqKqKIk1QZUlqlWrkewas6yGQD4vm3Ab+V+BZjKX/\ny8BV4DeB88Bt4N/T2hIS/HnXep/j9+8hrgt5Cr4EYbMJZZmzc/s2bVczPD2ivmo072mvw2TUJ243\nEZ5AzYJ+KIbjAVEjREtFaemoHRSSioqcav6xPagmyLMrZN9J2N0xzUnZ1TdYai/QbLbwYkl37wYA\n9Xodx3EIai6TyYS93Wv2uV0WliOebm6SW/OwlILVJy7z6rUOXryOY/sQIINaDJ2U73zlNc5smVL1\nzz/3DIPpmNH0lMPOXQDG775OtObixD6jaZdDy1OptId0IiotKXJNZnsfVK7wvADf9czisruR67o4\nF108z8O2fSDcCt+TRKEkcASudb8M/4ODCFrQbCAsuY0zHFANU6No7nkxH4mEno90XALXw3Vd3Nha\nd9pkguJG875g758vaVFQb7Xm30FMJiRJwsrGBjgO2cRknhpLS7QWF5FSMhiZWgytNVEUIT3PAPsk\nZp44jket2SZNc7rdPisrxrWtKk2el/R6g7lb0G63CYKIoigIw5jp1BRAua5/r2juL1g+rKXw94F/\nrrX+RSHEzGH+28AfaK3/nhDibwF/C4PG9ENE8l4XQr/np5j9T4Jj25g319c4vXmVvIKzmxtsnDX+\n7FvX3mG0M6QWr0N5rx6/M+xTUw1aUZNIB0jLOOWicXAoK4VjP47nhFSdmyiluPypiyw0zbX3dvcR\nNXC2WjTyGofXTb9AP01ZWVmhfWab+OSQW7eNsmhETTaffhp6Pv6xiR3s3XiXC+deYDju8dmf+vS9\nV09ycGucvLnDuJOx+rixNmQek3RHBGGby2eNVTHMCtbOP8XS+iaj/pDeyE7o5iKu73NyOqDIK5LE\nRMnTNEcpZd1YhXLuWQqHOzlC6LmPq3RJpcZUVYJKh1Q2U0GV4Dng+A5+M0IFtss0cnjyheepPzaj\n//joZDhNiZsa4QVkZYkszWDlCooso+H6ZHn5Q65iJGq0UUrPC6NqtYgCj2muGQxO7504SgnDEK1L\nMruDB0FIUgqyTAGKKDLvKoRgZ+eAtbU1Op0Ok9QogHprmbIsWd1YYjw2FgGOw2CckqYpS0tL5JVl\nKKu+f/7/RcmP7aQIIZrAT2Lh1rTWuda6D/wChu8B+/OvfNiHfCSP5JF8cvJhLIWLwAnwD4UQV4A/\nw7BFrWmtDwC01gdCiB9Y9CqE+BvA3wA4e/bsB7phVmhiX5gKZ+tXLq6tMtq5DiqBy8/huca0PO8N\nufPqHuvhJqULKjC74uGNAy48eYmm2zYmyGwHlAoccHPbtgxADEHCG6+9xgtXXjRxBGAhb1KPY1gI\nYTyhuWTuWZYlIlIQg78Q0h5ZM9etQA2htQpTY7K01xcZDDtMigJ/Y8t0eALIGEYle7tHnFk/g2PN\npJODPTxPk2RDhhPb3ShWSTqajRAcb5GB3bU7SiOqgoGTEy77FKU5npeFddMU0lFE9nwROVwqPkOF\nKbICKHVOnk3I0wllOkJYLInYF1QqZ1pMSHXBWNliJ09zoj3O+h89RmMlA3Lt4kchRZKRaxtbChsU\npBR4yOCDdUz2phVSSpLMFhtFkOHjSQ8nFrRbJn4wGpcmGJiVuHZOCdclKUuE7+F5HpN0ZK+qyZRD\nphzCxiJJOcvqREymQ8anw3tB01qI8B3qURM3CvHtu/TGRw8NNs2HUQou8CLwN7XW3xRC/H2Mq/CB\n5H7eh5ffl/fBsEHN/6Yqc9uyhMSYY4OTQ0bjHpPJgNWjPShMkEf2c/KjEaPbHRJZ4rZMWml02IOz\nFWgf0PNUJVVme/LlvaBaUeFcPIt77S1o1ujd2gHg5LTDdDRmq9mAoIay/uJ0MsJ1IK7HkEwYDkwU\nOggsu1Q1psxtYGltke+8/TbbTz4LzVUo+/N3zvaPuH1rh6X4AmDOX16qo4OC/c4+Yd0sxCvP/zw3\nb9/iOO+zubo5L1466R1RViXKK+gXOaXNPgi3wA8Eni+QUqNn3dqex3Bi4h/3t/gqUVB5OZ4nCV0z\nfsenHfzAw6m1qNcDGnXjxuW+xFtbYigkH7UDsbSxwSSpKCVkykXbBR1FLsqNSdUHT9OlGloRiNAs\n9NORJk1LIjxK5WCruSlLQelAoe5VIxYKSuUQRQ6jTKOx2R4h2Dp/ht6woNmsYb01pgoqzxQjzSsT\nfWH6dSRc2zlhbc3EH9z6Inp49CFH6qORD6MU7gJ3tdbftH//LYxSOBJCbFgrYQM4/kBXs0WAP+jw\nTKS0fRCqoEhNgObdd6/S8ly8OKJz/RqxVQpZnvHY2bN4VPQnY1Jpdrp6I+Kwc0BzqwWei3IsSGtV\n4DkCHSpSG0CiSOh9fZd0lNC/vTvvhqz5ERSa7o1dakFI98j4omWeopKMYjxlMh2Q9s0M85qgu0Mm\nkxGj1Owio1HGreMuf/mnfgoqjyq1ATtd8d3X3qLSCj8UvP22Gd7WUo1euku4WPCFl18w79JKOdm5\nzbSXUDVP55gM8TL4YUimJMedPkVlEaNcQavepNGogVDzWEOWJUSLHVzXnxPtaC0ZDsaMuiOStKSy\nEchLl84SBjFJUXA6HTGyvrIKPepKz4s5P0r55ivv8Ed/9EfkeW7rEow1E4bhPH13f+nynyfTUrK8\nvMxgYHo23nzzTYqimHOHzLIEUkqiKJrfE0w/RL1eZ3FxkVu3brG83Jw/x40bN4jjeF4ZCSaAu7a2\nhtZ6nuLMsgzP81haWuLu3bv82q/9GgAvvfTSRzRaH14+DO/DoRBiVwjxhNb6KvAl4C37338E/D0+\nEt6He9bCPE/r+3g2Lz2djHnpuScRoWT4zhsMTs3u7EUhrStPw7iPfzJmqs3HXlpvc/tkn+1iizAK\nqVwzmXKZIXBQImcqbflqllIPmzx28TFOjjpGGQCbTzwFuaJ7/TZ3j/a4dO6Cea6lBeh3uf3uVeqN\nGs996d8wx4Vi8PbbjHXM6vZFALq7x6ycuUBrbZuqmjK0vJMLrTq3dvZ5/pnnWG9u0DkymYZPf+5F\n/OYZcveQcNUugGKP9fCYg/4Rw1t3iVsmEHr+0uPUWzG7ez3SbDoPwnl+xFq7zkq0AkhOJ0aZdYeQ\n+lfx6k2aFoLe92r4KoPxhCTNkLkxz/sHt7l08XHW11eIBz53u+Yak6oi1JL6x1Bbs7Cyyt7RMScn\nJ9Tr9XmQ0HEclFL4vv+BG6JyJ+b2wREHB6aU/ZVXXsH3fSbjMUEYztOPk8nE4H5amD8w7sALL7zA\nyvZZToZj/tLP/1sANJtNfuVXfoVf/uVf5p133plfY2NjgxdffJGvfOUrZKUZmLCmqNVqOI7D62+9\ny0nXbDTClQ8NBvaHzT78TeDXbebhJvCfYFbw/yGE+OvADvBXP+Q9HskjeSSfoHwopaC1/h7wgzDf\nvvQjXgmq0jIsgXZcEkydTFAxz9Q4HgyATOUsDIwJecW5hNDPwCSheWYVdWwARllbgMU18rMb3Pl6\nl43CXPvMwjOcGcV0v3aHdL2BaFvQkyyjtrwOAxi9YZqttre3ufbsFk8cZbTHFZPY7Lij5SFuVeIc\n9gimPQZnTdowWU4JWw7qTkUUCNgwhTwj3ecgbrH2uy/hXTwPQPbmf8tP/Mwiov016CQs+CbY+u73\nxhwOn+TLz/+HHN56lchyPBzoXS48eQZ34MGspNe/xfnPrLM02ebd632+8eouANf7J3z+J54niVtU\n6pS9PYNnubW+SNKsc6wqwqDGqTIuwUDUKBrPc1yUrE9NUc+L4SaLh0Mu/PGr7Lx7yNh+hFTvUV3I\nENurrD93hqEt5b6bHtGoEhMY/RGthdlO/F4sgpnZPi4G5CInWllkqhxUaJ6xRCKFQ1pVuMKWB1cF\nQmik66Edh0LZqkMh0UjqjTWuXr3KtQNjbRSti+D5uC2HQgtKi59WOSllo4GvIbVgKnnosztu033t\nlDRt8Y/+T8Oveebi42x/9t9h5dO/yBujf8mXfvJzALz6J/+S/eMJ/YnDSWZcjWHl48smIhvTWLxI\nWJjnXtOKPfv+79co9knJQ1nR+H6G4HRaQGwiv17LxA68wIc7t0ElXJ90uHxkjbD1NvQd/N6Uc0mN\nxdxe9SAB1WSyd4ej4yMyGyhrNOsc3zzm5jvX8C1K0/krL/N4tEKy+zrFtbvUzph7+lkEVUY1Khnv\n9dh4wub2yylilBNk4I1L2DV+q+tOCZWmv1QwefdrAPT9KfWtJ2E4xik9sJWAx9/+NmeUR3Fnj1pa\nsdoy2A7Xbr9N++kFdBCxaLsk9e0Bst6kpXx8PWXcMSXeZ9dDKCt2b9yiFCVbq6ad2ZPQOTpmfX2T\n3rTHwZHBWdjc2GZ3vE9WFgwzM/n2s5zm26ckdw+p0hI3NFMl8hwmgyHjfErYEnDRTPY4ikxvSFHO\nO1I/KpnjGiBMa/oMp0AJhHxwsbiejxSg0JSlnnc4uo6PcFwODg7oDYbkeT6/NtLFBJhhptFEEMyB\nU2ZKLktzOp0OXSnJkimiMK7TK997E+pL/N2//av4tZg/+qe/bcbEqwgpkX7AMLcZIBnj11uEomA4\nHs2fY+YSPQzy8CiFH6AIBZgPZdd5FHiMAEe44JjdsjMacfDKnzL1Ba+f7vBX75hF5O8ekXy3Ttcv\niRyHpmd2NO9uBgtNFrMmtXbMYW5TgR2oyYBnl5+et+EO3u5ydG2fpV7GUnMNRkar9//FK0xkQV14\nnF+9QPdVW4rsCeJ6nWa8gJ8Dbxm/deQnZIsutW34+p8YpbC5EcHmMhzcArEKIxPHuP3WOzz/3JdI\nO6eEdXCXjI//vdf6jCeaypcIG1Bc0DGkDr/zG7+Lqhr87E9YA002+drvf5WNM9t4cQ1l0661ep0U\nzeH+Ab4f0miY6s/ReEA1LRAIdGnesXt4yvTWXbz+kEh4RL5RWtpz0FnJIBmTHp0gz5preK6wiNkf\nfX2+QILFqRBIxAwN1XEQQiKkRlsoOum6SCwpsLUkABSSNC+5dWeHXq9HYdOGwjHQbEoLlND3Mk/C\nIS8VDgLPxgi01hRVic4VZaXYXDGNc91xTuU3SDOFdDywYxUEGl9W5EWFaxvvlHMvYxHFMVHdwu15\nPw4I7ccjD5dS+CGRlpneUACxHfhGjcp1uPD0Y5TJMi1bAvvdo5scpqcUkWQxDOlOjemf3T2i2ajx\n+OdfYnFznbJn0kDlOGNz4yw89aQhjgXKP/lj/HOCpfOXodaGa6ZCsbx+TG1rgdblC1BB8qZhe3by\ngmB1kcalbTgYwo4JEkbLTWrnz+Hu9nGn5n7PPHYFECjhI32fN68bvsdeXaK3mxxOB2zWG2hrWp9b\nu8CZ1iWK8QA6My3ZZO+Vt0h6GZ956V9n1DfB1E5vBzGuWPTqhFHAqaXBk7kmEC6jZITvBkS2b+Hu\n/j7SdfGlQ2CnhMxS1Dgn1h7NoEY6A4xSDtIRFLokS3KEDWJm6QjWCj4WpTBDrtL25/weRilooQ0H\nBjMicEWlJI7vo63VNxwnHHd6nHb7ZFk+d1UdxwPhUGlt5t8MXVkJqkqjpSa0i1wKiyjtKPwwmGcU\nRNAg7Q6pb59nfLRPe9G4fMnpXcJ6iBYlrh3rQvjkSuPoEg0o23uT5slHPm4/rjw8SgH5gLUwR2Y0\nIwdAhaZyhGl+kffMvMWtTc6//AJtmRE/aTr0it/7v1jcXKC9tULZHZBdNwv0+PYBqnL43d/7KsG3\nmgyHJqV2fmGdeFvT3lFw5hwArn+OaXKTUvq4cRMsw3IsF6jCJVhaBSStU9tzMBriLa7C+fOQ3CW/\nYSyFoL6Mv/0Ye3/462w2zZC3z25Bd4SsrZJMJV953QDOtp95nLztUYWKouFwxz53IGKm75xyfHBM\nagFPnvy5F3FEk4XGFoGsM8rMxLqwdo5eOOLG6++wdX6TjTMmF94fdRnnI9ZWVshUSbdnzN92s0k6\nKSnyct4OrNISNc0JtUPg+OSW8EYikJXAQ5MlOaGwKcxcUWXFx0QjKUGLuXK4p3gk2k5hzzZ3VWVO\nVWgcz8ULYmwzLae9Ibfv7KIUCMfHtxyiQrooIUFohCMQ0hZG+Q5VURqwF2buiqa06dDAlQSRsZIy\n7RIstZjmFc7CEuPUuI1BEDPNM7K8QEtjfRZSgarIZIk7A7IBtPzkYgY/TB6BrDySR/JIHpCHxFIQ\nRj3ZXeaBBqj7YgquK+xGJph1qewfHyGEIE9T+iKl3TWZg5PRkGfPPcf5p59guH/I0LYP66Ti+Wef\n4zgbM6wKOkcGr8AtHG4MRnRe/0My659GzTrplYDO0YDLrVU2UwvuIRfJqhrpVBDWG/R6ZodWykEV\nAkoJlaRyLKBK5rP3xi5/dv1bPPGpJ82xRg2OUlheZXDjNsWxMcOffvIicVYQxzFR4DHJjUtQoPne\n23foD6ZMSzMgt/733+GJy89w+fJz+EGTZGp2qOHpHp4X8Pi5SwyGPYq+2eUDx8WNFyiSgkrlbK4a\nC8LzQ04OxvSHA6R9d7IKlZYmCOp4BPmsVsQlzzOKPEHlIauLJubRHqUEnvfx9PRoYzdqLdDqfhYm\nY0FoJI51E3RZIaSh9lNK0h+aOoDT7pDBcEpcb+E69+ISSgBKG5IZ6cyPCyERjunCVDMsBDTS8ZCO\nOdeZxTYqB4WLSgtwhEEWBzzHpVIlYT2mtLgdlQgpNQTCRaTevA08+IBl2p+EPBRKYW5xvseC+j4z\npoAy0LgISgu3PphMeWx9Azeu4wiX3CqFpuvQcjyKfh+ZF7ZEGgbZhHhzia3GFiuBx0W7wNyJIi4k\nd67f5u1btwCINtfJT69x8+23uHrzD7mozId96tJjZBcXufnGVZJ0zBm7+Fu1mEmuyDsZtVFFHFsE\nJ91g/6tvMo5h/cqz5ljlgViB44D9bx7wwrJhtjpPm/R0RNQGt+nxxLPPATCdTikSzcJ2MK9E9m6/\nw1e+8SrnNy/w6U89CY6Nj+iUsxsbDHp9GvU63SNT0OXXXFqrLfY7HWTocuYx42qNkykDP8RxpvgW\nwKUW1VGuh4sA6eDPFpHWaFWiVQlK41uQkCAPTArxY7CCtcbwVGoBCMP7gQkeCgRaSWbZTC08HN9F\n49Lvj9jdMxmWXn+E60co7Vi2LvugyigA6Ui0cO4hNVczIFcxB92RwsX1Ja40QcfcbkxeWGM8LnBb\ny5S9E2ozrIaJIo4CikpR2lLsEgVSGBCeJGGSmEB3Vnyw1u9PQh4KpTAX23D0vhNL3nNYE1uK3O32\n8TcuUOQVOSX+lqnq28638GLB8cldGm6AG5h/O8kGOLHDIJtQVg6rTYNjIN0QUogV+MoonBeffpJI\nn0GULuo719j5U+P392926Z326dcKCCV3K2NtTOOIzneGJFqxsrDKT3/+Z8zD1hq4r3Z49guP0Tr3\nvDl2MARnCXYm7L7d5TP/5pcByLUkiELysmBSpmw++RgAu9fuoCcTkgJOJyb1+My5x2itXWA6SNjr\n9Vg9Z2odbr1znVu7O6wstCnLknpgItw3b92gc3rMysYqWZHTuWtiHu3FFkWpKBW4oYmO1+t1Ut9H\n6xSKkrKa8WPkaAxsWqXvAb0Okwlh3vpYlIKxCCQKYUls7TzR0sYDILPt7hKB5/gUlaLbG7B/aDaJ\nUVZQqzdJ0xTHcZnZnFoYq0EKBwVz5SKEwnU9JIrCzgdVKYTQVIXhn4hsPKVKcpyoRpmXUG+T2uY2\nD4fhdILn+2QzmDUhwfFN7MKR876KMAwZfhxD92PIQ6QUPkB4wxGAQiAY9o2pHLkBC402o9GUaDEk\na5gPsvjkFkN3SrAYEEmf175uGn62zq1SlGNC12E6mfD6NwzS8UYVseov4O3u86QlnY0nOR0xYTne\nQE4hs8QsqSvIErh88RJX/vJPQ2o/Z5UzvrPLaJqwNxrzW7//ewC0Mp8NVaf87jHH0vBAus0tFlst\n+q8ccvbiS3Rtqipejhn1MvxaxF464p03XgegPlC4e2NqXsQTy8bs7yZDAjfAbwfowMd1zHO315YY\nnJxSCkWR5EynZkL6bsDOjR0WW23ims/pzj4A/YNDTohpthtIbRbLJE2QuiKIIrKjA/xFYxEMJ2Pc\nyKNUFWmR89pbbwLQ81OWXedjiVLNCF+0dBGOR2jLzZOsIk1zFpotMtsLk+YpfhjR7Zywu3dEYd2h\nIIyZZAVSYxCTI6MoD+7u48YRUgqiWoNBxyh4v1YjGw9ZXlujc2S+b7PdpMhS8jyn0aiTDy3beCMm\n6U2hEeAFEcXY4lrU6qhUUWqF4xsLrBT+/L0cx5nTAMx4IN7Lg/H/W96HeezgPaKweuC+XKSUEgHE\ndhG5lUSkFa7wcBIoKxOZ9z1Iq4JkOMVVgnHP7BhrZ88TNAMCzyVPxojSfNikN+RwsMfezh6t89vm\nfm7JqB2wnBRM0oShtdtHocc0kiw3mtCq0Y/NPdtBnfr6KnUFq6LEvWXIYY/++G36b51w0r3FO/vG\nwtntlXx689N4Q5dso02wZeISO/0DVj0IShdnLSKxUf8wU6xHDSbTlGxsUmGnWUqzAYEQaFUS1M3z\nrWwvoXXB/t4Bj52/yOmJcR+UgkvnL/Hqd1/jyqeeQdp0mIPGbToUlKiZT+1BqSuQEs8TqGoG/a7Q\nuqJQBSXKmNcAjkslPx6lkJel6XNAkGUZ2i4sL4hBuEzShNljCOnSH4zY3dvn4OAQ1wLj1KKYfJrR\nqMV0b98GC13nNlpEQchoNCbPMpq2LkRKyAc9hsM+s0BJmWdIR9BqNRmNRsyiAK7rUl9dYpxB0esB\nxhXIHEGR5niBj74vYzLvxkUiPx7T6kPJo+zDI3kkj+QBeSgshR8kivuqTmf8HFUFjjYVjdZFa3gB\n0ompS4d0nPPNf/LPALj03NMsb63hhzUCT+JbiKxWo8E0m5B5Ht5Wm+fP2ipAVYPvvMH3kn2mxjqn\nHY1IF5okozGHyZA0si20zZgylLDQII8kx3ZnKKuE5X6OqjSTs+vUnzIIzZMbp+g3Onz6F/5t3MuG\nkerdmz2eKrfY++O3OXrrDo5nLIXdzjWo1YkWG/ifeYLcMf5svzdlVURUkaZcMs+xPl2iUYvwPUE2\nGTEcmbqDVr3B6sYiUiiu797k8jkTl7jznR0WFlpcPH+J17/3Bp//10zbSlmmhLFpQ/ZCc+0w9Bmq\nDDRIV5IWxhqSUqNVdQ/mfG5ZaLTjfiwxhVkbs6gcRFEwg09o1eugBb1ul3AGrmqJaKNajahRp7J0\n9UWlUEVFvBCx8dnP0mqaYOCN27fo9YdcufIp8rJ4oAMznY5p1GOCFXPuaDggDkIee+wSuzu3Ob1r\nArtHOzdxVs/iRU0KHRB4xpIJyAi9JkmW36vCvE8eoMt7WFokeZiUguIBqihTs6SsUrB0bZ6DFIqy\nVPjW913yG4Y2rapwxiP6N8zC8C8IGnkIFZTplHRkPnajvYTTatCvJnR7hzQrMwRnZIObgz2ulV0e\nP2syAYPtGo6qGIx7DIZdQjvBXDdAuwazYJJPaTeNv62PD/nO1/6EHMmC/1kam6al+tmLj9GtH3C1\nf8TTK2YhXlw6z3K+TeP2lMY04vxPvgjAcHyR0VvXOOoP6O/0WXjcKJZ6HOMNRviLHolvTPmz/lkm\n0wE+0IxDEgsHXPM0zaU27VaDo6Nj3rz2FgCXn3qCW9dvcPbcNkHkc+2qwZZ86eUrnPgCUSoadROX\naNYzRrq0vLwaZVOjru9QqtIA3SiNY7+N6/pI6aLFR68XZjRtuqpw72sx1lpTazSYJpP5HDntdRkM\nBqxubrGa5Bx0jKtVlIqo3WJzbZ39o8M5h8eFs+dQ6g7dbofpdMqFSxft+7hMxsZ18CyRQ55MeenK\ns/zqr/4ycQj/4H/6fwH49d/+pyTZhKC5gNYelWWO6o/7tGr3YgjAXImCRGqJsMF1qR8eo/3hUQrf\npymV8eTeM8NcTEoosJHfoj+Fa3fAc0iKCXLBpNmCxQ1orIAumQwzcMzCFX6DUgRIT3P71lv0rt4G\nYDMPKI77TI/6LIUWPEO7kIxIRz2m4z5LpYEpdiYVWgia9RhUSd3W0cd5xZ9dv463vsI6itAqopb2\nOT7ocG5pm4aN7o+TEgYnnA73YUEgN2x58djnzLmfpP/P/4jdu0c8ftnUNSxkAn31XeJVSHdMfKRa\niRl1DnF8xdJ6i/UV4z/X1xbB8ZiMTtk+u8n1d033ZKYUz738EicHe1y6sM3Y1jWcdLrIqInvSeqW\nuCQKHByVUWU5TgmOBdWXSqClxtUCR0h8xzy3Ky1z88cAsqKpyPKENNNIJya1VYWD8YC4UUcIwd27\nBhVrb2+Po5MT4maLNMtQ9mHieo16e4FpMmH/2lVOl0yPzIsvvogrYH9vl6qo2NoyHa9H/X2G/R5+\n4BFafoc0maBUCcogiz/3jPk2G1/7OjePRub3ZYm0Y+VIgee55GV5rzRbA1LgaIHS4LyHvPZhkIdD\nKdxXoDSTueEgMPiJQFlUCN/DlRjKd+Do5h2K13YRtZhJ3WU8NIN7uDsGNWChXkcNJFtNU7oc5iFi\n4rLcXORzF18gbBhSkKUxTK7vovZGbE3Nwo0OErIwIwvAXa6xmpgy52Ko6FUKZ7GGqrsUtjaCaUq9\nERJsLVBrhEhb1EhWYQAAIABJREFUQs1JSpIkXK7XcCymYRA4lJN9OpO71C5tMG2Z4xOvoDkdczIZ\nUgU1qrFtwrq6z8KbuyymdR5bs8Awwz3aboXKckZJj2pkIuqRKJFxjXw6IitSnvmUqY349re+R7O9\nwMa5c7xz610uXja74v7du1AFBG4ApbEIVDJBqgpZKMgrfNv5WKoCIXx8KYikiyeN9eShcJT4WMqc\n77VWl3iRQ2YbCsvSlCEPxiPevmp6R4Q2xwtVkeTZvBEJKeh0OgxGA84/+RTbZ40FlpcVvW6HOIxZ\nv7jJeGQU5d2dO2gUcbhAaAuLPM/h5vV3+a//q/+Rk5MTti0s/2jYt5yWOUhBFBrFGgQtqjLhgdCd\nmHVjyve4D/+KWApCiP8S+E8xr/k6BmRlA/jHwCLwCvAfaK1/eGXGfTvMAzrzvphCqSoUHqoEx9ao\nq6Tg6PYxhCFZM+LyxGLuf2ufHe6SLi8yHfVZtZDw8uYYR0ekccJmcxGxuGzvn1KLEsLSpyZMMVLk\n1Bg4fbwLS6x/4TmWhiYynb2+ww3RY7wSkMcVy9NZNZHi8Z+4Qm+jSdKQFAMze7PeIY2Lm4zqGZk0\nWIzRQp2h6jNKDln2F1BTo1jc6ZBq5xR1MuD5z79IYZs4p4d9Pr15HpZgc9usvOktSdxqkjFkf9ij\nPzWu03jUwVtYxG0sg67oDcw9H3vicd5+9xpf+PxnaS8tcmfPpCTX11fpFAnSc5n0bKbi9BRR5Gay\n5jnCTnSpFK4LkeORe/68qElobc5V8NEjkGp838XJFQiBtIVHju+ilGJ3d3cOr7bQbhJEIe12G+/o\nhMQyck0mE6rJlGYcopQiS8y3qTeaNJtNOgf7nDt/htRmWRq1GmAIZWfs16HvUa9F7O7ukqcJd6wC\nqdVCUikoXI+i0kwsRF2hEvJhn6h1j5X8fhFazJXBw2MnfDiI9y3gvwBe1lo/i5kKfw3474D/QWv9\nGNAD/vpH8aCP5JE8kk9GPqz74AKREKLAEMEcAD8D/Pv29/8b8HeB/+UDXU3/uX8lDEOmQFVUeLEx\nlTdWVklPcqbKBJOeODYWQXI4pdAFm5fa9PsFNMzVbhy/wtGCJlkMiFs1mhYZeJuAxbCG02hC08QU\nxo5kGiqW19ZZbG1Dz1gVQT8jH0zp1GHiJDiWZcrPx6w+f5kTL+OkHOIOTMDp+NqbbIV14qfWOK4b\n07/mTmFyRJkNWfEcPMtMJDtDnKxB7XDMOjHfvWWqDv1RAp9/Gcavs9s19Q9ngiuQFlAmLIU1atYa\nutM7oXtywsJ2QdTaJK0sL6Hj89ynrrBzd5eFdgNlcSvTLEOGClWVjCzKUNHvIlQJwgddMSsEcEpj\n0nmOiycdXGvau4j3rTf5sFIUBTgG8MRQ0Vui39ghLwp2dnaIbct8URRMRwOEEIaebWQLy5oLBEtL\ntHyXO3fusHP7NgBPPfcs6yurjEYj4jhmd9fEX5JkShyHuI4AW9HYaDQM4cvxEVsb64Q2BdZLSnav\n3UUFEseLILG9Js2Q5vo6k+Q9ACpCILUwLvK/SjEFrfWeEOK/x+AwJsDvY7gf+lrPcNO5C2z9sGtV\nAvBgRhDpVArfUeRoSgSuNbGqzpB4qUkipuSOiSpHP7fIU3/tIr2j29w6ucOfvm5Muq5XIR2Pd8d3\naMUlpY1Oj+sBMhU03+gRqgmHNjh82/NYrUIupueZ/oYBTYkub/FUawFxeQlkAaFprjnwuzTdhJdu\nKXi3BoXxT/f+4HW6xyUXwlUWT+8ynJjg10azBt6UgxuCJwMT26gnGySvDymmF5B7HgzNNWK5xU5R\n8e7K02xNJU8NbS/HFlCegnqczf5nzEPH1yBLCfKKYFLQ7BovrSGadNAc74yYrAyplkxKzW/79Bnj\ntEscf8BKyyiF/tE+zsWX6e8csB2aQG016ZHrBSovohRTgtwoYVwJhWSxLimPx6xWFqlJjRg6R+z6\nC5zho+V+ODtcptaNmYZ1DqQmD8xHGyaK6Y0bLHprc/dB111Ydnmn6DFdk5Aa1+niuSWyzh6LZz5D\nWAuILTw7SnLt1g6N5XVOPZ+BDSrSbDHAIS0c2rZYKnJcbt8ckHsRRycJzcycm6khC01Bnp8iJj6h\nff+o7yCdClTJODQbROFKcGpMZAW1KactkwHqtDUcrXyk4/bjyo+tFIQQCxg2qAtAH/gnwJd/wKk/\ncO+4nwzmzAckg3FaZgf3fZ9yRhme5+BIFp54gubWAhcDk2Vorq4iPR92TxleP6Brq/omhamOGzsV\njpNTsxh5XpogdcIohTQzEymuBrylRoSvfRMvFEjb9DhVE1pLi3DcQRfRHAlpMOww6o3opAUBBe22\nRewhR2qXV1/7NpO6UWZL/SaDg13UQsGJP2JteBuAtLnIu0c3IBiytOpwPOvALCeUqoOsC6ap3bny\nzPBJKG3KFW2yWyKIHJeW67PX6SJcO6nbiwT1GtNpRlFNCUOTflw+u82N/pBykjPFTl5dkPiawIdK\neNQatoJUOugiJydjvxgTTY2irFxNJANqfPTdfsrNGed9iiAkjGL6QzOGd67uMNkb4CWaWs1khlJd\nQQWD4z75JAHLxD3tF1RjOHn3EKErlE3rVrpkMOxAMcKpufeQl7RCaI1UBakyyjNXGlVBWUGpIZkF\nX+WUXKZorXE16BlcHAJHQSEdlM2WoT0ofRypUHlEnNmS+jzmYWmJ+jDuw88Ct7TWJwBCiN8GvgC0\nhRCutRa2gf0f9I/vJ4N5YUYG80PcB5RiPJkSRCYfDpAWOXvXr7O60eY06yFqZlJKMYHxGOqK5ucu\n0BwYnEL2uugk53S7gZIOSydmUk86A7TjkgPZ0JjyejCgub3NJBvgBwFpaY4XRcrJ3phbb94k8he4\nsG1SU0cnO+RlhpQVi7UQHZkPXk00tbDJl7/4HKuh6Zzc+fZN3vzuDhVjptOM771lwFQGWtDt5qwv\nLVM769Ky9IbDoz5T4RHUPMbWbA3THKErpIURU3OIMUHk+CB8Jqoit26M2x/j+22044EfECya52vU\nQ2o3h0jHpyiMwjlVBb1I03Mqpn6OZwlohNLUpUMUBGSui66ZXbEWSoJKEHwMvdOZGKGDFHyDUNTr\nm0EZjY9xHEGjVaN/YuhFxkpB6JDsdUFXCNdYSWrkIvOQzkEHP3YYhuY5/aZDWJek2YTp8T7u1JZz\nSwchoRIV+cy0rxyc0kcWHkHlMmLGTi7AccBzKNwQWZjNoCo0jlbk0qUQVllqH1SAX0icpE5zatLI\nrWmL/pxL/S9WPkweZAf4nBAiFsYhmvE+fAX4RXvOR8D78EgeySP5JOXDxBS+KYT4LUzasQS+i9n5\n/xnwj4UQ/4099g9+2LX+vBDLAwVMAhzPRTguoQUIOXf5Iievv8px75RouYa+aDRvfjJl1NmHZp2l\n55+BofVzqxOypGD5xecgCOGG6bcfXr9L1a7jba5QHBv3obh6l+P9DOWmdJNTJrkJWpV5gRoD04CV\n1RXe+LNvm/NlRbjgc/mpi9Skx96bBlo9yB022guEWQtcm8POcybjLnEr5PmXPkt01uS8r+7d4Q//\n4KucefIiNGBx2ex0nd3ruI6g0kPTzgyUoo4UCq1MqZdWxrYSGgIp8VyHRnuJw9SYv5ODU9Iix1kK\nqG008dvGUkgcRdMJcdx4Tls/ciHbXIB2gwkKYYuaVJ5DWMPzApw8I7UboKMkQQ7upIT3VvF9SJF1\nl9Zqm34y4q133mLfAuN4ok6j1iDpD1laso1PWhCtLNFNp2gtWG2YdGA1nBLVmyw3FxgVfe72btjv\nIIiWAsRYoPIMUdgxRKPcksqrUK495nhI6eHKEK8KSBwTUCw9jfY0uBKtXVJrxkqlgBIl1b22fw1U\nErdwCbKQODGxmjBtYJJ1f/HyYXkf/g7wd95z+CbwmQ9z3fcVKfHdkIpyrijamxtMd24R1iSLn30Z\nVbfn3jhCZy7OSgCbEkoz4J3wlKIm2bjgGFJB15r4wqV2dhH/ymPQtcUBmw6rN+twtg3pHof7ZiKd\n3D0h3c8JJw2yYULTN9HwtK2olhIu/OxZSDTfff07ACzrRYrMxXkrR6Qm+MjekK2yBtOQlXITovMA\nNIoRdafOysoajBNoGncjT2IC1aYaJjQs4nJVFWgUFQptANAB8ISDKxwMWWTAph2rkySnd9wlJ8Rb\n8slt6XJ32scfO5RZyahrAnbTqsLfWqZxboswctGhRTYqSqIKpr0h3Z0O42umJd1r12hsrhP48Ufx\npR+QXlrRHWUc9QYM+hMaLaNArzz9Kc4ubJAeT3GUxUfAJVpa4ObBIck0ox2bWMPouMdCWIPVLQ5P\nbrKTXAUgyUdo1aIQDo4McGpmAuV5jnYStDNGYWkEdUKhK6gUqJj6DPi5rMhldi+2YzEmDGJTCq7p\n2QGgCkGURolrgZgReqpHyEsfWO6D4oeqotQwSkc0c8uOPByQq5JknLLY61Ja/7l7uMdo3GWlIaF7\nQjUySmEqx+RaMT66TTCtMRybXafHCTpzWOxEZjECE6eLW+YEFy9Cu8H6TRMKiuoOrcsbED8JexOG\nVlncVDfpL/fhMpC6xGdtxP52RG9Y0Pndtxh3jFJY9CvOyyZF4pB+o0PYM+cOOxNW6luIsM5xb8yq\nNAVTuFsUgzphqfFtTGFS5ZRoNCVSCxzfBgMdD7wQpAengznF3mYQoasJ+50+eayoUnO8kw9YLpah\n1PStUuh2uviOJu13yRNp0I4xc9uvIDvucrpzl5oFCdm4eAZRfjxdPQvLm3R7gp1bfSbdClyzQ39n\n8ipv8AZqlBFI2079/7H3prGWZdd932+ffaZ77nzvm+u9mqt6qOq5m2OLbEk0KckabCcKYAQypDgG\nEiTIl8CwgwCBgACBgORLBsRIYjiWIUexpFgSZUmWKIqUmyLZZHezh6queXj15nfffXc+8zk7H/Z+\n1U2KFKl2t1QCagGFen3r9n3nnrP32mv911r/v+vh1Gps7e6RK6gbIdl4PGOh1WES91HFAMwAGrKk\nyDKKWGj6PGkajewCxBhUAsp0pqpcYwe+A0rgmaY1VcbkKkH7JfEuWGmVYMUgU/0H9Ex6aZPYFpYd\nk9jpu7/vAbEHyyl8vzVlS4SASiXAdbRT2Oof0Gy1yMoZG7du3C+RiXhGRVrI3gh6V4kM03FRJChV\nwtYOqS3JjPS63/Rw84j0zbfvT8rltkW13WL0+h8jWgm7h7qGnQ1Tms0OxFPwbPzaEZNwzPLTczAf\nQuLz2EuaZan3B1NG1woyYd2foAsIaZWKcFyw+fWrjG5qPPZu7ZCFT68y9CDNclKjkmQvnWZv84AT\nrgQzDOa7ikwVZChKybs8hY6LsIx6dplDdqQo5dNAsDeNGG+llKV2RMIVRBKqvocwOGEexiT9AaMk\nIS6S+5yYtrDwbIcsisgnIcpsusD2EfLDWU7hFHY2Ew72cqTdojQDSvFwTFwWSNdlHGqnH1gBMozA\nzan4Hpnhy8hkxERMGN94HYL39B64LiqFLHNB1iEyqY+UIHIELp4ZniqtnNTNjcMocMd6IwuRI2RJ\nJqBUJdKArYIMJRJSFWqGKP1mkJDjMHNnzEypcuonH8q9ez/2YDmF99h3GYcAIMkTlP2uas/e3h6n\nHjkLbZ+d9SsMDa169fwateUF2OgzvHST0EAK3YunaCwuMn7jGmWcIxb1om6vLlKNFNmVe/c5+Zon\nV6C7yh/9yr8iOG7zxHOPALD6iY9BfIzJv3yN7Ws95IK+jQfVfR575jzM56gwY+6TeuZg9PZ1RnfH\njDszVk/qk0hMphxsbeMEdXI7IDMbqr5cIzhZ47ARkpQOhRlZbqx12bh6ldV2FVnohSTxkEKiVEkq\nLMojyXRp46gSq8ygWQPTkESY48iCaq6Iwww10Xc4aPnskVF1g/usRjWvCq5PKiR1wDGOSBYaQJpm\nCgeHwJTaqn4V6flEkg+4SwH292E0SMlDCyvwwahVFU4GroVbrzA/p+c42s06ZV6A0JRxO9t6vDku\nYsb5BLoriDLEmZoZlFCRRSU+HrZVY1pq7AQhsZWLX9o4Bo8vhKLMUnI7AcuiNLJ7FAK7EICE0kYa\nXIKyoJQhjpUxuQ/pxyAisDJKJyE2h1vkPCgFyYckKw/toT2077AHNlL4bpaGIV4tYKZmTMyQyvLy\nMiwsQDljeXWVdKY9fRgE5G4Fe3UVpzcmN60hjbNnSKKIxuOPsHnrFqKjQSux0IVhQtGsM5vp3oXG\nyhLcTjl+bo09a5NXrnwDgMlsir9VxXdqPPLpp/jmpZcBaB1vEizX2Zps0/SPUXP179xOB1z4+DPc\nPbXO8oI+Xdqt03D9Fle+eoMSm2PndQNXsFhin/Ron29xb2/Ezi190tmVFpXVKtduX+PxEwZniARQ\nID2bIKjc74vIyoIszfGsEqeMwFQrSBTCtaiXkskkBkeHrE2/zXUn4fHFVURd9wDslVu6McxxCGyH\n6tH48GRGFGckWYIEskR/R89xGUUzbOsvHikcCcseMSkfvZbnOsRXBaTJDGmVKCyy0OT4DY/OqTX8\nukfbdG2WaYZdKlwhieMYywi20GjCZAJ7KV7FpSYN43IaIgqBoxRqPKBm6SpGnua4doIvLSyln1le\nlhSJBalLbnkoA2KG6ZAktfBcD1/UMbw4eKWD4yjSfEKW6+guziOwfHBtTRZr0rUoSZFSIqX8ni3P\nZVnenxj9MO2BdQrvpWY8Mtf3STHousnz0jSFwRC8ksN+j8tvadXpJz/+HLbdhPGYg6QgMxUCJoo8\nKSmyklGpqBg5OcYFKJfMqzDt643Rv36PbusCGRlPPf80Vksv2le+9AoXW09x6od+FC5tcuWPNQL/\nYxc/R5LHtBa7WKXHlbsa4d4M9zlzUrDfHoBRiLJX6nhzC/gdi3RaxerqBVarFci6RcWGthSUwuTy\nVozTdMgDxV5fzz4sts6STiPcegC1BiNDFx6XOc1aVY8ypwm5ASYVBZSSihB0lUtm9pa1PcG/MA+W\n1GxKgLAlouGRBJJpXFAa7stEJUwcRWgL3KTENQ1TeVncZ0T6oK0QI4qihypGCMsH32yMekBRl0SB\nw/WBBoyrlo8dZYz3Dwn7A8iOWJQBEbBSOc4kPmRssCSnUsduS4gVVnKIneub4gKFSoiJKQw3pxB1\nXFUhUFVsKuyYzldZrYEUhFFOElnUczNRmihm/Ri3VeJXjRan1yS32vi5pEhd6kI/9/lgiS16H84N\n/Avag+UUvt+iEhZJEmF78j7hpcpyDu/eJWg4HPT3OHNM55bq1oD9N7eQdZ88U6hQn2jTV2/jtqtc\nP9yhkIqWmZex3tmjN5synkywDftxWySUB9dxXZfpdMojFzSt2QsvfYxV7zhYU7andzjzvKZYW/js\nD1OGVxgcjLClzxuXNNOxtzhP2slJKwX7ZizEUi6NboPJeUncU6RGXt5XisbMxr+X0j7IUYf6/ZUw\nJvArhPUOWzt6NmOxleB26xRSkE5nOCZS8FwHVWTERYLIYz23Aagih0Li2T4dx2dq2qWj8YTq8yew\nyuL+yZ9LRVJ3GHqKIo/xhenmtBMiXxJLRVUonJl2CkfO5MOoP0jvEMvtI+0Jjt9ANfRGEl0PWh6R\nbWHX9Mnf3xmg7u7A3kiDH6Yig20DkmSakipJKvXGTcschEIGOa6cUkT6+wdBQExOWL5nFlw5iNyi\nkmS4cQkrBtsggaQAKnjteeYdPT/iRjPCkc203MYyvsnOKwjhITKBmNmIsSHXGdsopb4tWvqrsgfL\nKXwX+7bJO4WmAMPCNZTZS/MLjK5fpSwki905mic0GKjeuM7O9iELH71I97GTcKBLbfHl60RRwsUX\nn2cWTSnuamAy2j4kmG8y/+ijYCTdi3fuscEBVc/l9rVbHHtcz3atPf80jEpGb14hqkeEPdPXsLWF\n1WnSyCWjYXFffeqF80+xPdqiklbIzJfZ7gl2wowsETiFANM0FKSSYCbJZxPswQx3rMPONJ0hkZRO\nldDWIe6l3joXX/g4MskZ9oYsmUYdYUkOD4cEriBNY6T5nUqWUChsFIHtoozqUx7PcJXASgvEUaXC\nEsxsxQ4hmZoSCH0dlqeY+YrEUpS5op0ZLUf5LiD3gZs7RtgTlDWiEBa2qXiouo2q6RlV5+g7zkYQ\njSHQjq80KUicp5RSEDOgPtfBNuxas+gQsjGFlxK5MzDy8jPLgcIFAhD6fiMrWLaLIxWOmwCH5voc\nqNcgl4T9Q270dSXEwaJZVYhGnVjqSCssgdKibnn4dg1XaEcuM/nQKbwvU4rA91GUYEp7jaVlops3\nUFlO89TjHPWZiLMLOPmQcbWg6iXIM3rsuRhsk4iCgQyRS1VUpBdHGk1xF2pwfhWW9WkhBxP8jR7C\ndTm/eo5GQ3/G9pVL2LbPwuMnaMx3yDy9kW699gZnnnmS4Ng5vvVHv085M7RcVUWUTalO7fun3HTq\nMInAdgQNT9I1ueViZlNuzBjvDJikM2Khr2VczLBsB79WhRN6onL38DL1yS6LrSU6C/OkRni2jFOC\nEnwpyOIM6ZuoSgpypShRuMKi6uqSpPIkeajVoaqmmmADcRYzKMeUKsI2zTfSVoSyJBElTdu+L3cW\nuA6OsD54fhUgzGPiIiHOM6xcN2wBOK6NVatgS4tsR88+4Gbg51gHA6J+RGmEWUoyhGsxy31mB01c\nSz/LimySiSp5WUBgwaLpLQkjzZIUVZCxOc3zEKH2mFojkFM8ozWR5ZIyqYC1gFVfw60amvgcxmVC\nmu6BNA/YqkJpMSk8yEeEBu9JZf5dncJ7dR/+ssarH1YfHtpDe2jfZg9EpPDnBUzWe9+QF0jHJs0T\nMP385cEh6SwiySfM9w+ZzOmTv6f2uSMOaBc29k5Eo6pDzrvxBnNrK+xvXePi00/QeUKnBGLOYzAa\nMVm/Qq3QvnLkTRne22X1iTWaF89DoHH1fP8OVrWEWsbscMwjT2qex3/7G19guhfz1Esdbr9yi5Mr\nuqIQpRPySsFc2MK2quZ7VRFlTiChWYTImQ450yEc3OsxnqXE1YJ0TiejUzdCBOB067QCnbM2pi2+\n8Sdf51SU8NypZ0gmOl3xMoEVtEj313GdAss0dJW2hZAWyoLSsu7zF9ZqLul4httoUjM0d25ekEcp\npZ3glCXBUQNPnmELRSEtXMemakC1qnSxPhyKRnIVgNXEkhZC1CmNrLtba9FaXqJasdguzWTrdB/i\nTcrBNqvtOs8+rzGmucUGqUq5ms3ovzmmsqmv+1TlDKWs8+b4LrfyGBo6nKeS0k67nE7nWcx1VCqz\nEbPqlMlyTDwX8YlFXbkaZjk3+zOu7lxnFg2IvZP6M1wXij6sSmiZKkhlDuIa9DPIY4YVfd3jIEQd\nqvvRwl8l6coD4RTum/hz/xMsC1WUlEUBBhC7df0GddvGtits3bjJ7FBvuju9HehKnn3uApQwijWq\n/A0mSDXlRz79KfAcMCFg4/wytW2X3TeucnNbD0nd2lzn3KyGk0vY7tPva7T5+FOrhF7C65dfYbYb\nEYy1w4nGIYUs2f3T66x4K5w88zgAYzchToYE5CgD+jlZSsv3aDolxWiH3qYmHh2NYJIorJVFirWA\nZEVfX9mt4DbqiHoTWdHh6fHoBLd6++zfOmBra5PF0je3yYNpgj1LsRo2hcmrC6tEOB7SdhCWhTKM\nTNKRRJMewlvAT83wzyxB+QluUFAtBHWjm1FkJbnn4LsOTcclMErcXqkolfhQQk9hzSPtBWy7Qi58\nlBlwsbw69c4cdsdhOtQVmVF2CO6Yz3zqDD//N17kRz6mG8haHYe4DLnhxYy/OEL+oXYswfVl1rcU\n7iSjn4QMXV2Oxq6zopb5nDzPJ4xGxFxjinrygPilMfkzBZ+pa+dMtcpQeXzh7T3+ya99nS+9rCn1\naXRpPn+OzoVF0qZ+NqXbRWYdnF7C8FsJ3nl9HZNaeP/7PnQKfxETFmWeavluQxAynUw4d/okNBy2\n71xm+5uvAeCsdlg5fRqqNTiY0FzUfAoXHz1Hq9mEMIPDGQXaU8vlRayiRG7u0031JvJOneQxf4ne\n4QE3vnabW4WeW3hx6SXSRsHG7gZO4rA6rz/7ub/3WWhd4M3//VeYq3WxTGfbyfPHmbPnETcOmBom\n1jxV1GoenYrFoByRzDTgmRc2otOh8XiH4pEq8ZzOleW8hV31SVEclnoBpbs1fvinfpxv/fafcP2b\nt2h3jGZBZFEe7GOtLkAxRBiF7iID4fpYjo3CoizflVhP4xiV5mC+ezkLwc9x7RK/FHjGKZR5QU25\nFNKjVrq4xg1YpfpwwgSgKGtAA0VJmuryIUCSKpSwsD2X9IjhNh1y5swCP/93foL/+FPPwUzzVGxv\nvU1uxaw+epLliyuImxqUTV/NUfdmPG53ud7MeF0Z+o9CsMQcF4tlPlno6LMlXegUqMd8slMCLmkd\n0tF0gn3iUf72R38Ib3GNvPVHANyaxZz8xDnSRUlPGh1SS1FzA+aPzeNly6i23vxb2RaLqvMQaDyy\n++CUbWreokQBARKZWffVoJDQty08y8aLNfLbWehCp0YqhuwtSk6tvahff/YcTjOAKAJVQkOj5yc/\ncparl95gJXOhO0d2qDeYlQwRFZupJ8gNi8+ZT30ErBPMR3tYb3wT565eSOM/cDj78ee4s7/OD/3t\nJ94doslOkXx9wmHeZXXFo5wzrERzLYZxxsnxecJCV0EsK+a4b9G4O2D3169w+rh2LG82R1T+02d5\n+0QNC5umiZeOT3O6acDEKdkp9EL/vfaUF1bOceoXnuGdbJtXX/5jAD5WPYW7cAz6MwiaWFKHub7t\noEQVG19HXKbiobKYrCOZlDmhmRGZKchdj8IXZFlJZADPzvw8U78kEoJECJRhnVKOQykFH8ZYz4vr\nMWu7Bxwwgk4bloxWY2ebxFqg23uM57b1nIm9f4O/+9mEn/iMwxUG/A//k+5beevf7OPtp7zwn5X8\n1y/6nD7EmYhTAAAgAElEQVStK0zl8wG3w3leGTjsLtwF+WUA2sLF6VcY2E9zd1VHmfOPvsyxn3EI\nzy7zP//WN/hv/6lOYReKKj//jMU/+Juv8dOfBH5eR46/+HKF3eAp6NnEb7wCQP/6VylPzFM+1mb+\ntMOuYfz+5NJL2PbdPxMhWJZ1P3L4y4oeHgin8AObBGlJIhXhi3fVeikKoiyi3W6zuKoxAr9ZoVAZ\nSmQIX9xHfwOgXamx29tlyfbw24YXLy1JeyN2h1MqgRGOGSZQT4l2DkgTwdKikYELLb7y5S+w/PQS\nFIK9u6aZSKzxhd99lcnehLWzpzljGJmu3LmD151np1awa5iNKnNtNpMx5c3rVNttoqE+6ZaPzdOs\nzDNIpiR2Qt3QmzVtnzIvKByLwNHhbOL4vD19h+BgSvDEEuVEf8ZX3rrDWjbk3IlHGezv4Ta006pU\nmwgnJ4lHlGmGZz5bVjyiJCTLEkozEVWKkrJUqFIghIVr6v1CKKQUOJZECnG/G7EotGryh1F96Hct\nRk2LKJSQlTA2vRQHI/Z2dhnbAdOJTvmeuLCC/akalyn5w6uv8huvfRWAdM/lMTHP53/ri/xo0+b0\nio6q/KUK1VYJhzPStARf36soLihyC2VFOEYusLXchargzTfu8n/+k9+A6KMA7Pdzvplt89RCg7nH\nzlDvLuj3z6dshkN6d3qwfTQyv00vnTCw2oikg5jXkYH9ody592ffNwUUQvwzIcS+EOLSe17rCCG+\nIIS4Yf5um9eFEOJ/EULcFEK8JYR49sO8+If20B7aB28/SKTwz4H/DfgX73ntHwNfVEr9khDiH5v/\n/kdo4tZz5s9H0dTuH/33vsojiry0xPEtciHwDKX3aDBgMxpzKCecfPEi7krd/E8FMpySuSWFL/BM\nJ6HIFAtejX464catmyw5OrTuX98Ey0fW2tQ8nUOG1/rE7RRmEUUWUG3oqOLytde4tvs2L/z0zxCN\nExY7WjPyT3/9FbJJxqmlY4w2+nz98zrMX3v+UfZuDukpRdbQJ8LEiqklIdP9PS4qBxEZgpR+wUpS\nY1r6ZBJaxm/LeEZvOmAziRnW9HunYUg5Cmk5LqefnKe1qiOI0aML3Li8xY39t/jYE8/iGrS+THLS\nyYyK7SK6zfsVnLi3SeKmxEVMVr5LaKqwoLSwpcQxk4mIEltKPGkhBRQGr4izFCtP8f69H/aftduN\nkJ0gg0xpjkNDxmrFUKQJPWuC/7gmoxm4BV9vDflGcovXxiFxw3Q0tqsMRgllUqM/gbzQ0YbtTnGq\nOdUgxiosKPR6iMMEV9Zw3YJU6FRjTISbNdndt9m9LeCYBp49r0KRRez1CzYPBkTHdKTpNWuorIBZ\nD6aGVWk4gjJHLVRxlyQNV681og+e2/L92vd1CkqpfyeEOPkdL/8M8JL5+ZeBL6Odws8A/0JptOTr\nQoiWEGJZKbXzfa/kB9AgPAJhPDxw9M/tdpvB5l3iaoYUFn10zj5f2pBMUUGFDBdVaOygEllU3QqH\nXsrGrU3cXC+OzddvceLCkzz59EcJTJlt8vp1DsJ1Llz4GFShd0+DUJevXOLip07gtav090PeeP1N\nAE4sPcM3f+P3+Mx/9Al29m6yfkd3x6VzGYEf4PgZjbpeBDfu3sTHp4rLdO+AR5c0pnCjNyC63Kfx\neIuwbrF7oH/naNgnalgMj1XpGZKe00EVt9lFFIp+Atmcrry4c48RB4KNP3mT/csv89z8GQBOVeZx\n8xJRFiAtpn1dwry1fhvv+FmUVISl6aBUqZaHs21saeEYZiOBwBYWhdCqUEdDS0mWYhf5h6J0NLQn\nzKwYsEBVqJgqi1taoApKkbDyzFMAZMMJl8INsKYUC2sEzy7r6x6Pmb21zmJwjHHmsGkcoh3tE+YS\nSxUUSQG+dvzCypGqAoSEpV5Tu3HE8KDCcNJiLrjA1uSGvg61gmMvkCqHzYOIW452FmM1RyYicDJs\nc1/zNMPKPFb9eVaDJdYa+vpk9Ndf92HxaKMrpXaEEAvm9WPAxnved6T78P2dwnfYt0nRG7M9LXNa\nonEEgNMXLtLPYsZeRL/XI1vRC2a+0gVHgCgQFPfbXSlsqNQJN7apujVOmDl85iJWWsvIlTP3T6K6\nu0/dC6G7Cu1FbnxBT0k6geTFv/E8OCW+3aBhKiG/8+t/yH/y936O11/+Gs88cYGTJ/Vnv/Lm2/jN\nJivn6qiePnVOqgqtXFKtzTGd7CEX9GneEiWXv/g6YniC+vllWr5usW11WoSLDlbNYmraNhfHfcbp\njEGUMEpSEsNE7IewMjfHSz/9M+Sv30Pd0pt//eZNWlaFRqtJpAas93VlY2s0oNluYDmSxDAK56LA\ncxwC18HH4Wg4TyiBVGAXgjItSUxbdFLkWOWHI3/WIKFelCSZjW+5iEg7qGyWko6GECtufut1ANa8\nIVVXEE8Uk50h4Vt6Q7MeUU8TBn3F5kHM7jG99IMyJUom5GFMXsygo522bacU05ScsS5dA0MZsL0+\n4bWrYw42ImhrHCMsJSNZYRRX2ThMueFobKef1pkUE8iGlDPTEh1GVGSBdVgQb4/wTZt4xTmSFPqr\ntw8aaPxua+L76j4c/wF1HxCaoNTB0f3tAGVJlqbYgWT97jorpgWY6goEApGmeG6BiYr1YIzroSYJ\nzcIhM7RudqNGGmdUNg/A1ilIrkpq5HDlBrgWb1/WvQQf+ezjsBJANqHaXObEmn7/4vIh79x4h2ef\nf5r9jT0C4/1Pds8ymE2Irtwi8vTueurFj6HCErkoUfP73NvTUUXeDlBjGL3Tw44k585q8ZjWYoO9\nXsz44JC6IQz13j7EK3MCS4FrkxuBEy+XdGmwVDRZfOwMk3tfB6A/vUO11iCJJLeHA/qGlKV98SmK\nIMASDqUZkZa2i+965NLGKd+FnoQQkIPKC4o4JzFTiGkp8LA/lD6FhTCiE2dEsYOwXK2dAORljspi\nyK375cH6Yo3VPGJnsE94fRe+oU9oP13go2cfQazUEdUadyb6uzfGBdKts9zyqSRjRkZgN0tTHJET\neC7C06nqrcMdXjlY5852TMP3sc7qVOO4+xing9M4FY+b+wMuGz7Q6dJxvE6dyM6wzZSpWyqsccze\n5Tvs3b1G/xOf1a+fO/bAOIX3+wz3hBDLAOZv03jOJrD2nvf9uboPSqnnlVLPz88/GMo4D+2hPbT3\nHyl8Hq3p8Et8u7bD54H/Ugjx/6IBxtEPhCf8gBZHGU7gIOB+Ljtcv0eeZlT9BsUgI7ujQ2WOnQXp\n4kwzSAUo074qBExnuJOcYpxx8+AOAJVGnaxIsa5cxwt0CDmSKS0b9m7d5dbtu1iOPvkvPHMO4h2w\nbJA+d+/pOfil46vIWcSd3h0ee+xJrl/VQKN0Kjx58Sm+8Y1L5HWDV0xGVBtzhO0AcW6Nww193YUU\ntNuLMJ2Svr7D3rc0ycrMEYRehl8rOFbX4fOxrMuxoMbMsxhKgfB0CuILD7U1ZvPGFfrrE9YsHcmc\nPP4CVFw2DjbpMUauaf899+xFemqXUkosoy7tBgHKqZArC/ltjUkCKy0p85wiLI6Ii8mVZRilP3ir\nZDFemmLnVcCmEPoahZNhy5KiLAhc/XxPRTbzt6Yk22Mupmt0j+kW9HPVEzyz2GJzLqcfHvDKFf3M\n5m/DUrLMYkPSGW2zW+gItCJy6hULz7UYmsatG72QGyrEq/q88OwZyjP6piy7T2KPmmz0trkdbXPL\nRBDqTJuK2yKuOdhCr1c3z5CqZLI/QkWbzA50VOE9/iHcuPdp39cpCCF+FQ0qzgkhNtGU7r8E/JoQ\n4u+jRWF+1rz994CfAG4CIVqa/gMzx3FIgJT0Pr22KgpWz5yFtTpzt0fE1/Qm4mIJdR/CBOzKfWAy\nKSLC/ph4f0xducxqOuQ+/vR5rH7G+OtXKIxMmLvcRtYXWQx9fuftL3H+o7rK4K42UOU9hNMhnaZs\n7OjccmXuOJ3jHUQY89r1S5w79xwA6+sb/H+/+9t84iNrZG2NTL929RLHLz5JokLs5RoLJ1YB2N/a\n59r6OieXVjhZX6DWM0Qem0OKMqahZuwmeuGOxCEhipktSHwbZTaGZ3nMySqLqsXy0nGwTE1gOOLa\n+i0OG5LV5z9CtqJr8nfTiKwsKasSYTgaXa+KEB5xXiIKzRYNIJBYeYlMSlRWkps+hUwIlPhwhqfj\nQJF4itTR1DtHzNJZnqHyDPKUNNXPMc0ypv0+qh9yYnGepTXNgVGPXGa7u3zlynVm6YQFSzv+0706\nIm+Q+xW8rKBtJPZadY9KlDOZDomn+r7O6g5+vUXTtnDiMb1Yf9ud3oC7b67TC28Rnp9in9Kgca3W\nYjLLUEIRjo3W5WhMp7ZCzXGYzEp8R39G1ecBoVj5waoPf/d7/NOPfpf3KuC/eN9Xc3QamePmO4s0\neZ6DY1Oq8j6ph2s7sLoGasjJteOs39S5+e0/vUpjZQ5nENKsttjpGVm2hs1KZ57Hn3iGg2u3qC9r\njNTyLKjZNFbbjKe6g61xdgmiCVm8S5oPefL5H9YXEmQkaYbvV9i90Sc0FQy75ZDZMYXImbuwxmuX\ndGvHoHfI6QvHeee1l+mc1+DjsdVFtrY2SHLBQn2eWc0Aja3T2IttBnc2uXx1kxOG82BN+thUWLJd\n5mtdc39qJKokdiQEFZSjc+2yEFiFwC0lt7Y3iUNDBSZKxGqT1pkVipUWoyPS0ExRsSpan9Nsiiwr\nyMMcJ7CQ0tMELWgylTLLSUYziqhgvqNLgXFeIGznLxQpHHXqHVGMHTVCHf191MG3K2P2iInsAscW\nWnsBsAKX2vEOqVMBg+6HkwjqAa1ggdyVrG/oQHV6u0+ydZ3tTp3MjolNNWpV1uguzDOcZLRVlZ2R\nBgmrts2xdhc5LkhsvR7Gqo/bUsz2Dyh2d3l9oKsPapgz23Gxuy5RXFL3dGQ2CnPaC6vsK3A97Zil\nBbPJgMSugyOIE931GkUJZVn+mc7Fv4o5iL8eHY33FaL0D55wcUyHXZFmeoqoAVkUU4ZGdn0vwZMF\n0/6Uzd2bJEbl5/gTj9Lw2mT9fQ6iCa2pES+xbWh7TNWUe0NdQLnYX4BBwltvfJOV41XaJ8ysfbGF\n32hDanPr9ga1tt6kETPCcoKywbdr2Mvms8sBu8kuH1s5w+0d7bR2Dw7wT62xeHwNy6kyMazNZQnt\nEwtUOlVYXSLa1JHP5d0+0WTILI5IZ/q7zIWKwhIo26FwHTALr7QlhbDIFcyfXkJW9KnoVz1S16Jn\nzwizmVZABvyqQ6WskMwSThzX0ZAjbW7s3iOPS6QDjqEoPxwOkFLSdgM6zTrzXc1L4NZ9lKU+lPTB\naTSxAx9mUFgFuIbboVWhtraIanQ56OmOwcIFr9okn8742utv8eYNI7o7c2jXMqLQxq7b+IFp8S4H\nhOk+9coKa+UyW8YptH0P6YFfEYiOcRQLU3bzPQ739jh46w4HxqnWkThulbyc4TcbdLo6ApvWqoyG\nUw2MStNbIR1EqSiIQUVg2setI2LHB8AeHKfwvfoU3vOa50nGlPhYmpcfzdEYXb+BnHfZjnboLuiT\nyxqlWMUIv+IzLQWdQD+o6qjg8PAGvWxKFnj3c7rNL36FoFVjyIyJGTi6+9ZbnIw7vHPtEi/81Kfh\nmL6YKJpQWVzm4FbM/t6Qk2snAcitmJSEzLIIixmio6+xLgLSSY/N9QnNed1g5DUDDsOMw81tqt05\nnKp2OElZsB/18KsezmpAYgg74mWHslwgEwWZiaGCnR6VIEB6PokFpYkUrKpP7jmEKBLPJjaty5lK\nSfKMrEgp8wxlJiKtVNGafwJflnR9nd7YoiRVKaM8ZGN/i8zcb1Gk1Cp1VrrzrC6tEjT0+yeuRVpM\nSdJ3GdA+KAv6Be4og0lEWQygZqjsRZXYl/j1KkcdEuF0xPKJGgtnz/PmxlsQ6JO4rAX08ajttnDd\nbRxXY+NxOGBj3+ZMt0NXdfC29fdcO75MvBtzUBugcu3ITz9qUa+32E1tvvJHOwgjspNHCkelFMmA\npueSTXUioKIJqmxDIbFMW5flaGXrUmVQRFpgBrD/Cqciv9Mekqw8tIf20L7NHpxI4Tvse4nB3Ddz\nKs535+i98zZKWDQW6yzV9EBUfH2TeOuA2sVTnHj0HPOePtGi9W32DnZY/PgFOseWKIx4zOabl8ld\nlxOPP8LSnI42yjs7XPrWPjJwOffsEggNKGZSUinbrN+6ocHFhpmJz2YU0mU6y5nNBrRNbrl0okk6\nzvHdJTZ6Oh3I0hmLZ07gteocTEdEE8Pr0GkzjWfE0VTnk40j5uIGGSVRkWnqMCAJptS6c/hBhXgW\nMkt1KKqCkiKASCjGxZRxpHPi2WyGzHNarsdco0rrCFSUkoO9Ad1Wh8O+vh+BLfnkc09TeIJLN68Q\npjpn77TblFGEzEp8ryBMdaQ1LhW+M4f3AUcJAGuTKquRT4Fk6kIYmJVRsVC+gy09Fqoa8bd6lznl\n1Dg97/HYWpOXLT2shleFwz386iN89nMv8Hc+oyOwxX7G27+8jX0oOS6X+eTi0wA8sriCPz4gc0P6\nqb4nx+aqLJ5oMD/o8A1xCWW0OMs84bGzJ7nwwhrnf3aZN1Z01HelVWFUtLlbOEgzfGYJSWEpFDGU\nyX2sRn4oidf7swfWKXybmfuV5yAcQUkJRlFJrq5SX79NZKUsP/IITPVCr0xCkskAx7NZOncSTKgc\n7A04f+ocsrUAbg1pGn5alTZBpY106gQVQ2haDrl07TbnPvEE8mydcaxVpGvz58mGVXbXC158+qNI\nk1sWhzGzENK0JMsygq4pHS60yBoFr0wjVs/rdlzHloyHA5STc/GZx+7L2t24cY2gUiFNcxzPp97u\nmGtR7O/1YZZQMU1HweklGouLeH6FqH/A4ECHuYlKsYSFsC1Orx0jivRnDwd9ZgeHWHGCk0q6hi9y\nodNFhQrHKjic6M9otrsk4x7t1WWevXiWSzf1CLIkpFl3WWi0qc3PExsK+qvjPUo5QTAFjlR+Pxir\nJhWqicRPcvIyJlRHE4UVZF4iUkndhOfVOOdcvcKqV3C27cDQtNAsLUC9hkymHFtp8tTjGu/p9kre\ntiLCSYhdZqwY1q3WOMOXdabB3P215tghpxYXsbouSRSCp9dUkQxxyh7nVld48ekOqaU3+tX9HXp7\nEVYuEUZAVmFTUmiNylKXJwEc9eAE7X89nIIx2wYHoVWnzanIaEQcxwzCIY3eAa4R84ApB0yxJ31q\nrEKsN8YgDXEsj0YmIZFg2IdyZZMOc5yJDaXeLP3elP7U58effgpaKdHAiMQ0Oty9mlNEbZrVJRA6\nh5RpTjQMKXKHqhvQqOiFULG1XJu1tsRBoRdBS0pWTx4n8Cxm0wFZYtDwhRb37tzFcRw6rVWW5vT3\nCWcZuRrQsnzm6gZnUEPiXGIVEltV8I3kmUxyPOXgOy7NQ8FwZLr0eiFerKhVm6x0FlgwICH1BidW\ncva2tqkGRrtSJnztS1/m9GNnWDizwmS4a273ADULcXNF0KiSGjn7cd1mpX0RwYwP2in0bY8+JeM0\npJgcwsQwRo0l1ighyyIGO7rPY8WRtDoec8twsutBbJiUNveRjRa5c5ta1aHmmPkJUaMsS+q1Nguy\nycmKafOejIlKl0GpKI5roLbpFTzWXSJpT0nTIb6rcSqvklFEd7DiKV2nw6NmcG5hmHAnnOJmApHr\nk60oFKUq0CQhJbI4OvEenNHpB98pfJeoSiJ12ABsXbmKXyoc2+bG1WscMzd3dzYia8J41qf2zlUO\nZjqEPshDisMRx69YBJbH7kCHv8NpiHcYcTZ0UaYk9idfeY3lUz9B8/RJUN/Ca+vPDmcJN69OmK+f\n5nDzEMfWp1E0HKLCElsGuI5NMtELcpj2sbOQljiGfSTMcjBmvD+ivtJlcbHBlhFIvXTpbSzHwsJn\nOhvQ29OPKJ4VRKMRfmJhFRoNt32fZFBijRKsCOomSionEb5SVGVJizrNXX0dckcxSQtkJeFguMvW\nNR1ah1nENCno7+0yZwhmtuKUy299k1nUY/h6yImLut6/uFjHz6s0bIfmfJfUDGEdVBTBsToYJqsP\n0taP1dmfD0hURr0i8Q3oaWUgJwmTZMbE6GUO3JzX9q5z4uJx1k50+cgjWpPjja+OmBvazB7Z5dTp\nx1ia15FCulPQ25vSDTOcVsJSTT+fervN3UPJ9VGfKNRObm/jHhYpKwt1PvLxR/mtV3Vrte+36Dab\nXDxXZaE2xTPzI5MdgZqewspLyIxTyHLd26FKUCCPWsjzh+nDn7UfgIWqKCC1TfXB6D4Mh0MunFyj\nO+9yc+cq4x1dTrRX2jz69JOonTGjW1tMTB5++mPP4roNijfv0ru5QdrQi/rCiz8EBxFcWudgqEPo\nwSzixeeeg6ZFno7wuvp37t47ZH095tPnPsLG7bexbV0OC4sJtmjhywpFmtDf06drKUO6niSQBU6m\nI5yWVaEdSNLekKt3rjE1xKPzrTqZKEiKksG4zygxPfOyRrVRpVlU8JR+bINeyDiZaEXoWUZmIpm0\nN8SaxHg5iPRPcY1Ks+XbiKokqTvQqiAMb2At6LJ0rMLFR8/SNVWQw7sbnFtZIKh7/PLn/xWf/XHT\nltKsQ1ECtm4O8/WDq8eHjAOLkg9ePfkdO2HPTsHKqZHjpvpZ5oMJ441dpiIHqTfVvWjI77z6DV78\n4b/FxScv8A//cz078oeVy8yrGqv/aJHPfSxguKdp+77y+7e5dTPl3IV5Fpo1RF93uKaRoMgWKXIX\n03fEW69e5ZM/ssTFR36M/+4X/yHlr/4aACerCzx3TPATn1sEa4ut9bcB2Lrt0e1cpF8MjzSLKUtF\nWeooAdS7WEL54KQPD86VPLSH9tAeCHsgIgVFCdYMEiNEYklszyYEUguOAO2yBB+LGROiqT7N55+7\nAG4NkpCTlRNcCTS6f6Z7DIJjiKV54rGibrgEXSSoEdZqwv7uHZ40wB/+GAIBJ5fY/0MdbSxN5zn5\nt74ItVVyXiArdFXiS1e+yf4pwZcbN1GqhtrXfAXdzKdT2LQnGRUyXFu/369IXGlxrnQ1eypgZ1Os\ncYkb+niT44SmaWYWj0jLGcLL6A83KJTRNqxUUHmDabnCLNWYwr0zt/G8Pr47wLVs7K4GJTvHA/wF\ngbM4oay7iEC3UAf1k1iywBJ95mpVkGY6dVQlWvu/8MKzWCMdblcrgktvvMInf/Iz/OTiJ/lf/49/\nCcA/+Ln/Hv/4SRgeQBzDpk6dtncszrzwUWRahZUf8Lmbbr3vJCv9TvGTTx3Mc+pgnp2kQl9WkErn\n8p3DZdpfdemIKVVHY0bX3tzl965ZrM5H/NzP1vixn9ah/2d+ChyRM+ZxppOUL/+mntP7/X9+DT+d\np5pPaPM4Sa7vrec7yMk16tPbbLyqo4fZoMtv1gcMf+42T/zoI/z2J/8mAJmyyCsL3MLjd1+9wr9+\nWaefzbLKWkPwthOy0TERwTwwy7EKQTWu0jpS5GLvB7tpfwn2QDgFQKvx3O9c1NmExbfrPtgCCpVS\nFT4VE+a+cfsWC4trsLYCo30OfZ2bL9ZnBJUx6WTCXnpA3dKLY85ehqBGem8bJ/cp90zLrFOBtErx\n9hWGV/VswUdPfIzq+Q5qluGlIdzVzuLCJObjjSVEFFIkEiswrCdJTBmnlGWEyhLi+Kj1NkWVKTcP\nR/hGidqTMVaWUoRAaiNNnix9geWXyEDQvdih2tY04s1WDd9rIos2TvskAI+2LDz/DLgFlAXkRzFq\nBmJEIm28+QYDQyiCs0ktqKPigtlkHzfT6YZDjWQUUIxLaqazcmvnKp0VCUsp559vsjnQYOpv/tEv\ncmJ+iWJ8QDJNKIUGKzdGing55cknfhRM+e2DsijpkRR9lEhxHQvp603nuRB4DtISWAYH6jaX6N+7\nzP/9T3+T25cu85kfegKA82cXqfo2b/Zv8m9+7d/yJ//6KwC4+1UutOb448EbXAsHdCq67b2IM3ay\ndd7JbnNjprGX4Z09Ln9xl7dme6w+sswvPKMPA7vW4tLdS/zqH7zMKzd2cJe0Y+2ceoK96+v0bq7D\nvmFeygoQEpVnpHlCJszi9iw9LfQA2APhFDSZisW7TB7vZjUS7g9BCAVlGuNXPFSqF++kf8jt3iGr\n/V32hwfUAo3WH26OsOMNLOXjF1Vm+3qD3h3foCItyGIcavT6+iReLLfAW2Rj0KMI9G1Z+vGXGKmv\nUWYpbc/FunMXAO+br+LnAb17h+SFxczILYdSkHkSq16h2qoRmHkGx9aCuI89+xh+RW/cwEvx7AIb\nheu6VOo6vxdVD+o+WRrhtKrkhvVHWQm2WzAarlMNzGvpHsqqaGQ7DcH06OMYMtVcsDfoYzf0JhVy\nwuFoD69o0Kp0tTMBivEG/vRx/LJ5n0tie2edpeMpjO7B6RP8yH+ox/i++qWv4RfvUJmH+fo55s5+\nDoBX39ggrozB3QZOvI9V8L3Nq09xKzMsFRGlIewavcfJNggJpbpfXcJWkLTYvrnDb229xWt/qp+Z\nI1PGwwM21AlIoeLra3TPd3knjHnt8Kv4vEanoSOF8WSEkDlFMyZq62cT0mR9fcAb698C65v8ivmd\n1QbMSjjoow+2hj5U1qv39IRuvQ2HerCNpAQpEEVOkicklpn3qDgw+EBv2/u2B8IplAgKLKQ4KssY\n6W9A3Kdg0i90KgGJmuFUNOj3/JMXOXzjLe7dvsHpC2dZPf4f6Pd+/VX2LvVZPHuS1tpHYKyZb3bv\nXmWrv053pU5kw819LRe/d+llXC9gOE0IzupW5E89skvZqGNZCYxDBiN9YjjWgMWmxalPrkCrCW3D\ns+dLqHlaDahRAxPN4HggBFFho0q9cUs1wLZmWFaMJUKiI1qzQiBljYlQiNDB8oy6kwoRRYiqh0yV\nBjabaoUytSjCnCzMsE2k5XsuSrikqWRhbp7QLN6yLGk5TSy7BcMSEh1BSL+LGHTMMJR+r1epE3Rs\nps4DsHsAACAASURBVKGglhRgANnnf/gpXBGCbID7LOxqOrFrm5c5trJIiE3wfhfC97CcAbk1RMgU\nYbmowkQ+kdJMWoX1LlBnuVTtNYL5Y1hFxMGBUctOJpRFFco2TqejRWGB7XAM5QyOVUDN2J6aSf+l\nCpQ5ZNn9SheFA2UHihYUJVOlB6IONgAJ9YUu2HUmA3PkD0II6jA4BNOHgmMjPYVSGmxMzJKP3b96\nvYcjeyCcAghyJPIoQlBHr5qfj8Yl0wgcC1FmxluALQs96iwyrEYAv38dgGRzQnww4+71t0jEW+yM\ndO47yAbQKLh5NyWrxO8KCh+r4c41Obs4x/KaOeme8RncHTHXbJLbksVPPgPA4kcehzyBagX6uxqF\nB3AluJJUJkxURmhcf4agFJKW10WZuYqiHILqYTHBKjNEaZCTvIFdthjc7jOdZLzwrB6/lg2XWe8W\nQa1JkmsHV0zXcO0M2y6hvgwm10a5QIbnxjCAak1jCowUs919XBVjJwWznk5lolFJv3KXg4Ntto2T\nFEGfCy99gtrpx4njkvFIL+qF5jxE+xBLSF2+8UU9CfrqG1uc+rFPM8P6wJ1CWqTkeQpljuf4xEcR\npbRAmGGj1OyuYcgsi0n9ANepURSmH8FdpNGuMWc7HIwGzAbm5HYU1ANwbHAsqJveDaEgTkEpOJpV\nKhWUDpYS2EDFiO/gxyRhxuRAIVSCfaQkjYsVWXri1HyGTY5rCTKlKG1Bbjx59ABB/g/QpTy0h/bQ\nHgT7QUhW/hnwk8C+Uuqiee1/BH4KSIFbwC8opYbm3/4b4O+jW7b+K6XUH3y/36HnHN7jn0wYbL37\nj9o8B8hwpKR/W4duv/0b/w/hzTvUqw5fv/46wVTnxKdPnaNzbBFKQd32WaxowKnebCC7DchmDGcH\neEZQtNJpkKuCUgrcQKcPbFY57Z4CFUDaY2a4EbO6YHvUY6nlMEZhCx2iVhyXiltBWQ4IC8vIujuO\nR2nbRFtjLFujzZbIgBilJjg4NCydywr3LGTL7P+7z7Nzd5enUq2FWLl4nuqsBcMJvmEHyq0qWXGP\nMN0jG9kMtnWIP9wvKcpDnOqEjXu7eEIDYuFhBZGHdJpT4tEeO7dM1DLtci34p/zIZ1/ipc9q7GDh\nmachSBjPbrI/cVlaMNGTFcIkhjCH5IC767cAmD/WpbtSZcwu8yx/v0f+FzLH6QJViiyiFBKOugBF\nCkkOKsP1DeFqo4FV2liWBm+NMh7TJGK8O8KLD/CqNRbaOiIoPJd+PIVeCK4NpvWb4UDjFaKmNeUB\nx8qwyohyFpEnMYOZqRzYHsgajm3hlfJ+tUwUJSQZUlpEZlGrssQqSgohwLIpTdQTPzgM7/8/e28e\nZNd93fl9fnd9+9L7hkZj33eAJLjvi0RTC6VIli3b8YzHmYzHnqlKKXG5Ukkq5cx4ZmJn4pkax5N4\nIsm2bEmWLFMiJVEkQRHcAWLfgUajG70vr9969/vLH7/bAChTEgRBGagGp4qF168f37197/2d3znf\n8z3fc8NzH14EfldKGQoh/gD4XeC/F0JsBD4NbEIVpr4nhFgrpby+aWLXpFVCXpFPuGqeg7QlETHt\ngwqVf+jpxxkQJmZXCVIazpAKrU1MvGpMVuahsAKm1Q2sHzzP3NmLjBw5T2uhiZuExbY0INYxdZuO\npQk/+RK9uyVmZxZzqAOjS+EY9qpurEBj3geZMq5oDZixRso30KRBVl59IL04JkKQE8sIE9ptIGN0\nq6DajAMLFhJcYsGGIIU5YbExPcjoqyo8Fy+dIW3lmbx8GaGrsP/7+iFCpknbDjm7l6ihronfsGjv\nTLF8ZZGtm7dwYL+qmjSrFh96/H66VmV451tfY+KiulZbdq3mH/zjfw6dHRAngrhiDl+2sLNlOrI2\nGircbrZmyKYCMNLU3z3G6ROvAzCw9xECb4ryTaY4A0RhG4GTJ3A1TKHYogCaYampYcKiuQTk6Tag\nEcaArmMk2JNMKSA7k7KxTB2/lTSJzfuYwiSTLROG0BxV76czBaLAJfQaxFfumY9lRKTTOnY+z+xS\nouR6EEcITSMMfZykAc1EkjUNkFxlsiKIpURIQOiIpUkZ8hbJ5LnBuQ9Syu9e8+NbwCeS1x8B/kpK\n6QEXhRDngTuAN3/UMQTX4AfXHgeSumTiRjMWMQGzfpVCAjatuPsOaLoqt+/tYMEeAcDSbEzNot6o\nM/7OC+z7m1cBmDg2warSEGs6VnLnfXeTySQ5pJ1Vsm3zi0RTSzz/KkdeeI3R+WnKqwcwl6nd/K5f\neJTVQ/0EjQizkLt64n6k2IV+DKFEc9XiTbda4LjUqlWqDVWP9vUpugY9rDUFsNqhonao2TOS5//m\niwx06+z4Z59g/uUXAPjeC68xtHwPU406A4NKW/GZjzyBJtJ0tPdT7OxBttTOPzuzQC7TSWb5eqg0\nWLEnqdTMTpFPZWD5Wu7ItRG2PQ/AuZFjDAx30pfug85N6m8xZhAM4+IgaTHfUs6iHVMh6oZB7Myz\naZUiJTzz7MeYFA4G2R91q2/IDDogLhC7IT4aMmE0appD7AcgJNlEZp/YQ+gmadOgFQf4XkK7tgwo\nZmi10jTC8ApOlUpZGOjobgMjiEgnvSlhpYWmx2iGIMwmlGip48UetShUIGTSPGVYNiKO8Js1iAMy\nGRUrmJrEDRy0OCZMpo3rmoEmBDIWEBroyTwNK/4ZtJfeoN0M9/TrwF8nr/tRTmLJluY+XN+JXOsU\nrhVdWcIffZfA0MlYRQxL3ezF0XFKugnlEoQh/aPJCArdgrrPO/veYP+Lb9BTUiHtxz/763T1DkK+\nHZrNKy3YFFIqTJQ6ulA7bltUZu9IJ9snpzlz4iyXjijU/9uHP0/f0EpWbdhEe38flWRnOH9xGOKA\nQiqFNzePmwxbycQxWhAxJ5fjayoymaif4L5nVpBfOQBBDlKKSOT4LmSy0LkI2WO0f0oRkj5550No\nqz/M1/7NFzgXKQDz3oEBBnqf4tBbFd799j7yXao68ou/8hEiZzlf+k/v0mg0uOchlQ5tfKCHQ987\nyLf+l2+yef1e7n3mKQCK1TR//UdHyGVO8vSzHwMg25ensKOHUJ6g2pxhIKdSEOGGUJkHAcWNO9k4\nojpHD71ygNyGzfT3rb2e2/0TmQwlmjRVOhZL5NK0L0NgGhJDE2gyUa6SMXHkE6Gh2RZGMtkqJAK/\nhmcUVVqQKB0JTWDICCk84rCJDNX35PJZ6q06oTSIzaTnJfAVndrIQBxiLHEMYoc4CsAKQUZXhvG6\nMiI2AgxNEIfK2WgiRIRqRqeBhZ0MttH9WydS+KmARiHE76Gw2b9YeusDPvZD5z4IIQ4IIQ7Mz94q\nkpW37bbdtht2T0KIX0UBkI/IqzzVn2juA/CnADt275b6tYDitcKt2tX3RSZFQECTECuBc8xcAaQA\nr4VXb2GHCphjdoFjX/oSJw4c4Km772bDViWegWVA1oZoEdpNsJPQOp6hKZrE6YjIXpqnGNDu5cmv\nWcH2uzaw/bQK/UfePsPY2QneO/EqoZWm2KuikNXbNxMQUK/NsWJgA7mBJBRdmFfCnfoG3CQ/PXw5\npiOVh0CDSp3JC2qXf/PwNGv3bGXXAw6UzkFWhe1arpfhUy8yYs0yvaAiE/3wGPebD3FuJGLes5mu\nKOzg3UtvovsuNTOHlzLYf06NQc9v7uXA1EHm7XaeP3KcjnvU7e/blqJ7+V70qIMv/Nm7AKzcbPF4\ntpPsSps2O0+1ovCHEp2QHYS6A35IR1FhCF//ynN8esUOdBdIfdAdv3ELwgoyriFEE8NUKSSApkMc\nhshYILWk6UuAFJIIjTiOCcPk4VG/gNhT5cdkP/TdFn6zDjKgkLXItas0znFatHSXWBMIPZ18B4pY\npwMBhFVFojIsA93UEJYkRhInEYQi5emqK3Ip1dBtjFhD+jGxG5DIUaB5P+c8BSHEk6jZkQ9IKa8l\nZ/4d8JdCiD9EAY1rgHd+7PeB8gBLCKzGFapzxJUGOECjlQw2EwmVNnZRCLQfEUYS+8RRAC6fP8eR\nEydZtX4tGz75MUjmGUTnzqCbkTqApV+hmWrpHOV8jqZsMN9UnAYvcMm2GbitUSzPoLhMkRqG+u5j\naMpn5NAwp04O00xy+Ualzqp1K6Gri8rcOI2kI9LOZ2h5HhnDR9PVH+m5AY4PhVAiIx8nUeBZbHk8\nuHoN1kYHwuOQpCbEgpmpBtVqQC6ncJCiLuju7KCtUOfA8UsM9qt8ftXKIbx6gfHpN/E9hz3rE4Zm\nqYQUDTAF2Q4Nu5QsroxkzjhOSgzy2C/cD8D8/AnmxoYpLouJPJdsTtXkowmJe3ma7IpByGbofFSl\nCw9Mu8StKfA2QurmtgGbZhXNqIC2SBwbBEsth5GAKCJER5jqb9eFCRggdWSgqVRB/UJ5kZQPoaMa\naQAsDfQcBD41P6SWEL1syySIDUCqNmdI6OOx4i4EHkaSgmhajERTqYsMQC6tdKlYukEACevVRwNp\nIMIYGcXYibhKxri51PCfxm507sPvAjbwYtK08paU8r+RUp4QQnwZOIlKK/7J9VQexA++SJDHK8FD\n8r4E0uTQ8FhMuPgHnn+F1qlhvEYDh4j5WXVDfN9h+6472Hr/XbRqFRoTCp0ud5TROzvA8Yhdl1ZD\n7dyaIaEGwq1QiJP25nIZx5shDMDFIk4lqjzpHKKrzNDgTvrv3sSBVxSMcubMIaLmLGvXrycvdJpx\nIotuani2hhmO4RjKOTkCZDoL7WVEWbAyrXL2baPtfOXPv8xDMwabPtypxt8B1FZy1+oNnPjuXzLQ\no8hIj29aDzWfR+9bz+qVz2KUlAMJJiLaSkX+0WceYnr8Mrmiclp6tZOP7f00dw75zFVd7GTISY++\ng89+LsfZt6vs3K4goJf+/Ns0p+ZB24qtFameVZ99/mtv05icpafwNjnDYGiFInSlMjkcdzGJvNI/\n7pb/RGZZTQy9jpQVglCHJZUi0wLDxNRt/GT8n6kZ6JqGlBqx1K6hzJuAjh1O4lWbCcELUoUOhJbC\nacUQWugZ5VzCKALNQjNC0okj96NFZNxCFyFh7JBOKPV+GOD7LojoyoYGJOUzDXQzaTcHgpAwlsgg\nQI8lIgHRrZ/FuO4btBud+/D//IjP/z7w+z/NSS05hGuDBwDXdQktyGg22YxStxlMd2O2a6S7dSJd\n8NeeuiOXLpyicqjJ6Zl36OrK8OST9wJgrhpi/PgRCpk0+bRFsUNVFIhaEFQwYheZzBAwGj52LVa1\n60KWevIwToazaJpDoVQkU8yz1V8HwOwJmzNvHmbh7Gnu2ruXYlIL92qLlLJFInuelFClR5HNEll5\nxcgTDciriCVX0Okut7MwPA6Tg1SOKSDvzPE6Xb0F2pw2yglj9vQrOhcn/o69j+1gaMNGDr95AoAL\no6PkSws8+MBGBnZt5bn/6+sAfPOP32HL9g08/qmHmZ4eYd9LKqo69Pw8n/lciZ339XHo26oiMTs2\nxmCpB+oroSF47etvAHDwdZfHHv4QU5dPcWr0MgdPKp5CM7b47G/dw8+CJCvDFnHkKjK8pl+p7WPo\naMJC00z8VkJ91uPk+RGKgagvTQ2SICQZ30fKmFRa5TiFTJ7qYgSuTueylezefQcAL3/vJXRDJ5eS\n2Ib6bs9vEoUS24iRmkYzaWLzPE8tetNCmFd3fBnHEIKhG4QJfdwUGoYmiLSQAB83UA7bS6LKW8Fu\nHciT+H2RQox2FaFMFmPazhKHHilNVzRboD1IUzY60T2f8bkZpi+qduX29G4Cp8KZY5cYK1QIIpXF\n3HOPx5o1awi8BjW/SdRQObvnzJPSPUo9bSwlxfMj52g3VkJDErgeIq12l2KxgJlOEXsuTrVOdpeq\nHGT72klrIe+99C7f37eP7Vt2A1Do6oVaBF0eRsK5D8IMrpcFJyKMFjA0lbI0wmkqzSk0WWH60BSn\njqhI5uzJgObbr7Dlnh6OX1AV4Tb6mG5NUzx9lLu3bePiBRUNLS6YXBo5wYY1GaLzDgtjKoJoExuo\nX9KhJjiy/yATJ1UFp1RYwzdfPcK9q+/nxBHlhERNEE7ZcKnEC3/+PMcvqvMoF9ewefd9PPGxj/Ll\nP/861UoyPKbWwsh1K4LJzbbQQJcWBjaRsImj5BghxH4LV/qQpA+RFGhRgKZJdO3qYJUYDxFrBEGe\nKLSRocJCZGSh6z5YEk228FoqAhVhBRn5EOtXpmNpsY2MssQyRsYhwZID0CzQdAw9jSkMgpbCqUI3\nhChGtyD0l5yWgZkyME2DOPQItYRqLm++YtWN2m2a8227bbftfXZLRAoSqQhKP6CnkFR2r34w0jAj\nDV0TqlcVSDUg0wR/3mHh1CiyqXYMGUMUV+lqL1Dq0Tl/XjVKNb0Fyj2fJJcpsO/Vg5w7cRIAt1kh\npUfs3bOZ7dvWA2BlsoTCwMjYmJaOmYwTd+otoqaLYRgYKQtaiYy4Ieh+aAfbsjle/+Z+Dp1QVOwH\n0r1Q7MGQl4GEjhuWSWldkLExxBTYaieJ8rNMueeRKfjqy9+jp30PAEG5TN9gP4/+zh7mk+jmjS/U\nsFyHM7Nvczd7Ka1Uef+rx/dRamvQvmIrsqHhCjUyb+zyCENWN/Q/QVdPmjPDikeRyvts2vgsUUun\ns30IgMZ4lbEzw5TMN7h0+gL5NsXd0HsjJpsHKKfW00pXEQmlefuOnci8C/Ys10lNuW4zRQEtzkEY\nEmFcxRQMQwGFmqn+Q2F7yAhEhKaB1FRUIWREHEoa/grQJPUEaGxVXeKoCVSYnphgelJFlGbKIvA9\nmn4WkpQvMrLEZPEjAy/wifVEp81Q9FtdGOCC4apllZYmmZSJJiKqJPL7UYAWCKQRIfQQmbSvy9TP\nefXhZ2I/wGt+HxV86Xp5HpqQIPUrtFDdjcEBy4mRFQczr8qGuiVJWy0yZZ1Y93ClIvw0A5OpuTkm\nL5/lxRcP4tUVGNhV6mZ0fILa1BgpqVh927fuZVQ7Qh6TvNSxkrNKa4AIIY4IpBI9BdDtHIVckb6H\n7uRes8S+L30PgO+9tJ9Hn/6EUuxNHt6wpRN6JoQaoXAAxaKcqF9gz2NbWD3Ywamjx3nmo78BwBuv\nTbJm1yr80mn8gvrs5gc34ehDvHTgIM+//Xnue/K/AqBmD7FmTRbfHiVl6HzmN+8B4K2XLjBy6Tzu\npVfZ/sgast0KlzHMAazMMsywxe6dqnSrt6c4+MJzfPVLf8pnf+tzVDJqIdbL82x9eoDpiaM88vFN\nTFxKxGu6exlYZ+FxHvsmOwURpyE0CTwDKUxI2Iv5cge5QpFcLsfFc8oBEwbEcayeExGjJS2OkfQh\nDrC6dtDVWUAXKh2anjqNuziPyEWYRkTgqoVuWiZB5BIQEZuqTKlZeTRZxPMtwtAFksqQsBOxDwND\nSjJJpaun2EZfZydh4DA+eQmAxfosnvTw45hIRgoDAYR52ym8zzSE6lVfCgp0MIkVEyG4OmPPTQvq\nmo1GTHtS1IisLOQKHD05wrlYo6InD2kuixk2Yd5BnzNpaymsISvbmHizwbvHL+A2Sxi5REDDMnBz\naeZo8e131GJevWMZU+/t5vkjL7NjTyebtym2ZL4cEjTnMA0N2aySTdSKLNEA5zzEJr07urnfUQvs\nwP4ZXjs1zKZHTNo8lffv8CbpX5AQ92LELlxU799ftwiW7+Fy3MHRahPjbdX78OiHn6JlmHzpq+8x\nN6uAzfZ/foFfG/80G//tJob/3WEOfkqBgdv+4B7mhMuf/OnLTL5xlt//p78NwB19XUw8p/H6vx9i\nccDnrt9R3+Pm9zP3xxv5+vgX2PSwugmffPx+et6pcckc4sWzs2x/Vqk5b91+P/v+09vsG3FZ9njM\nxx5TEcTc2QNc2LeStdsfgI6f7nn4QavaLlEZ4rAF7ixdTz0MwKon9tLssKl5Hv01RY/xzy4w+cXv\nEnllcG3oTBxU00Xs2cnWnXuxNJ2ObKLQPLqR2eFRhl9/B9+X5BJOglGBsNViz+o7SCeqXaErKRe6\nmG8sEptwqKK0F1qpGM0MaKs79DuL/A+PKmy+WCqwatdWZqanqQ0rus73X9nHS/VjjIosjaxOkFRB\nrOZtiffrN8GVSEHTNGJipYCb1HCEEPi1Gq1Wi1wud6XxxGvWyGUt9u7YhSV9jh5W6r2u6zB85gyL\nc/OUSiUujKnQOpe3yWUy6FGdxVkVVYROSKbsMjZ1ntnvnuLCiHraH3xwM4WiiWkJhGlgGkkTkPTB\nyqiWt9ikf60a1lqrpjl1/CLj503aliczJdI6WlGnN2oRN5poGeVw5mWN86OnuZTNcOTCIaxkKvEj\n2+6iVqngnD2OHSoHMv3aMdh8L2PRW5xtnmdOKY4z9N566hmP8WqLjnXb+MaX9gHw4dJm0tMNwlUT\nnF88w7IxRX/u7hvhW0f+GLFujncOq3Tqk+s3o1Fn4/punju2n/xOld6kdZ9GbZz+ssnEqaNMrFPR\n3cXjR9jY1XvTiUugJjLPzMyA12LNQ/ez++H7AFj76B2MyzqHzpxGT0A/faCNcMc6Zo/MAjrMJ2zZ\ntRt4+OGH2bRlJXt29HLxjLqux/QaPSWBEc5z9qVXcW2VKmhRzIZ1GzEzLssTfY31qzbhNkMuj02y\nMF/j8oxyiCNzY0S6j7AM7l59L9myuq79QwOseOpxVlye5MA3VLPwnnvv5cz+OueaF5HNFiLhUdjp\nm61CceN26ziFaxb/D/+IuPqvnSC/sWRxcYFms0G+nKc9CS2l38Kpz+HUOqk7VRqJ8lJbeydz1Xnq\nixXa0zmySTjfW+ok8usYIgRP5eyLM4tcmjmLMB1Mu5vRS+oE33p7imr1BKvX5di4fhmppEVai9Jk\n9AwijAAD1qjda4Oe5uTICS6/Y7Axq3LLST0m6DQJMzqaW6C/rHT94nQVu6NIe3+Btokiy7rUg2eX\nSgwGPuV4nkBXu05fxYR8jNibYu7MImai/Tl4xiW3rcif1SZoBg5Pd6qdVby5wBZR4I3RNwjuDujI\nq110uTdEq3cflyqX6e9IHFw6x4ZdW/j8v/siWo/H9t1K7KVncBlvf+89pifm6ds+yOYOhb/45Qmc\nuEIY1zHIX+9dvy5LWykymQw1v8ry5d3kbBU9tmdjnMinOnGMFZ1q4bZ1Z7HWdPPqWycx7WUEnvrs\n4PJBNq1ey8cf6KQJaOvVOX54zyNcmHD54+kTnJVT2G3q+WnO1Mj1ROzcvppf+bTq9yvnYGYaiLbw\nxS98i3apNomRhXGEZSHCmO7ly7mUOKLyYDd0pWDG4Oykwp2KxTz5zg7saBr8JoahIoVW072p1+yn\nsdvVh9t2227b++zWiRR+mF0zYy+WEZowVbyQ6FuFkU/gu0ShT9q2yCZaA5miyfTYAqeOv02jMks2\no3aAzRvu5NDJ86Q1iVer8sg9CoQbHFzGy999jlK2jBap1CSseUT6JHo6JBYZAk+9H0d9DI8c5MzF\n4xw6fJRnn/kkAOVMLyLdBrkY3AoYCSFlVY47n9zApX85xswxtWM0vUXyVh++bZFJdUFdVU2CRUmY\nabJ56xayxYfZaCimY9CoYvaVuPPDOzELCvHv79gLUzbZzZto5t+gPKdAsunvvELvXR/nf/yVJxke\nc9llqwpG4ytfplRrsvveAXp/+VFsK7m20z6f/Ac+pyZn2NGrPsuYBYsp/AZ85JlfgtSSVoPk2U99\nhunxBTzTIBhV12TnuqdwdIFRuPmTjnTdxNINkCF5S6OcVceoT1+g1GWwa3UHTjK4t780BG0ahAsE\nMge6Yh1mTJM4CBgHtAjcMEkfZiJOHXmPQDRhZQeuq6JEuyTw9BZaThIkbMPheR+Jxqo1Bm0rC1iX\nFP4gDBMhwJMRs06TMGmZPzU1Seab+5ienOHCvOKhZOt1HN0g0gzQDNWPAbjXYGf/ue3WcgrXPE8x\nGvr7axCEYYgwTVXCTIamRlGErusYhoZp6tRmFcMu29VBZ8lgWW8JrT9Db9K0tGXLKs5dHMbWQ2xT\ncN8dahGYms4b6BTMFLFQuER3qZND4+8RyZCG4yCiRCtAFpG0Mz5xhoWFCnffqRbXoumhy0mGBtvI\npA3q0wpxzufSDGwbothnM3bmPABadpJ0o49U0E7aTyMPqrbs6uFxmuNTdD60hU277ubY2wrz+L9f\nfYGuFW3sfawXK6dYcH/2r49TvTDLo+v20j+wmb4u1aBz5vBbOP9qhLW//VnWbtzGqX/5NQC6aDE+\neo6OagfyTMx/3KeAyb7cKh769TypjgFOPqeON390jtb+UdrSq/GDAn/zvJLEiMMqv/HkM3QvX817\nrx3hlecUkNG/aoiVd66jp/vmazQKCW7LAbeF26oy0K7y/qmJYToKXTy2cwMXjqiSc94LGalOYmcE\n3sKcKlsClZkJWgtznDsd8PD65czkVPrwwte/iVdtML9Qh2qLyFGLU2bLxIbFouNycUo5nI3rugl8\n2H+owvHhCzQcdR9kHCMdD1eD0eoCd9z/AAArV69krL5I96pBHvz4RwD4zrdeYFEGzDcaoAuihIiV\nzuVg8dboFr61nMKPsaVmzIjg6lAVQ0ckDqGYz1KsJfVgb5qOjgx337OJzvYCpqlyN93UyeVjgqBK\nR6oTr6GwhotjEwjfxWtFpBL5rWJbmam3HDSZQtMkblKuOnX6BPOzPgbLWT4wwOKiWgbvnD7M1MR5\nfuGZe7nz3o3ooYpOnDjGjGLyu/to/u1r6ryDcdqaqyh7DgYGYaIAVVyUxPOLRN86ip5axsU3lJDq\n6ESVkbDKtifu4OQRBQZ+fzhka2MQ53iLxoRL6i5VYVm/9wkOPP8CK78vYLDAkaMKg9jaVWT3px7k\n2NlTTP7tm0zMqIf9eM7nwyMWvaXlvPTSAQCaJ2zaazYdm/uptyIuNNV3GJkWxy+fYlP7IKdPHeHS\naBKduD59d27kZ4E0yjCgVMjRqGkcfOMtVm1RpdTyqiwlXyPTDNjUrfCRd7/7Hm+9/AqRE4Cenwwd\nmAAAIABJREFUvvKET1+6wMkD7zA8l+bw/qN0tyk8oLUoqc95nDsxBr6N1qH6SuKL44zk51m7VuPt\ng+p6v/LqYSwzg0Aj39bD5XEV9Wm6ToxOEPmcujxKac0QAHf+0qNXE3T1mPHFbz7H2EKFCKFUvpe6\nNcNbR4/tlnAKV5qe/h7QqHEtY0HT1OnGMr5SfcimbRwZY2iCru52PrRZcdfnF6bIZQ26+2ysLBAo\nGmkU+6xc2807h0OarSkOHVSKTAsziwRBlThIsWHnxuSILqLZT9SYYGFxGE0kFGUvTz5TRkQZQqeD\n2oK6sTMzc4SyQa5oEQU+jq9Cn/b2TvymC9t94neU09InFumMfVJhC8e2ya5TkcyaFUOc2vcuza8e\nZXFUQ06qRbeyd4h8RwcrgnbSDbX4PeN11pTWsnIxzdn5CpWSOo+Nv3wfhVPDfP+PXqJ7t8lswtE/\nmD7Hxo9/AvkHL9M6Jdm0RYFz+8JhihPrIfAYTNK1qrOAnmrHj5t09eRoHFeRTDobkOl2SZccQvMy\nDV05ls5lZbLtgogWN3vqdOA7aAIwLWYuzvKdrysk//6n7sQKJEHoUplUUdLr33mTicNN0MFIaUtD\nxZHzU7z72os83PdRJs+N4RZVmrAwOcM7r+6DqVnIdxAnQ1tSg6tYuHCJl8WrbN+6E4BctkRbuYu5\nuQovvvgizaSd3CqliXWbyPMZWZjmG6+9AkDb6iHWbFhLZb7GsYNHADgwfJbR6gLpfJ44pcRW4Hbv\nww+3H2A0KruKKRhJKCilhIxyCqmURT30gZhSW5kdH07GwE2OEYV1NMuhWp0gSr4xk2tn5YZuepZl\nOXd6jPPD6qD5TAFNd0jlLO59+E4AQmeOhYsWgSvQwhr5ojp+s9GkkOqi6Rj4izqnjqmdxLId7rhr\nC6tXD3LgrSOcP6PSh8cff1ztdCsmsVYlA2ImJZnIxPNbTNtVVm5WC9RY1U76zRL5eYvhL79JR9IR\nWZ0eZkV3Gf1Ynf6s6qn4vV/tZ9m+IkV3Lb2XNKZTitPAyiG2/94THP3DYb595jvMDKl6+od+axus\nqtPzismhFw+ybLfacf/nzzwFUnDpub/mzPGXAPi1T/42jXGNy5HDji0r+M3NSqUpLlYZKKdxKhM8\n/ewe+i+rkWrllcuIjXky1zsz7iewciHDzNQkOD6FvjbO7Vc79IVTX8O0oFywmbqsSqZaCJmsAeRp\nOR54CevQjogun+PF//1fQP+AYj0C+AGMT0KpDI05SCnH706PoLeVmBk+zncn1N/Y1tZFve4QtDzV\nDp04nDgMCEMHU9eJbIM//cpfAvC1l75NR3sXQRCxMK3Yo9VaFc0U2IaFW68S+AkGlstRm7/pl+6G\n7Hb14bbdttv2Prt1IoXr4CloaDRlkzYtC81kdFoUU8incZ06YyPnWOYpEMpZnERLhdilDFoUKO1D\nwNZt7EKepz/2IG/vP87F82rX8QKPNRuW8cD9eyGt8vvK/DjBggECcqmQTFJNEGYWzY/QQrC1kIyt\n6K49y1M8/fQDzI7XmBn1uHBK7Q5/Mf5VHn38bgY2G3R3KxS/FuRoXHbJZdvxzMu0DPXZi9k63Q/u\nJTXmMHfkMGuLCj2PoyYv/J9/xMS3y/SbCuDq+ocfofjAL8DnX2V44XX27B4CoKm9SXZ1D1v/t51k\nR3uodyhQrb3vEkRTdH2qH+/EEU6/q7otB3s0nvve3zHRusi9n1BRSPtnNzP8r17i3Kl5Ct95nR2f\nUDwKrSPmm//vV2iOV1mxqZfBOxXp6sTcSRpOlhWl3VcnAt8k870G+ayN1ArUJmtXaM4FciyOzlGx\nJakw0UdI5fAciRNIpbCUgMa4dTBMyGkwc+7q3E3DhIIOYQWQECb7ZEon8mewCjZhQhZbmJgB08ZI\nWYRapOj2QBx55LMZ6gtTGDlLSfwDLSLOjgwTx1AuK61NZApNgOM0kG7rCiU6/nEP//+PdkNzH675\n3X8H/GugU0o5J1Sf6r8FPoQal/lrUsr3rvtsfkw1KybGEkuTlBLJtDggnU1RKheYGB9j2SX1kKYz\nKcJ4kdrcBFZBo61N5bl+4NF0A1at6yOVSvHA/XcBkE3lSdk62a428FW5qtK6zNSlJoOrJZXGAnc9\nqFSJXn/tApXJy6xbvZNK8wz5nHpoHrx/C+gRtixx+sgsJVu1VI+cP8hL8nl+Z8tniSOl35DWBlgY\nh1xdI99tIVMK3Bw1q+jrVqAbDRbGDGYTKbVHfvMX2fMLa/jWa3/L4Tkluzb5f/wbVnl/gZyvkdnW\nZPVGxaTLImjqs7TMwxR3t1E2krkF0ST+XA1r5x6e/NwDHP6G0l947t//PpmtQ9zz7B1sf0zNx2Cg\nDivTOMd1Zg/P4yaS+pnta4gO5IlnBC++/Q57C+r6DdemKK8tQI2bTnPW4hCn1SSOBVauk8hPmpnm\nBCJooz3VSZAwWd1GoF4bAjSBZiSDezWBlCGhk0x9umZsqZABcRhCLDASWTcpBZEfIKWFnRCMfCtE\niAhiHyIPbHW9RRAQNDyEqRGKSDVIAU7sYdg6RiRohknDW+CipW3KxQJN6ZFKzi8Ibx3y0o3OfUAI\nsQx4DBi95u2nUBJsa4A7gf+Q/PtjTRIjkju11CF5raAzQBjGmD8gWxWGPlYqRV93N6+fOsTsWeWR\nO1f1YxgaQsSYpo0bqJvSdFyKhS5i6dDZkSGdSga/GCa0muDP4joJzVmrsX1rB5Ozh3jgvjtZM6j4\n/6+5p9mweoBa8xydnXWeeWYrAD1DJaL5Rd57c47KZEizrnb/gZVdbN3US6NiMTKlHt5Fs4fIsRiM\nCxSDBexEst7DZzTr0bern+ZEiupbCpeQjfNkP/oET90Bl1yFEUyeOYF76iL9uSxDD2wj1zukPtsM\nyHpFspluguoMrqmuoi3KuGGA0ObpuauDZ4buTe7adtxdG0ktbxBGpwFwKq+hD5Wxsxr+dI1MI5lL\ncSygdSDG9jt48M6nuOfux9Xfc/RFIvfmy7sDtLeV0HWdoNlSsyPDRBZd5LGJEW6OIAGSfQTF9hJW\nW4qZyjRRUzlsqUlEEIKVBaFdwae0WENGMYawyGQyFJKeCN8PWVhYIPLklWfQEiZxHCOkmokeJs1W\nIvKwDZ2B5QM0dJ9FR20q7lwFQ9oYlokrlqYkxwRei0YrJPQa1CsKSPBD52dy7W7EbmjuQ2J/BHwO\n+MY1730E+EIi5PqWEKIkhOiVUk5e19ksXbgfAXVoJCVJK/HoUQgypLOrnfiYz8k31EN9T7qEsbaT\nfAz16iKhnbRaWylM08RvushYRyZRQdyKaDl1LFti2srTDwyW+cSnN/DSKxf56NNPMnxR1fCb1Sq5\n1R5TC2d56r499AwlMxzDiMpsg3OnhinlsoQJXfrevXew664+njs9ztlRhdYX8nm6CznIFjGbGkaC\nPhc0jZrRpOeuIfLeNhxNPTRf/Yv/yMPdTdr/2yfp11UJtO+RFRQvD6PrLmR1wpYCMYPLZdLjFhgS\nszOPqSf1sP515NrXQOAQxRF0JKyc/gEq2jRWNEzdVfX+Ylmy8+ktRGccjn11P1//UyXoGok83fYG\nqEq2tq2EZB7l3dvvxGsN/NgU8EbMbbpqtzZCpbOYHMO0LNKxTuB7V4R8c+U8G+7cRqY7z3tH32Hs\nnLq/KRlhGBpOlEUIgUhQf5ks8Ew6Q0+p50p612w6hM4I1WoVgmQjkhJNKodimYIayhFZhqQtn2LL\n+tXM4nB5VpVvRxYqaEEAMQRJC7dShhJkhEboQT6ZEVEq5WhO3xpI4w0BjUKIZ4BxKeWRH/hVPzB2\nzc/XPffhtt2223Zr2E8MNAohMsDvAY9/0K8/4L0fOvcB+EcAywYHiblS4fmhthTyxTJUop2o8eqy\n2URL2XR3dzJ8SPXVL+saZGVvP2RyRM0axZJKE7Rilsr0PIV0Cd22rqj96pokLzWk4SOFCuVNI6J3\nZZVP9t6LyMgrcg8rVnVx8fJb3PvAFrbesR6cZNRarpvLoye4dOkMxFmefuYhAHbu2kC9OsXzRy8Q\nRGpnXdNbRJRjFmOHTKSDVKF3IdY5e/k89XA9Kx7bk3TgwOGX9/Hey2/z8KOr8bsV0DhS7mZrWxtp\nbwZijzijQl/DsZn54sukz5yl2lfBf1JxIIYevxPRStG4GJJv60W2qWOK7iKGOUJeE5SzChBwwxoY\n59jz4TSDxT5GLqk0xossdrV188qfvMLchYAjf6VEaxd2tbNhWx7SN78kWZuvIWMdTdcxsil8qa5h\nrT6LxCSNyVKQHyBpxVXiSNIKGpDk6iLWMMMYN2leW7qZMo6J45iAGNeJcB2V3vlehO9FBH58Vc0Z\nkDJExKo7d0kYRwYR0tEImnV8rYXnJQLngYsZKVq+TM4ZLYQgxNBNbMMgSj5ba1Ru+nW7UbuR6sMq\nYAVwJNG/GwDeE0LcwQ3Ofdi1e9f7HMff43YttU4DISG6TKSzUZhCq1Ena5dYNtBPMKH+71Ovn0Po\nBise3UKpNAjJYqQpsWIDPdSU7PbS+7HiOkjPwUv08nzpEQdHyfZuxpmapNylwvZPf/YJjp56iz17\nV4LQcGoq3758fJKJyxVifY5de/rYkTAM7UyKV18ZpTo6z6qCqia05SS2VaPemiKfMSClsBA9jDl6\naD+dq/OsePwjHB1RkE3H0E7mphoc/857bPsN1To8bYAhdfAlQS5GS6sLZcR16sfPkDo/TqUyRcdH\nlbPQ2gVz783y3n84S9aYRG5SKP7qZ9bTs2MVnHIgqx6JTDkN7hxs76V7qIfuRsJUXLYTXmrS+PxJ\nLk84BKuH1PU+PsyKjTWwA+DmypX3dg1QyBWZXZjDlwGkErXoZhVPQooUTqJs1KjETL8zAYUMzM2D\nr65JGOpESEKxiBDiChFOAnEc0mzV8SbqVKoq043jmPriAhARxsk8SilBSsJYqt6bpFPX91pUnApH\nD7rMpUJafoIPeC5aLHB9B2Enx0uZ4DRpeDUyBJBMu8pmbeo39arduP3ETkFKeQzoWvpZCDEC7E6q\nD38H/JYQ4q9QAGP1uvEEYq4nm5FIDM2AQF34MAzR4wA8l3K5zJ4NKlt58/DrHH39JKn2DL13DhEl\nNNKg6ZEttkEoVEnJTVDfwAErQtNijKWqVCqFo1+iMX+GdGYDqZy6XKK7m8e23Ys7v4DbiDh1QjmR\nv/2bfaxYU+SpZ3Zz7wPriTyVy7/71ihv7r/ADj3NOkvtUKmZMQZ2tNNnRsgghASF9ojRUiGZbgvf\nnWUspRyR3reRudHTvP3t99j2mGLYrdg2juk0QISEKUktUKKrHV6WDruHwrIVROJFWnYS4gQVsi2D\nZZUujEaaSUeRruwtCzDwPzH55VOcP6cqEpk1Bn7bAnsfXQdtMXK9cmaz3mGsgSKbH1vF6ZemELNq\nHsTy7XfRO7COyK2gm1cej5ti83M1lg+uZEF6uMKnu0thOLpTxGq5pGWMlgjiLoiQihbjGQZ6eRlt\nyVi29iBFxhNMm021uDV1TXRdOYvAjxSIqF9tTMqVs1i2gZloNYS+j6Zp6LqO53lEOeXIc3WPvBtT\nj1sUcjamppxWZ3uaDtekUW/iltR91DoKOHNz9AUxOT2ilFPRWnXx5yhS+KC5D1LKHybx/jyqHHke\nVZL8r2/0xD4o51DjIAQGOnhLJZ4QTdPwfR8za5HJq91516Y7eP3cfl7+7qvsTbms3KtUhnTbVgNg\n3RAwrio7hRHoEWHo4yaRgiUN0jlJaKiHh2QIyeKlo5T6LHQDTLOTxTnlWCYnfCJGePaXH2Fy7jBt\nBXXMd94+htdq5+5MmrZJ1fTitS6xvTQIbUXciVGMQKUgviFYtXk1dz96L04l4vF/+OsAXH5tAXm4\nh7FLz8ER1fSV33YCogiKPaStHFOOckJxmKLVyhCOQHPNKnr7VKclpRKiPkd6MWZ5Ok9PUjrPL3dg\neh3dc9s59I7qtRg5O0K0bpy9j67FNxqMRcoJO3YfK1cMsPlTTxJNHeKVEfW+e97h3Pk59m5bfUP3\n+0fZrh3b+fznv0hheY6J+gJmUmLNE5L2PGTLuVKSjMs5qqZJUwpskSbnqjQzuxiTCwxqHRWlr5ho\nNJqmiTDMJGKIr6g/h8m9ti2DKFKvHaeFZVmk7RRB6LEYKEZoV6BTNmwWarO4nRnqydyQbpHBmnHJ\nlIvMJszFZhpkvcGKTI7m6DDZNQrYPF+b4FahDd3o3Idrfz90zWsJ/JOf9CQEghCDRASbjEzmf4rk\nDJN8Qm9FlFJC8UsSuXULjVwtBjugZkKmqXLw9mUG29ds49W3XuTwS28ycVEFLOt37qJjwxaYvgRM\nQlfCgS8u0KpV6RAd5Asq2pgJoevyLsx8FYrjeLa6sdlcBpwsZiOG1n4e3qWOOX/hO4w3Mgjn14gm\nd3D8RYUmt4/Pstz2ePB0mRFLlcguDthUij7l1jCybZ5qS5UvHf0STw09yPwXfF56+TjtnWrlfvxz\nv0Kx8xLH//Aw8zPK8eWjXVimBdWQmrVIJq1CHNMYx5DnsPJlam0VmquV05LmWVLheYbMAnJ6gal7\nFakp2LGWttnvUx//JvfWR9R5iBEag0XqHe00ckP0zSfKS6EBVQf23MW233mA059VUm/LG2Pcta0H\nekvQfX3YsqZ9cGS4tDCXFuO0N0ahVEDU6/REGoZQu66UklCmMfIdV/lSIaTe14WcbC9FgUuEEedU\nUHYtgPUBXctL58A1LQkZKynLemCQIZ00zmGH1PAwyja5MCJ35cs9wnZBjRp64sjapIHIm1TCJvpQ\nH9UkvbH9Ap7uoWna1WOjHNXSe0uvf9Z2m+Z8227bbXuf3RrxSmJ/z0P9YC1Du7Zr8gcSDCEQugYJ\n+QTdY3BgFQ/fEXHh1GnOvKX4CzMTLVa8e4YdW9dAmyBM+ud9Q9BW7CHVTNOaXRrckYGOLK7RomlB\ntCRK4oHutki7kIklmCpv/eRv/lMWaxbvHjjEV79wgE898CkAHrz7bk6++X3mZR27MxlCkjJpuB7l\nAEJDRyYCnrWmy+nZc1Qmp5mb8wjCZEjIxYv4MsBBYrepnLque7TbEmRAhIchl1IhQS4wSHtpFfnE\n7cnljAlkHlNPs1go0OxXua/V1kHb3BTNyCcdK+ygZXYglnXiFTw0M7wyxIbYgqHNTL56ku/+yatk\nNqjPnzeqlFoN1hdufgX62t1TSnmlhf7qXOOrUcWPs2t34R9l1373T2vXe8xbxW4pp/BjTVfUVQlX\nBoRGGqBrYOqK0ppRoWW9uoiYlvQXhuhf301vVhGPFoKAQ6+8S2ahwbqP7sDIqQXjeU0MswCyQCZQ\niyXTuYqa7tJE4qNmFgKYVhrNTGHaOqRzNOdV70PouRQ6Btm4Lk85dYrREwq06162nA7DYN6qs2Kt\nKs4U5GUEFvgWIkyRMtXiytol2vO9RNUQJx2zrFM5gMbkGKdOHMfKZjGTCkYVh8AMMcIAqTUxkiG1\nRBapyIQgRXdmkIJYKgg5uNEs9QiqmQxhj/rba2SJ9Gm8KCTvq+N5ZkBhzSBeqk4h9DGjhKBldNOY\nF3zla/tYnJnkH/+zzwAwYiywUBr8sVT1GzFd19E07YpDWMIDgA90ED/Kft4W6H8Ou2Wcwo/sh3rf\nOLmEAn3tuG9DA11HmAYk47nmFxfxPFjd2YNud9JnqAcpb0vCvhbnT51j3UfvB1uhv87iBJqRhbAD\nGsllqRTJpFxSdppYixBBotHfMjGqJlpDQhCS7VXipbK1gPAz9OR7uWvzVuIR5SwmT51CW5yjZ3A1\nepdyLPVLLbyKD26BtKYhA+XM4pZJyrB49pmnGX7vAk5dYRCvv/AtXn/7uzz65APYG1V7eIrXMcMY\nYgc9al4ZfIJh4VsaumlglDsRS9WAoA5BkaC5iMhGdCaoekQTT8ToYUgqCZJSVppybz/VcJR8LYRa\nEiUV2jjw4gFGDl7gVz/0S3RuV70ScXdA1eknckC/yTorS5HCklP4IAdws3Pt25HCLWxSXDOe8Mp4\nenmlq0wamirnWYb6t6SeyKkLs0yMTWO1HFasaSdrqUVnWCa7Nu7hnbcWYapJ2K7C9ozRgxG1w3CT\n8bcUtSKIxhjanoc2pZpMgnATpkEWwDfAN+G8AhSrIubE+BGmJ95jdqJOMVAPQyZ2GOgvs+KxbSyt\nuuaYREYZMAfQdZeolXRgRjm++51XyIVdnD58muHTKtrw4go7d+7m/l/8NOTVAihWDHAFEGHIgGAJ\n30pFLOYi4myA15YisJUjsqRHRupEbp28IcksdS41qnh+NwR14mS4bimfIp3NEjhFaJShkQziDTXq\nbx9njVWiJ9XH819VgiLTQzEPPfRJ9Nso1c+93VJO4cf6U6FaTGMgTurMUk+cgq4R60JNcAbCfEQ2\na3Nx/iQdNuS7h9Tng4ixSxMEfsyJA+dZ1q0aPwc3baV18hJnv3OQ2eOqtCf0EpdfPUu2aJHOouZN\nALooYKc6iawsFdNj0lR5v7aqkwVNJ24vMtA5hLGg6ChdUcSGrRu5kAmQsXJm+trlONk25kcWmffq\neLq6FU62k8yQ5PXTR7C0iI6VqtKwfM0Otj+ynTCdYmpBlTU74zJGHILpAIJ4qXckJXGzLkG2hsy1\nQE+4GNJDNz0sbZZMyoWiKofhT4HXyYKoQFpVQfoGV4Olo0W9oLWBSLb/kRrm2AQr2+7m7YMneXlO\nTa52Vzg8uH0PFDu52Y9VHMdXdm4hxAfuvNemFD/Krjei+Hnb3W+m3fbrt+223bb32S0TKajuxx9i\nS05bqtRBIpBJpCCWIgVDA00w1VDEHtHp88SzTzDy+n5e+MZfURxTqHiubTXN2Qi32WT07CSrH1US\n74SdnHl7H+OnL7O2dy0A+bYewmkfp7WIV2niJkBe051j0V/AzeXx+ooMPqa6w4cevYvUil4WnZCs\nZlFKhDys6gL4Dn6PRibp199z/wYylkndbeGHLkabAg9XDW1li2Ej52qs7+yHhAHp1mYY8+a4vDCL\n2aU+K/1l9GktdC0g0KpX5iaiS3SjRahDNtsiZSSSZFoDUlVmMpOU2mqU+pI+hfwUsR4xlW9R6VJR\nz8D6ItgmtXQOjXYyblJ9CJpEtRbp5VmqxSz5hCy2an1Mxq6BNQqsvP4bfx0WRdEVYpGiKCct9lJe\n2dH/S97Zb7bdMk7hAy0BFq/lmVyZRr30EOiaei0EMUrhF+D0G0e5Z9ODDG3YwqnZbzM5rlqCl/WV\nWTm0kfH9As8Bs7oUFrcYPT5KV7bI2geVcAiWBVEvzbGLtBarpBLOvRNKJl0X2d9DZvsG1j79CADn\nqDNhQmiDTUQtUXtqz6exA43agMFiVWEHnUYe39NoiQxmLoufUn/lYqOCTNuU83mGZ+fIR+oWuabA\nybfRvXw9FyqKLJWaN7H1KlndwRMLpFkS6tARmofUfexME91K3jcc5jMtzhbrpDsD7upV+Maw2WKt\n5VHLCcJ2VTKN+4fQUhZV26NmNVjfSpxCdyeusEl3Ftj+4bupnlbp2vadBbIlwL6+0uBPYkvpw5JT\n+CAHoOvXN4vxetOM/5KdzK3tFD7ANDRcHFIJzdm2bSXPFXn4fsRLR74JQNs6m8n6u/SuyfLU//oh\nqCd/amYXvBUz+c5lilGRg99TkuZr7tqEqMesGFwOGfVgh7VRmsJElMvkSm1LPVjYGZNX9r/CXdv3\n0HX3Zs45CmisF3KYxQzHj71LZ86mOqqilv5MjqjZYNTPwaJyCuNeCtu1sUQW0nXmAtUt5+sBWQtE\ntcKybBYvpYDQC2OXkF0lKhMzDG5TOIipd+P6F7Eys8TkEUvy6mmdpiVp5HwK7QInUQJq+j7/X3tn\nHmRXdR7437nr2193v97Ui/YF0dpAAsS+L2YsA3HFGzZOhjiJY8eeqqQ8cdUsrlS5KpVUJlXOzMQ1\nnmHsyYzBBC/BGGMwBmMwAgkQQgghtZaWulu993v9+m13O/PHua/VLUu2EELdgvtTqZ766vZ957z7\nzne/862jhg4ru1j30euRS5TxcIIywvW4Y+utZD51p7pG4zRu7SBmcpKyGKMaxvnHzGaSbW288PJ2\nLl7ewifuU4VaKs5Bdr++i6tu3PK7011nMdvFWF/Yvu9jGAaNYU/GarWK67qYpjnjnpz9u1LKmQza\n38V8LPbZngzHOREi6XnezM+u69La2oplWXMiFzVNm9GSzkc0Iyx0oSDnGj3cqoOesEgSx9CVmut5\nPjgOR/oP8/zIQcw2ZYS75uabWLS5nYmp1xCxAqm0Mqp5o3t57pldrFt9CYnxZvYMqPDnI7sPkLVS\ntLY1gauSUyaCEWSlgWwiTXlymmJZqeFlrUy+OkzeHWFRRp7wjtRc9mzfSWcujTc5wuVrVan4if4B\n4g2tlIOAhga1EO2iQTqWAtdk2nPJhM1JqpTIpi0mi2WaYyncKWWs3LS8h7wQWLUKh55VVZs35DZg\nxBM0tOUwjGamA2UkTHsGjWt7WHFZK/q16xnyldBySxZrr/4oa2URGpIMvaaqFHdv6AGvmcRbByg/\n9wIABzN96B9zyd4mSOOhVcJGJRmTTHcK89AY+V07qN6iQn+f3v0knelLoZJ7xxXeT16opmmSyWSw\nLCUQR0ZG8DwPz/N+I3ip/vdMNYVzyZkKGM/zZsZXf63Pob7QLcvCtm1M0zxvi/90RIbGiIiIOSwY\nTeG37vTq5bdsi5HCOGY2RTwsmjI0McnOR39Mc1cbn/6jz8H1YfRNboTBoR2UjTFWtrYThgEwdHCI\nojtNR2crIreM+HG1Px84dIjmtI6WMpFVpSk4ukPOEQTTRWKBpKldaRt9hYOsWddO+/IE095xEill\nxCyVq7TF4qR8D18XlCbVdTTDQtgp0s4U6TCvPvA8YmHtRMeTWGF5OVGT6J5LOhEnFo9TmVJ7dsdx\nMY0Y7bFGNEddI6np2DpUK9NMksepRzS2rKPl2uWQzpHf9zKarWwh3eYSeNNj8OE36J9CKfM+AAAc\nxUlEQVQYZ3KxSga7/S+3wS/eZPSXhygePwJA7+I+2m/OsaTcTNIUEA9tCrFxjK4Kk0d3s3HpMloy\nyqjo+Bl8uxlKvCNN4XRPW9u21dYQZQfwfX9GW5htFzidjeF0nMugpDN9ote3PqA0hfq2R9M04nF1\nb2KxGJZlzbuWAAtIKMCZlffLZZuoIaGq9uCpWJyNPetwNJ8jr7/B0kxoVNucor17EQNT4+TzFayw\nBERcSzI5uosyZZK5BKtWq4rLpWMl3KCIVyrihfsBK50krtnIwEEk4mCq4/2TB7loWw9t1y/l7f4j\nyDBHrxokaG9vYDp/HGF4HJtQ6ryVacZxqph+hcFC2KmqFjAiClhWjEDXcItheq5XRtQCTEMwWSiS\nyKqgq7eOHaQxtwjbStHWouYybU/RlCrj2EUMXZIVYTBSsRF2vMrIy//Kq+ZB2m9UjWZal2yi+r0+\ngl2TGJUKsXJoFNwV8Nqv92HmKzSGtQoWb26gbd0i4hgwUgUZVhNqn2TVbYtZ/uA+luptkFfpw5de\n83G0RAtn04V+9qKeHY9Qp25bqAuH2edomjaz7z7fvJM8ivpir2c7+r4/U5vhTN7nfNpCFpRQOBVi\ntqRwfLS4joakPKGewtlECmvzFtyjh/jpq6+x63VVHqzllpVc/eU76c5swT0+gplTtQ0Sa3rYdm0T\ng08Os0ofIbs6DA5KTDJwKE+5UiORUl9C0xVI6SBamiiOD3PkoCpi0h8cp2fVJmidRnoOrq4WTEJL\nYFZcVnYtIl8UdK1aBsCEZzJWCujJdjBZVE/nWDrJVLWM1DUSqST5gppPW6wRDMF0tUa5WiO7SGkh\n+uI20iKJXhPEwr4ChcZBaokJdH+cbFwnkQ8rEu0Zw316H0PbXye9soIRBiNSyXH0pRdptZbiZEw6\nLlHdp1i5iMl2nez6BPYyJRS6b2uhdf0ywIFSTZURA946PsjaFddx1dXXsev/7uWN4NsABPdtoO2K\nDC2WQ+sZNn6Y7VI8+fjshVSnLgBOxXzYFM6WU83j5M/i5JyOD6RQ+K1aQv0/NY1SqYhnC1oyYeJO\nECAPHESPaXxk20cYUrYznt6xiyv3+mgXr8V7o0L/wBEAlizK0d51DaOTD+Ik+rA61FM03REjO50i\nFktghN2l/Smo6CVqhRFeP/QWrRvVe378nk/BRpPDgy+T67yYwmhYvm3CpzvWRTJfolVPMDWhtJbm\n5g7SLmTGj6CH2XzSqiA1B9dwMfwKCaEMioZnEbOTxNIm5LIUp9XxhlQKbaLCkngzE0fDMOzsIYQ2\nSVAbIC4SMKm2GsHLA0wPTbFh/UZeKD/LyotUjUZWpRhLHWfaqqKv7abrnrXq+BUON62+EYYtyIYl\n75tGKDNJqW+Y3HQjFVNpLMu6roGJLtavWcuQ9xiTB5QRc+yFXXRfshxfnLn6O7tWQJ1TPRVnxyic\nfLyuip+pUJgvV+PJW566UKhnd3qeN7PFOJnzPeazbgYjhPhz4IuoEhU/kVJ+JTz+VeB+VCzSl6SU\nPztnoxWCTDJNjQACtVg0XyJicYRfZbh/gFiYWNQVW4Kmr8L98WGe+u5Oinn1pXmRIZYlu2iScQrl\nAtnyCABlfYqG9gYskcKbVueaWhI97TA2OYlImDSHDVFY0QrJUcx8gaRXQC+qG3xw+xC1YBCtWCae\ntBgZUgumYfUWNJkipfdjxpSE2zt2iLZ13bQsa8EtT2GHX+r9u99g+959XHbDLcSb29n7kmr17o5X\n6RBpcqlF7H1WeQiCP5hk0aYmDIogjJm6lQNTU/SO9tNsGZR70vhbwwzHDpfFf7qahlwHqa2rcZep\n9zwWPENHYgBvyVrsMPS5XI2hT3i0kITOFSTzyt2578fHKf76TdYevoiVdFAuqzkmjx6jq1DF6D7z\n58xs16K6vaf+8p9OO6hnTp5Pd93ZMNvFePIcZ3slZntWThWUdTrN6lxzVs1ghBA3ono8bJBS1oQQ\nreHxi4FPAD1AB/BzIcRqKeW5j2iJiIh4TzjbZjCfB/5GSlkLzxkJj98FPBQePyyE6AUuB148k8Gc\nrorzDJ4/0wTGK6i9+eTkJK2epxKiPB/LVypYbdKhsL2XHc/spKHWwYdv2QbA0f4CWqGGW+gDzadY\nU6nJU0Ge9kwjTOtUimr/nMo14huTLF7czZK1S3nlwHMAPPn1b3Pjn26m69oepgeHacqoGoiTI/vZ\n88RLXLd+M97EECtzyh5wrP8NjGQnUj+AF2oKr7/wGNd+/Bas5q1YxTyklL9/4vmXcfuPs/oj90Ii\nRzmujIdvD+zhitUr0PvGGPqJGsfGe7owzBzkp6HmQEIZTbs3rGV04yhTyTE2/9k2zOuUraHojNL1\nR1eAJyinpzlI2EdTr9HdNECx2kwxNCgG0wGtNQN8HfoGePZRVTr/2e8eYdnwJoypGJ2+R7mgsjiv\nv24dqVwrU2dyo0NOLrt28lP/ZF/+qTSK+fQ+nOn7BkHwG8FV9a3P7G3Pb5tLXYtYKJrCqVgNXCuE\n+DpQBf5SSrkD1fhl+6zzTtsM5uS+D4KARP1+edpcCRGOspbQ0PBJ+AGElYBG0i755BSLHY1FZRuk\napC61mniuf/wFOvX97B080boV0VWlloaLDahtRVqFRhTPSlzeqtqLCjzpDPhm7tjDMWTJBI6hia5\naPHlAKSOtNH58mZwL+XQN/4RIR4HIJ2MU5YOY5aLtbQDL1DX7lycplg8yKTVRDqsjrTcaKKrKMHJ\ng/0maEPqGs2vsnbJbbw6kOPv/+4xPnS9MgZ++s9vheE8g0+8QS7oUZ/HVsmIczGt+i1g7wf/KTXu\n+0xW37WYY4Vm6KpSqfQCEJ9IgrccKg0kmlKsH1HvSVMnLHqDymiZ5g5lB6naeWQ1jRB3MPT4KC/9\nswpe0mLrsbddzGt5jb5YCy3NqrrUy4N7ufKno1y0bQ20nOqOn57T2QNOXgDnYkHMh03hVNGW73S7\ncz49EGcrFAygEdgKXAY8LIRYzjtoBjO778OlW7acWnyr8s0zFwgIkIGPiVALmDASzLQwPJUoUQxb\nvk3HNVZsvYRUazNYkiCtLOLTpTyZeBOVcgVf85Hh+bquKjUQnJDepmmStRPgg1Mq0ZBU/rZiLMnr\nzzxHx74DOMXSTFPSUs1Dc32qhSLS9bDCjsLUfBK6zZTUEGHzGTRdpXobvqoiFfYWcL0YpWmXZ3/4\nIL7r8/TP3wLgptw1dJQ8nv7lL7ntys8A0FYuwVtVim/3MnR0B42LVDBGURunraedtZevZ2r8efRw\n3KQbQVjUCiPYTXmCdFhBWk4RK42SzWiM58P2cLUK6dQyKI1yrPgKY5oKB/+Pf/vX/M/v/YCCiOGm\nW7n7L+4FYP///gXOqhFoOuWdjLiAOFuh0A/8IKze/LIQIkD1Gj7jZjCzEYShlbMqLM3+G8w5cy6a\nrLt3JPjgZ5RBzGxrJB63iTdmKItgpsN4TZjk3QpFzQENdF1dU9OUShcEAXYYYBSP22hVl4RtkS9X\n0eIqKicXT7C/r5/K6CTLli2jrU25Nd1qlfamZrLZLFKAGQbf+GUX07LU6GW92pGJowckzBr4cqZD\nlO83kM00kU15DL65m2s2Kg9Bx6qr4flX0OwYw1WVU7H/D3+CX+zg6F4PIQXJhjAYKu6gd77IpruH\nWHVPG4EZVnsybWRKMGyMommjJJP1T9UkNtxOQlRJNIX5E4EDIxIOHWHvvle5/aMqEzS52MXoAqPB\nZowxnCalgSUuNxhrH2UP46yj7gONuBA5W6HwI+Am4FkhxGrAAsaAR4HvCiH+C8rQuAp4+UwuqBaM\nNvdA+NefOaSp1etL8NQT3nUcDMdHSAN0g2w8XFzJNIXCJEE8hhABXtgJqiGT4fjxQeyYRRB4qrQT\nIKSGhkAIDRGOQ/oBslyFeJK0YSMLyuWnOz7dbYvIphtoa+9GlpRLslwq05DKIHyBZphghG3Zpqch\nFsOSJ67tBxpV4YAo4WsBelgwVcpWpgsO93/6o+RWtXFlj8rYrByp8bNHXqBmGewbexWANlbSlO1g\n1R0ryTVdRGFUic/eI/s51Ps8v/hfz5NpvIa2m5TbNZ4TFJwx9Ng0KV0jiRIW08ddeKODA2/spmGx\nsmHEUhrpbCe0XU9mupclcTUOveNWbrraZs+BMVatWYUWlnq78bb72Z+3ieuRQLjQOatmMMADwANC\niD2oyvifDbWGN4UQDwN7Ua7KL0Seh4iIC4t30wzm06c5/+vA19/NoGZrCXKOTQGMejmW0IrsexIp\nQbdiYKYQxbBz1OAEDYaG0z8GwqcxLGLiDudJVQKSho7reciweahh6pimBQKEH+YhOBpS18F10HUT\nN9QIKtMl2lvbyGYa8YtFikV1vFgqkU414LoetqHNDLzm+MQrLrZloYV7Ic8LKHlVoExFQEoL+z1q\nzRx+u4/1t03yB//2o+x8TMUpPPQvu8gfLXDJph5alyitZ9/RpYzHJlj7+1swOy9j6mmVxzE0NkLa\nWotbHuNH33qO+9o/C0D8ulasfD+5bAyt2knlJXWd/T8fYOCRH5Gv5aiayhAq48fpWXWE6+9YTVNx\nFa98Xxkgjxx7ilv/4sOsuyLN0w89yt99X9VovOTWq+jauoHEu7rxEQuBBRHRqGwKJ20dADS1dair\nGrJuXZACDKX62nYcK5ZQNQQNG9mv8g2mjg2xsmctw/191JwyubAn41DfIZYsW0JQ88GVEFZAtg0T\ntFgYACTrbwi2hpvPYxgGWmiA1Awd24rjy4CxfIFkmPZsS4mWSCArFUikIDRA6naCAB2zVIYwBFjW\nXBXHT4CjCwj7PuiaxbGDh9m9/QVuWNrCU48/CkDpsE/PiiXc8OnrmKwoo9/XXh2mFn+FI/0jfOnO\nZXzzWw8DUDWnWRJzuPnGj/LTnzzE60+qff/W5YvItnfD0TEOPj7MK99TRVqHXjPo6ta54tK7ee41\nlUpenfZ59dXnKR4bp9HaRtpUVZqGDk/w1nM/Y9WWNnb+6gfkx9UW6bFvP8GfrLqYTNe5bzAbcX5Z\nEEIBTjIhhj/UBUJdU5BIfDxABzuswpxJYcYrBCUfrVxgfErFHSQbMhQLk9QCD5eAvoFjALhC4mng\n+AHoAiPM2SdmgqnNVU00DaSLFwR4jlNvNYFuW0hdo+y4GIk4I2EeRrlWpabpuI5Pm26im+rasbAX\nBRPDSvAAmltDkxpC2AjNmuljIXDArbFm6Roqk2U6O9SefWBwGC9ZhsUNTB8Lr9GhI2Mefd7bjGnD\n9Bt96jNps6lKl6Xr13PdpM+uF1Rcw9brXZCN9D7+Fj97cJjp3qUAtDau486vbAXtEp6eUkVrXU1j\n1YrFjB8e5OjgCxgJNb6R2gBXdl6CnnMI4oeZdpQwN5JttLeW0TgErHkntz5igbFghMJvEK5NtT5P\n+B8CGRZoM9TTyIwn8CyT8fEy/kSxbn+ka81yfvXMMyzqaMGIJ3jrgOoQdcWVlzNZq6BbBoZhosfV\nAgtsiwCJ6zrIUCqZmonp+8RTKWrFMpMTKqQ3Hk8iYia1mkM8kcILWwxSMnBqHlNTRSw7MRPvnmls\nwnEcWotFsJV6bro1zMAAMmhyGoTa9kjybOxZw6KN14CscvvttwCww3iGzRf3QHOMrmZl9Ptc7U2O\nTo6y9ZZ1pAObf3f/nwGw6+e/5sa1SzFWXsqyIz7bfxLGjg1kYXiQp//fYRxvFVd96lYAWrtzxO7o\nhGoLGxKbADg0MMrdH7sLOeLwjb9+HC+uBN9N2y5n5YcuZaL/AHd95l5e274LgBXr15FuNvBJvoub\nHrEQWJhCQfymMADQEOhCU5qErwKDki05quNlasUqRw4fZqmlImcqk3kuWrmCw32H0A2Nyy5R7du1\nQDI6NMzylcvRDB0RCgDhBeiaBrpZby6NX/Pxi0WEpgEaDQm1+l1f4pVrxAwbLRAEFTe8hupw3phq\noDpVphaWjPOqAQ0NDSBrYCkhJIIqemBBLQYxG9LqVpTL/VxyyfWM79zHd5/5BSs3qVyED3/+DsiP\n8Z3vfpNdB5U29J/u3cTNt9zDRO84X/v3f8tlWz4CwKc+8xko9rLnhw8ytL1AZ2opAPse2c1o4RjV\nIxnar1zKtV9WTWXoPMRD/7iHV3pf5cZ7LgPgk1/6HEOHBnnqB88Rb+lhaPoIAFd98g6++Q//ld69\nU9x+1TXce//nAZgqHqU0bpJu6XpH5dgiFh4LN4skIiJiXlhYmsIp4iEFs+2OARpC7b/DijXJLZsZ\nePMgu/fvpTWXRQ4qT8DAyBi+79IST5NIxNHD0ku6b7CyrRMrCOP6tfAjkDq4AiPwwVVPfs8L0LEJ\nHRQY4bm2BmgGmmEidJ3hktIItFCLicdjFCtFqKlfLJbGkeUajfESIggNirapGth6KQjiYCndqKnZ\nZrIwyWtH9zA6Ien71S8B2HKVzcHeN3ni188Qa1DFUp964p/5WNOXePCbT0KhkQN7jwDQ2/s6BoM8\n9uun2GhdjgxLzfe9egxd17lqzc386tgb7Nj9gPqMg1d45cltFGNlenerGIjNGzt58pFdHNjpE0yX\nWH+9yu945qHvs//1Karj3Tz5/f2sXaKMrJn2gFxnD844hPVsIi5QFpZQOAklEIIZbVQQuiR9/4SO\nk4yz+tqtDI6M03+ojw31CsCBh4FOS3sLsUSMclkFHmFoWNmMMiIaOpjhRyAMkAE4Ej+0S3iBP1Pp\nJ+BEYQzDstEMFSyFELS0q4hG07SpVqtkGnLIgQFSSXV+Pp8nZts46Th2LBRxmRSemcARSVwZg4Ly\nmkyUirSmMshalnx1mDJK4JiNJqm2JC6CwphKWqqsbIYVW8h2DdJo+kyh8hOKqTGam5KwuIneg8dp\nTimBmHE00lYjew72ks+V6Fy7PLzOOB0ZjXEp8abUNRpNyaKGGONJQbHq07NG9cLwG6sEgUN+ahQ7\nbWGHNpl0xgABVvxd3vSIeWfBCAXJXEVBEJxybyNAJUNV6+5JD9av4YaGJp58+BF2/lz59bHAsg3y\neom0JUg0qieaFTNxTRPHqYLuo4Wt1qRQ3gVXd3F0ZQz0pEezYeIHAVIKhBHmRFg6umWAUPn8RwvK\nAGkYBmNjE+RKU/QdOkw6pWIP8pNTNDQ0MJ4so00pN+CggKxusyjeQKyhg8BR2okebyFINHDbNXcz\n3vIr7GYVNe6IOMtXrOez9y1i32Fl27j2piaqkzE23HQD/tuvcOlaZX9oXR/HqQnW3X4ddr/F7kd+\nCkDBKPFvvvJVlvSNs9+aYCQsCa+R5qaPxKnYHTSvUUJVWj6X3LQIDYeEtp5lmzYAELQ5bBiboP9w\nhZsvu4aWjaq6FAmoBlM4Voa63TXiwmTBCAU44Qk8kQIR/IbNqlatYlsJ1VAWwHPULJa1cNv991LY\nqLogaxrUajUKpQKOU6Mc5jhM+Q7VahXLShPMSqMNgnofgVmlsAJITEnVoQhJvXuqYdoYhqH6WgYB\niSalWpumSXl4jHhDI6mMTi6nQn5jU0Wy2Syj2gBGGI9gJzKUsymOVUv4UxWSZXXtwFrMi6/30n6V\n5Oa7b+Hw6EsAfO9fHqIjlWLjhm1cceVmAJ5/4lF2+Du45c5r8XLDlA3ldv3BY4+QSa3h5ut+j4ne\nYZZ46vzCsUEqqy0yG9fQ4pT5wYvPAhDva+LLty7hyIhg/5jSFH6480dsu2M9l/3eJmoTi/nJ9rcB\nWHntYjbccR1XJ5IsTi5BhqLbBaY0nSmIhMIFTmRojIiImIM4l0UnzpYtW7bIl3bunJFQ6pkeMGPh\nC6u3Bo6HZppILzgRSGRa4KpGMWbCOqFuuFX1+DdCF6as91kUSo3wPBUqHcyKkgxE2IIuHIkQMBae\nJzjRqk7TTlhApTzxnrYN0yVIZ2B8AsJIR0olFWwVq4AIjxVqyKwgn81TdodoCZTmYx6XiMbVHB4d\nw17ZRt+ASp1uEQErcx1gpJgYV01pDD9NvlQgkQM7N07MVAbFkSkXw+8mbqWx7RrVmipdl0m2c2ho\nkGJ1gvaubooF9UyvlbIsTe+G+FoKYRZn39iLtGZHaPAbScurKLlqfIWYj5WdwCbACAyymgofn3Jq\nBFaSEio1NmLhIYR4Rcqw4MhvYUFsH05dcGHWa/hvUY9eEFCsKGNb1rYIbHCEha+BXd9vWBaBDFB9\naCVVJ+wHQUDciBEYYSZkMOsjCFSWZB0hdER3Ym5KN7P6WdZ/Ley3YOoG5Ex1RrbxhHAJzFDYWDN1\nE0jHCWxwtTKa3YhVTzc2ciAhYzpIG9YsV7EEjdKEaaA2TVNDOOZYkozWBHqNsidwKmqAnbElSp+v\nSkhOcHxE1U3Qkqtpas/QwCGyTNKebToxr9RS8hWDMDucdd0biDGEXrbQywZ2eDwT13FoxMRDFH2E\nreaYrbigQ1KDU3mRIi4cFoRQgN8iGGZpMsIw8apVjHiCbIMqRlqsOEhDx7Z0PECEC9TSNTShIcNA\n6Vg9aIh6OLH6w+wKOFr4jQ5O5D6M6idicYJZrwFSpVqD8kKo30TXBOChaQIjnJUerhIfGyNUWAwB\nVQ3AwiSNF9ZTMDygBumWLIeDozRpKu3Zd8CoonIqYmEXab1EYapAMtOEZXRiiDDnYCIcqCmgUmNp\nh/IyDFcgFocYPgYTUAhDvPMXQSpByhD44cdR8vJYhgl6Tn1LwjpruguJtISyrvJNSuF/WD5MlLDi\nyXfcNi5iYbFgtg87d+6c72FERLyvOdPtQ2RojIiImEMkFCIiIuYQCYWIiIg5REIhIiJiDgvC0CiE\nGEU1MR+b77GcR5r5YM0XPnhzXmjzXSKl/J1dORaEUAAQQuw8E8vo+4UP2nzhgzfnC3W+0fYhIiJi\nDpFQiIiImMNCEgr/Y74HcJ75oM0XPnhzviDnu2BsChEREQuDhaQpRERELADmXSgIIe4QQrwthOgV\nQvzVfI/nvUIIcUQI8YYQYpcQYmd4rEkI8ZQQ4kD42jjf4zxbhBAPCCFGwlaC9WOnnJ9QfCO857uF\nEJfO38jPntPM+WtCiIHwPu8SQtw56/++Gs75bSHE7fMz6t/NvAoFofqy/zfgQ8DFwCeFEBfP55je\nY26UUm6a5ab6K+BpKeUq4Onw5wuVbwN3nHTsdPP7EKr58Crgj4F/Ok9jPNd8m9+cM8A/hPd5k5Ty\ncYDwe/0JoCf8nf8efv8XHPOtKVwO9EopD0kpHeAh4K55HtP55C7gO+G/vwPcPY9jeVdIKZ/jRNJ2\nndPN7y7g/0jFdqBBCLHo/Iz03HGaOZ+Ou4CHpJQ1KeVhoBf1/V9wzLdQ6ASOzfq5Pzz2fkQCTwoh\nXhFC/HF4rE1KeRwgfH2/FUc/3fze7/f9i+G26IFZW8ILZs7zLRROVaPn/eoOuVpKeSlKdf6CEOK6\n+R7QPPJ+vu//BKwANgHHgb8Pj18wc55vodAPdM/6uQsYnKexvKdIKQfD1xHghyjVcbiuNoevI/M3\nwveE083vfXvfpZTDUkpfShkA3+LEFuGCmfN8C4UdwCohxDIhhIUyxDw6z2M65wghkkKoiq1CiCRw\nG7AHNdfPhqd9FvjX+Rnhe8bp5vcocF/ohdgKFOrbjAudk2wj96DuM6g5f0IIYQshlqGMrC+f7/Gd\nCfNao1FK6Qkhvgj8DFUK8QEp5ZvzOab3iDbgh0JVgzaA70opnxBC7AAeFkLcDxwFfn8ex/iuEEI8\nCNwANAsh+oH/DPwNp57f48CdKGNbGfjD8z7gc8Bp5nyDEGITamtwBPgTACnlm0KIh4G9gAd8QUrp\nz8e4fxdRRGNERMQc5nv7EBERscCIhEJERMQcIqEQERExh0goREREzCESChEREXOIhEJERMQcIqEQ\nERExh0goREREzOH/A0ziN7lEArbEAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea8c92e438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "print (plt.imshow(pic_arr2[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea8cac4208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (plt.imshow(pic_arr2[2000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AxesImage(54,36;334.8x217.44)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1ea8db44dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (plt.imshow(pic_arr2[3000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categories\n",
    "\n",
    "Cdiscount provided the 'category_names.csv' file shown below. This file contains a list of 5270 different 'category_ids'. However, I made modifications inside this file and used an excel function to append a new column, 'cat_1_encode'. This column basically encodes the 'category_level1' column into integer values ranging from 0-48. I have also appended a column 'catID_encode' which encodes the 'category_id'; so 'catID_encode' has integer values ranging from 0-5269. This is because there are 5270 different categories in total. I also appended a new column, 'cat_2_encode' which does the same to the 'category_level2' column (however I did not use this category level in my project)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category_id</th>\n",
       "      <th>category_level1</th>\n",
       "      <th>category_level2</th>\n",
       "      <th>category_level3</th>\n",
       "      <th>cat_1_encode</th>\n",
       "      <th>catID_encode</th>\n",
       "      <th>cat_2_encode</th>\n",
       "      <th>cat_1_encodep2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000021794</td>\n",
       "      <td>ABONNEMENT / SERVICES</td>\n",
       "      <td>CARTE PREPAYEE</td>\n",
       "      <td>CARTE PREPAYEE MULTIMEDIA</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000012764</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>AMENAGEMENT URBAIN</td>\n",
       "      <td>ABRI FUMEUR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000012776</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>AMENAGEMENT URBAIN</td>\n",
       "      <td>ABRI VELO - ABRI MOTO</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000012768</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>AMENAGEMENT URBAIN</td>\n",
       "      <td>FONTAINE A EAU</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000012755</td>\n",
       "      <td>AMENAGEMENT URBAIN - VOIRIE</td>\n",
       "      <td>SIGNALETIQUE</td>\n",
       "      <td>PANNEAU D'INFORMATION EXTERIEUR</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category_id              category_level1     category_level2  \\\n",
       "0   1000021794        ABONNEMENT / SERVICES      CARTE PREPAYEE   \n",
       "1   1000012764  AMENAGEMENT URBAIN - VOIRIE  AMENAGEMENT URBAIN   \n",
       "2   1000012776  AMENAGEMENT URBAIN - VOIRIE  AMENAGEMENT URBAIN   \n",
       "3   1000012768  AMENAGEMENT URBAIN - VOIRIE  AMENAGEMENT URBAIN   \n",
       "4   1000012755  AMENAGEMENT URBAIN - VOIRIE        SIGNALETIQUE   \n",
       "\n",
       "                   category_level3  cat_1_encode  catID_encode  cat_2_encode  \\\n",
       "0        CARTE PREPAYEE MULTIMEDIA             0             0             0   \n",
       "1                      ABRI FUMEUR             1             1             1   \n",
       "2            ABRI VELO - ABRI MOTO             1             2             1   \n",
       "3                   FONTAINE A EAU             1             3             1   \n",
       "4  PANNEAU D'INFORMATION EXTERIEUR             1             4             2   \n",
       "\n",
       "   cat_1_encodep2  \n",
       "0               0  \n",
       "1               0  \n",
       "2               1  \n",
       "3               2  \n",
       "4               3  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the CSV file which has the categories names of each of the category IDs\n",
    "categories_df = pd.read_csv('category_names.csv')\n",
    "categories_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Final Training Set - One hot encoding 'cat_1_encode' column\n",
    "\n",
    "In this project, I am consuming tons of memory when trying to divide my numpy arrays by 255. I originally had a laptop that only had 8GB of RAM and that always ended up in a resource exhaust error because I did not have enough memory to divide the images.\n",
    "\n",
    "I went to attempt to use the Google Cloud compute. I had the virtual machine set up in the cloud. However, there were even further problems that arose from trying to use cloud computing. First, when I transferred files from my local workstation to the cloud, it took around three days to transfer over 200,000 images. This was most likely that my bandwidth was slow, but even with a better bandwidth, it still would have taken a while. Another problem with Google Cloud is that renting out a server with GPU compute capabiltity cost over 1000 dollars a month which I did not even want to attempt setting up that workstation, even with the free 300 dollar credit that they initially give.\n",
    "\n",
    "I decided to just buy a new laptop with more compute power (32 GB RAM) and a NVIDIA GPU. Even with a lot more memory, my new computer often froze when trying to divide the whole 50,000 numpy array by 255. \n",
    "\n",
    "I am dividing the image array by the value of 255 because it is the maximum pixel value and we need to get each value in the picture array to be a value between zero and one. Our predictions will be much more accurate when we have our input values closer together. \n",
    "\n",
    "I am going to only use 1,000,000 of the images provided to fit my models due to the amount of time training. These 1,000,000 images, I need to reformat by dividing each picture's pixels by 255. If need be I will increase the amount of data I will use in this project.\n",
    "\n",
    "I will create training data corresponding to the 'cat_1_encode' column. In the code below, I am also saving the training data into H5 files so that it can be easily accessed later without taking up too much memory on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py \n",
    "\n",
    "# I am going to categorically encode the 'cat_1_encode' column into an array of values from 0-1. \n",
    "# Then append this array as a new column in the dataframe.\n",
    "#catID_HOTencode_arr = np_utils.to_categorical(np.array(categories_df.catID_encode))\n",
    "#categories_df['ONE_HOT'] = list(catID_HOTencode_arr)\n",
    "catID_HOTencode_arr = np_utils.to_categorical(np.array(categories_df.cat_1_encode))\n",
    "categories_df['ONE_HOT'] = list(catID_HOTencode_arr)\n",
    "\n",
    "# There are 140 H5 picture files containing 50000 images each. Reading in each file at a time\n",
    "for i in range(1,7):\n",
    "    \n",
    "    if (i > 1):\n",
    "        from keras.utils import np_utils\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import h5py\n",
    "        \n",
    "        categories_df = pd.read_csv('category_names.csv')\n",
    "        catID_HOTencode_arr = np_utils.to_categorical(np.array(categories_df.cat_1_encode))\n",
    "        categories_df['ONE_HOT'] = list(catID_HOTencode_arr)\n",
    "\n",
    "\n",
    "    # Read in the category ID\n",
    "    h5f = h5py.File('Category_ID' + str(i) + '.h5','r')\n",
    "    cat_arr = h5f['dataset'][:]\n",
    "\n",
    "    # Read in the product image\n",
    "    h5f = h5py.File('Product_Image' + str(i) + '.h5','r')\n",
    "    pic_arr = h5f['dataset'][:]\n",
    "    h5f.close()\n",
    "\n",
    "    # Create a temporary dataframe, which binds the categoryID array with its associated picture array. \n",
    "    temp_df = pd.DataFrame({'category_id':cat_arr, 'image': list(pic_arr)})\n",
    "    # Set these values to None to save memory on my machine.\n",
    "    cat_arr = None\n",
    "    pic_arr = None\n",
    "\n",
    "    # Now to merge this temp_df dataframe with the categories_df dataframe, I am going to merge on category_id\n",
    "    # The result of this is a dataframe, that includes 50000 records that have the categoryIds that were located in the H5 file\n",
    "    # and also now has the categorical array which represents the true class of the general category\n",
    "    temp2_df = pd.merge(temp_df, categories_df, how='inner')\n",
    "    temp_df = None  # Set value to None to save memory\n",
    "\n",
    "    # This is used to randomize/shuffle the rows of the dataframe. So that we can have random training and test validation sets.\n",
    "    temp2_df = temp2_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Now to create my train and validation sets, 20% (10000) of the records are for validation, 80% (40000) of the records\n",
    "    # are for training\n",
    "    x_train = np.array(np.array(temp2_df.image.tolist()) / 255, dtype = 'f')\n",
    "    y_train = np.array(temp2_df.ONE_HOT.tolist(), dtype = 'f')\n",
    "\n",
    "    h5f = h5py.File('x_train' + str(i) + '.h5', 'w')\n",
    "    h5f.create_dataset('dataset', data=x_train)\n",
    "    h5f = h5py.File('y_train' + str(i) + '.h5', 'w')\n",
    "    h5f.create_dataset('dataset', data=y_train)\n",
    "    h5f.close()\n",
    "\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Training Sets - One hot encoding 'catID_encode' column\n",
    "\n",
    "I will also create the training sets for the 'catID_encode' column. This means that each of the pictures provided will need to be reformatted by dividing each pixel by 255, similar to what is done above. The difference here is that each of these pictures will be associated with its respective label that ranges from 0-5269, because there are 5270 unique category IDs.\n",
    "\n",
    "I will save 1,000,000 images and its respective category ID label in H5 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py \n",
    "\n",
    "# I am going to categorically encode the 'cat_1_encode' column into an array of values from 0-1. \n",
    "# Then append this array as a new column in the dataframe.\n",
    "catID_HOTencode_arr = np_utils.to_categorical(np.array(categories_df.catID_encode))\n",
    "categories_df['ONE_HOT'] = list(catID_HOTencode_arr)\n",
    "\n",
    "# There are 140 H5 picture files containing 50000 images each. Reading in each file at a time\n",
    "for i in range(0,41):\n",
    "    \n",
    "    if (i > 1):\n",
    "        from keras.utils import np_utils\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import h5py\n",
    "        \n",
    "        categories_df = pd.read_csv('category_names.csv')\n",
    "        catID_HOTencode_arr = np_utils.to_categorical(np.array(categories_df.catID_encode))\n",
    "        categories_df['ONE_HOT'] = list(catID_HOTencode_arr)\n",
    "\n",
    "\n",
    "    # Read in the product image\n",
    "    h5f = h5py.File('Product_Image' + str(i) + '.h5','r')\n",
    "    pic_arr = h5f['dataset'][:]   \n",
    "        \n",
    "    # Read in the category ID\n",
    "    h5f = h5py.File('Category_ID' + str(i) + '.h5','r')\n",
    "    cat_arr = h5f['dataset'][:]\n",
    "\n",
    "    h5f.close()\n",
    "\n",
    "    # Create a temporary dataframe, which binds the categoryID array with its associated picture array. \n",
    "    temp_df = pd.DataFrame({'category_id':cat_arr, 'image': list(pic_arr)})\n",
    "    # Set these values to None to save memory on my machine.\n",
    "    del cat_arr\n",
    "    del pic_arr\n",
    "\n",
    "    # Now to merge this temp_df dataframe with the categories_df dataframe, I am going to merge on category_id\n",
    "    # The result of this is a dataframe, that includes 50000 records that have the categoryIds that were located in the H5 file\n",
    "    # and also now has the categorical array which represents the true class of the general category\n",
    "    temp2_df = pd.merge(temp_df, categories_df, how='inner')\n",
    "    del temp_df  # Set value to None to save memory\n",
    "\n",
    "    # This is used to randomize/shuffle the rows of the dataframe. So that we can have random training and test validation sets.\n",
    "    temp2_df = temp2_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    # Now to create my train and validation sets, 20% (10000) of the records are for validation, 80% (40000) of the records\n",
    "    # are for training\n",
    "    x_train = pd.DataFrame()\n",
    "    for k in range(0,50000):\n",
    "        x_trainSubset = np.array(temp2_df.iloc[k]['image'] / 255, dtype = 'f')\n",
    "        temp_df = pd.DataFrame({'pic_array': [x_trainSubset]})\n",
    "        x_train = x_train.append(temp_df)\n",
    "\n",
    "    del x_trainSubset\n",
    "    del temp_df\n",
    "    \n",
    "    y_train = np.array(temp2_df.ONE_HOT.tolist(), dtype = 'f')\n",
    "    del temp2_df\n",
    "    x_train = np.array(x_train.pic_array.tolist(), dtype = 'f')\n",
    "    \n",
    "    # Export train and test sets to H5 files\n",
    "    h5f = h5py.File('x_train' + str(i) + 'catID.h5', 'w')\n",
    "    h5f.create_dataset('dataset', data=x_train)\n",
    "    h5f = h5py.File('y_train' + str(i) + 'catID.h5', 'w')\n",
    "    h5f.create_dataset('dataset', data=y_train)\n",
    "    h5f.close()\n",
    "\n",
    "    from IPython import get_ipython\n",
    "    get_ipython().magic('reset -sf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Model - Logistic Regression - Ignore this section, wasn't able to use this as benchmark model\n",
    "\n",
    "My baseline model is logistic regression. I will only use 10,000 image observations to run and fit my logistic regression model through. This is because of memory limitations for sklearn logistic regression package; I initally tried to do 50,000 images but there was no sign it was ever going to finish running. I will use 10,000 image observations to validate the fitted logistic regression model. \n",
    "\n",
    "The goal here is to see the accuracy that logistic regression has when classifying the images in this dataset. I ultimately want to build a convolutional neural network which should have a higher accuracy then this logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Read in the category ID\n",
    "h5f = h5py.File('y_train1catID.h5','r')\n",
    "cat_arr = h5f['dataset'][:]\n",
    "\n",
    "# Read in the product image\n",
    "h5f = h5py.File('x_train1catID.h5','r')\n",
    "pic_arr = h5f['dataset'][:]\n",
    "h5f.close()\n",
    "\n",
    "# Transform this into a picture array from three dimensions to two dimensions. This is because sklearn's logistic \n",
    "# regression package can't accept more than two dimensions\n",
    "dataset_size = len(pic_arr)\n",
    "pic_arr = pic_arr.reshape(dataset_size,-1)\n",
    "\n",
    "# Now to create my train and validation sets, 20% (10000) of the records are for validation, 80% (40000) of the records\n",
    "# are for training\n",
    "x_train = np.array(pic_arr[0:40000])\n",
    "y_train = np.array(cat_arr[0:40000])\n",
    "x_val = np.array(pic_arr[40000:50000])\n",
    "y_val = np.array(cat_arr[40000:50000])\n",
    "temp2_df = None\n",
    "\n",
    "# Initialize the logistic regression classifier for multi-class\n",
    "clf = LogisticRegression(multi_class = 'multinomial', solver = 'sag', n_jobs = 4, max_iter=10, verbose=1)\n",
    "\n",
    "# Fit the model with training image data and their true label\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# Get the accuracy score of the fitted model on the validation set\n",
    "clf.score(x_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Statement - Using Validation Accuracy as Evaluation Metric\n",
    "\n",
    "In this section I have tested various convolutional neural network structures to determine which would obtain the highest accuracy score.\n",
    "\n",
    "A convolutional neural network is one of the best ways to categorize image data in today's world. The reason for this is because of its success in image training competitions such as ImageNet. Another benefit of convolutional neural networks is that you can combine nodes together from one layer to the next to create even more complex non-linear classifiers. With 5270 different images in the dataset, it is probably safe to say that we would need a complex classifier that can distinguish many unique patterns within various images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CPU and GPU summary\n",
    "\n",
    "This is the information on the CPU and GPU that I'm using. I am using a NVIDIA GTX 1070 GPU and have set it up correctly to work with Tensorflow. The GPU on my computer will make it a lot faster to train the convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10259264407376851264\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6762058875\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 1234559033269605352\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "svmem(total=34314883072, available=31125389312, percent=9.3, used=3189493760, free=31125389312)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import psutil\n",
    "psutil.virtual_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Solution 1\n",
    "\n",
    "#### Experiment 1 - Based on Solution 1\n",
    "\n",
    "I am going to use the following structure for this neural network, this is the first neural network in which the goal is to classify each of the images into its general category that includes 49 different classes.\n",
    "\n",
    "We want to specifically declare the Relu activation function because it mitigates the vanishing gradient problem. Since the gradient of the relu activation function is one if the output is greater than zero, is is much easier for the backpropogation step to more quickly find a local or global minimum which minimizes the error function in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 89, 89, 80)        3920      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 89, 89, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 44, 44, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 43, 43, 60)        19260     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 43, 43, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 21, 21, 60)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21, 21, 40)        2440      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21, 21, 20)        820       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8820)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 49)                432229    \n",
      "=================================================================\n",
      "Total params: 458,669\n",
      "Trainable params: 458,669\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Initialize neural network\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer of the neural network is the image array which will have the shape of 180px by 180px and have a depth of three.\n",
    "# There will be 80 filters/nodes in the second layer. The kernel size will be four, which means that each filter will cover a 4px\n",
    "# by 4px area to find a pattern. The filter will move across the image array two pixels at a time. Using the 'relu' activation\n",
    "# function.\n",
    "model.add(Conv2D(filters=80, kernel_size=4, strides=2, activation='relu', input_shape=(180, 180, 3)))\n",
    "\n",
    "# Implementing a dropout so that we do not overfit any of the nodes in the second layer. This just means that we randomly turn \n",
    "# off some nodes in the second layer so that others can get trained as well.\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Implementing pooling layer so that we reduce the dimensionality of the array. So that we can focus in on more specific patterns\n",
    "# in the array\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=60, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 40, activation = 'relu'))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 20, activation = 'relu'))\n",
    "\n",
    "# Flattening out the array so that we can predict into one of the 49 different classes\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the output layer where there are 49 different possible output nodes that the product can be classified as\n",
    "#model.add(Dense(units=5270, activation='softmax'))\n",
    "model.add(Dense(units=49, activation='softmax'))\n",
    "\n",
    "# Compile the model, loss is categorical crossentropy because we are minimizing the error across more than two classes\n",
    "# The optimizer rmsprop is efficient with quickly minimizing error. And we want to minimize error due to accuracy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training my model\n",
    "\n",
    "To train my model, I am going to use a for loop to read in each of the H5 files. With the image array data, I am going to feed that into my neural network to train it. So 50000 images are going to be fed into the neural network for one iteration of the for loop, then the next 50000, and so on. I am going to load the model weights from the previous epoch so that my neural network can continue training, but on a different set of the next 50000 images.\n",
    "\n",
    "I found that I needed a GPU because it took a very long time for me tor train a neural network using just a CPU. I was also only able to train my model with 20 images at a time (batch size) due to an initial error I received that says that my system was exhausted of memory resources; I initally wanted to train my model with 1000 images at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "39980/40000 [============================>.] - ETA: 0s - loss: 1.9115 - acc: 0.5402Epoch 00000: val_loss improved from inf to 2.10149, saving model to weights_best.hdf5\n",
      "40000/40000 [==============================] - 844s - loss: 1.9116 - acc: 0.5402 - val_loss: 2.1015 - val_acc: 0.5756\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "39980/40000 [============================>.] - ETA: 0s - loss: 2.5832 - acc: 0.3566- ETA: 6s - loss: 2Epoch 00000: val_loss did not improve\n",
      "40000/40000 [==============================] - 695s - loss: 2.5835 - acc: 0.3565 - val_loss: 2.7337 - val_acc: 0.3516\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "39980/40000 [============================>.] - ETA: 0s - loss: 2.5850 - acc: 0.3415Epoch 00000: val_loss did not improve\n",
      "40000/40000 [==============================] - 938s - loss: 2.5848 - acc: 0.3416 - val_loss: 2.4990 - val_acc: 0.3504\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "39980/40000 [============================>.] - ETA: 0s - loss: 2.5837 - acc: 0.3382Epoch 00000: val_loss did not improve\n",
      "40000/40000 [==============================] - 993s - loss: 2.5838 - acc: 0.3382 - val_loss: 2.6028 - val_acc: 0.3131\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "39980/40000 [============================>.] - ETA: 0s - loss: 2.5801 - acc: 0.3312Epoch 00000: val_loss did not improve\n",
      "40000/40000 [==============================] - 940s - loss: 2.5800 - acc: 0.3313 - val_loss: 2.5401 - val_acc: 0.3246\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "39980/40000 [============================>.] - ETA: 0s - loss: 2.6948 - acc: 0.2952Epoch 00000: val_loss did not improve\n",
      "40000/40000 [==============================] - 925s - loss: 2.6949 - acc: 0.2952 - val_loss: 2.6266 - val_acc: 0.3018\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "\n",
    "# This is used to store the best weights for our trained model. We can access this later. This is a work-around for\n",
    "# early-stopping, to save the best weights that minimize the error for both the training and validation sets\n",
    "checkpointer = ModelCheckpoint(filepath='weights_best.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# There are 6 formatted H5 picture files containing 50000 images each. Reading in each file at a time\n",
    "for i in range(1,7):\n",
    "    \n",
    "    # Read in the product image\n",
    "    h5f = h5py.File('x_train' + str(i) + '.h5','r')\n",
    "    pic_arr = h5f['dataset'][:]\n",
    "    \n",
    "    # Read in the category ID\n",
    "    h5f = h5py.File('y_train' + str(i) + '.h5','r')\n",
    "    cat_arr = h5f['dataset'][:]\n",
    "    h5f.close()\n",
    "    \n",
    "    # Now to create my train and validation sets, 20% (10000) of the records are for validation, 80% (40000) of the records\n",
    "    # are for training\n",
    "    x_train = np.array(pic_arr[0:40000])\n",
    "    y_train = np.array(cat_arr[0:40000])\n",
    "    x_val = np.array(pic_arr[40000:50000])\n",
    "    y_val = np.array(cat_arr[40000:50000])\n",
    "\n",
    "    # Free up memory\n",
    "    del pic_arr\n",
    "    del cat_arr\n",
    "\n",
    "    # Load the model weights after the first iteration of the loop, so that we can continue training the weights from \n",
    "    # previous models\n",
    "    if (i > 1):\n",
    "        model = load_model('my_model.h5')\n",
    "    \n",
    "    # Specify GPU to fit the model\n",
    "    with tf.device('/device:GPU:0'):\n",
    "        # Fit the model to run the neural network. The batch size is at 1000, meaning that each epoch will run 40 times, since\n",
    "        # the training set size is 40000 records. The model will do one backpropogation step for all the records in each batch.\n",
    "        model.fit(x_train, y_train, \n",
    "                        batch_size = 20,\n",
    "                        epochs = 1,\n",
    "                        validation_data=(x_val, y_val),\n",
    "                        callbacks=[checkpointer],\n",
    "                        verbose=1)\n",
    "\n",
    "    # Save the model weights into an Hdf5 file. Then we will load the model this model weights for the next iteration of the loop\n",
    "    model.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "    # Free up space/memory\n",
    "    del x_train\n",
    "    del y_train\n",
    "    del x_val\n",
    "    del y_val\n",
    "    del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results - Experiment 1\n",
    "\n",
    "It appears that from each of the six epochs, it was the first epoch that actually was able to reduce the most error in the model and obtain the highest accuracy, with a validation accuracy of 57.6%. This is most likely because not all of the categories were included within the first 50,000 images.\n",
    "\n",
    "However, training a model like this seemed very innefficient. This is because a single epoch is supposed to conist of the entire training dataset, and here an epoch consisted of a different batch of 50,000 images. I initially thought that just by loading the model weights from the previous batch of 50,000 images to train on the next 50,000 images would do the trick, but it was not efficient.\n",
    "\n",
    "The goal of this first convolutional neural network was to be able to classify an image into 49 general categories instead of 5270 unique classes. What I was imagining was that I would run the image through the fitted neural network model which will predict into one of 49 different general categories. Then from there, the image would be transferred to a specific fitted neural network that has all the specific unique classes in the general category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2 - Based on Solution 1, Training using fit_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing some research it appeared that the best way to read in multiple image files for training under one epoch was to use the 'fit_generator' function in Keras. With this function I was able to loop in all of the images under a single epoch to train. This was a lot more efficient than what I performed above.\n",
    "\n",
    "Below are the generator functions that I used for my train and validation datasets. With these generator functions I split my dataset into 80% for train and 20% for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Create a function to process the training data for the generator\n",
    "def train_generator():\n",
    "    counter = 1\n",
    "    batch_size = 20\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # Create arrays to contain x_train and y_train. There are six of these files in total, so 40000*6 = 240,000 items in the entire training set.\n",
    "        # 300,000 images for each epoch\n",
    "        h5f = h5py.File('x_train' + str(counter) + '.h5','r')\n",
    "        pic_arr = h5f['dataset'][0:40000]\n",
    "\n",
    "        h5f = h5py.File('y_train' + str(counter) + '.h5','r')\n",
    "        cat_arr = h5f['dataset'][0:40000]\n",
    "        h5f.close()\n",
    "        \n",
    "        # Since training size for each dataset is 40,000 and batch_size is 20, loop 2000 times because 40000/20 = 2000 \n",
    "        for i in range(1,2001):\n",
    "            if (i == 1):\n",
    "                x_train = pic_arr[0:batch_size]\n",
    "                y_train = cat_arr[0:batch_size]\n",
    "\n",
    "                index = batch_size\n",
    "                yield x_train, y_train\n",
    "            else:\n",
    "                x_train = pic_arr[index:index + batch_size]\n",
    "                y_train = cat_arr[index:index + batch_size]\n",
    "\n",
    "                index += batch_size\n",
    "                yield x_train, y_train\n",
    "                \n",
    "        del pic_arr\n",
    "        del cat_arr\n",
    "        counter += 1\n",
    "        if (counter == 7):\n",
    "            counter = 1\n",
    "\n",
    "# Create a function to define a generator for the validation set\n",
    "def validation_generator():\n",
    "    counter = 1\n",
    "    batch_size = 20\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # Create arrays to contain x_train and y_train. There are seven of these files in total, so 10000*20 = 200000 items in entire validation set.\n",
    "        # 200,000 images for each epoch\n",
    "        h5f = h5py.File('x_train' + str(counter) + '.h5','r')\n",
    "        pic_arr = h5f['dataset'][40000:50000]\n",
    "\n",
    "        h5f = h5py.File('y_train' + str(counter) + '.h5','r')\n",
    "        cat_arr = h5f['dataset'][40000:50000]\n",
    "        h5f.close()\n",
    "        \n",
    "        # Since training size for each dataset is 10,000 and batch_size is 20, loop 500 times because 10000/20 = 500 \n",
    "        for i in range(1,501):\n",
    "            if (i == 1):\n",
    "                x_val = pic_arr[0:batch_size]\n",
    "                y_val = cat_arr[0:batch_size]\n",
    "\n",
    "                index = batch_size\n",
    "                yield x_val, y_val\n",
    "            else:\n",
    "                x_val = pic_arr[index:index + batch_size]\n",
    "                y_val = cat_arr[index:index + batch_size]\n",
    "\n",
    "                index += batch_size\n",
    "                yield x_val, y_val\n",
    "                \n",
    "        del pic_arr\n",
    "        del cat_arr\n",
    "        counter += 1\n",
    "        if (counter == 7):\n",
    "            counter = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 177, 177, 100)     4900      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 177, 177, 100)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 88, 88, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 87, 87, 80)        32080     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 87, 87, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 42, 42, 60)        19260     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 42, 42, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 60)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21, 21, 40)        2440      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21, 21, 20)        820       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8820)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 49)                432229    \n",
      "=================================================================\n",
      "Total params: 491,729\n",
      "Trainable params: 491,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Initialize neural network\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer of the neural network is the image array which will have the shape of 180px by 180px and have a depth of three.\n",
    "# There will be 100 filters/nodes in the second layer. The kernel size will be four, which means that each filter will cover a 4px\n",
    "# by 4px area to find a pattern. The filter will move across the image array one pixels at a time. Using the 'relu' activation\n",
    "# function.\n",
    "model.add(Conv2D(filters=100, kernel_size=4, strides=1, activation='relu', input_shape=(180, 180, 3)))\n",
    "\n",
    "# Implementing a dropout so that we do not overfit any of the nodes in the second layer. This just means that we randomly turn \n",
    "# off some nodes in the second layer so that others can get trained as well.\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Implementing pooling layer so that we reduce the dimensionality of the array. So that we can focus in on more specific patterns\n",
    "# in the array\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=80, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=60, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 40, activation = 'relu'))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 20, activation = 'relu'))\n",
    "\n",
    "# Flattening out the array so that we can predict into one of the 49 different classes\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the output layer where there are 49 different possible output nodes that the product can be classified as\n",
    "model.add(Dense(units=49, activation='softmax'))\n",
    "\n",
    "# Compile the model, loss is categorical crossentropy because we are minimizing the error across more than two classes\n",
    "# The optimizer adam is efficient with quickly minimizing error with momentum. And we want to minimize error due to accuracy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.8971 - acc: 0.5068- ETA: 2s - loss: 1.8972 - accEpoch 00000: val_loss improved from inf to 2.18812, saving model to weights_bestID1Generator.hdf5\n",
      "12000/12000 [==============================] - 4583s - loss: 1.8971 - acc: 0.5068 - val_loss: 2.1881 - val_acc: 0.4050\n",
      "Epoch 2/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.6001 - acc: 0.5766Epoch 00001: val_loss improved from 2.18812 to 1.83066, saving model to weights_bestID1Generator.hdf5\n",
      "12000/12000 [==============================] - 4901s - loss: 1.6000 - acc: 0.5766 - val_loss: 1.8307 - val_acc: 0.5220\n",
      "Epoch 3/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.4890 - acc: 0.6004Epoch 00002: val_loss improved from 1.83066 to 1.78240, saving model to weights_bestID1Generator.hdf5\n",
      "12000/12000 [==============================] - 4921s - loss: 1.4890 - acc: 0.6004 - val_loss: 1.7824 - val_acc: 0.5469\n",
      "Epoch 4/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.4210 - acc: 0.6150Epoch 00003: val_loss improved from 1.78240 to 1.77329, saving model to weights_bestID1Generator.hdf5\n",
      "12000/12000 [==============================] - 4931s - loss: 1.4210 - acc: 0.6150 - val_loss: 1.7733 - val_acc: 0.5499\n",
      "Epoch 5/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.3741 - acc: 0.6245Epoch 00004: val_loss did not improve\n",
      "12000/12000 [==============================] - 4960s - loss: 1.3741 - acc: 0.6245 - val_loss: 1.7863 - val_acc: 0.5457\n",
      "Epoch 6/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.3376 - acc: 0.6327Epoch 00005: val_loss improved from 1.77329 to 1.75527, saving model to weights_bestID1Generator.hdf5\n",
      "12000/12000 [==============================] - 4998s - loss: 1.3376 - acc: 0.6327 - val_loss: 1.7553 - val_acc: 0.5542\n",
      "Epoch 7/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.3069 - acc: 0.6383Epoch 00006: val_loss did not improve\n",
      "12000/12000 [==============================] - 5188s - loss: 1.3069 - acc: 0.6383 - val_loss: 1.7814 - val_acc: 0.5494\n",
      "Epoch 8/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.2860 - acc: 0.6418Epoch 00007: val_loss did not improve\n",
      "12000/12000 [==============================] - 5010s - loss: 1.2860 - acc: 0.6418 - val_loss: 1.8114 - val_acc: 0.5398\n",
      "Epoch 9/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.2663 - acc: 0.6473Epoch 00008: val_loss did not improve\n",
      "12000/12000 [==============================] - 5013s - loss: 1.2663 - acc: 0.6473 - val_loss: 1.7872 - val_acc: 0.5472\n",
      "Epoch 10/10\n",
      "11999/12000 [============================>.] - ETA: 0s - loss: 1.2470 - acc: 0.6512Epoch 00009: val_loss did not improve\n",
      "12000/12000 [==============================] - 5090s - loss: 1.2470 - acc: 0.6512 - val_loss: 1.7916 - val_acc: 0.5496\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# This is used to store the best weights for our trained model.\n",
    "checkpointer = ModelCheckpoint(filepath='weights_bestID1Generator.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# steps_per_epoch=12000 because --> 240,000 (total samples) / 20 (batch size) = 12000\n",
    "# validation_steps=3000 because --> 60,000 (total samples) / 20 (batch size) = 3000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    m = model.fit_generator(generator = train_generator(), steps_per_epoch=12000, epochs=10, \n",
    "                        validation_data = validation_generator(), validation_steps=3000,\n",
    "                        verbose = 1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results - Experiment 2\n",
    "\n",
    "I will not continue with the same solution structure due to the results I recieved. I believe that since the general product categories are so broad, some products may share patterns with others products in different general categories and the accuracy is not as high as I hoped. \n",
    "\n",
    "Due to a low accuracy and the training time that it will take to train many different neural networks, I will go ahead and see if I can just implement one convolutional neural network that directly predicts the product into its specific class that includes 5270 different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Solution 2 - The solution I Implemented\n",
    "\n",
    "Now I am going to directly predict images into its specific class that includes 5270 different classes. Predicting directly into the specific category will save a lot of training time and make things simpler to manage. It will also make it much easier to experiment with the neural network's structure by adding any layers, filters, or nodes to improve the accuracy. I am implementing the 'fit_generator' function in all of the experiments for solution 2.\n",
    "\n",
    "For the first five experiments I will train on 800,000 images and validate on 200,000 images. I expanded the number of images to train on because with a larger class size of 5270 different classes, I needed to make sure that there were enough samples for the neural network to distinguish unique patterns amongst all the classes.\n",
    "\n",
    "The first three experiments will have the same neural network structure (same number of nodes, layers, etc.), but with different optimizers to minimize error. The first experiment I will try Rmsprop, the second Adam, and the third will be Adadelta.\n",
    "\n",
    "The fourth and fifth experiments will be about changing the structure of the neural network, but using the optimizer that achieved the highest validation accuracy from the first three experiments. The fourth experiment, I will restructure a custom model on how I see fit, then with the fifth experiment, I will use transfer learning to train the Inception model with the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Create training generator\n",
    "def train_generator():\n",
    "    counter = 1\n",
    "    batch_size = 20\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # Create arrays to contain x_train and y_train. There are twenty of these files in total, so 40000*20 = 800,000 items in the entire training set.\n",
    "        # 800,000 images for each epoch\n",
    "        h5f = h5py.File('x_train' + str(counter) + 'catID.h5','r')\n",
    "        pic_arr = h5f['dataset'][0:40000]\n",
    "\n",
    "        h5f = h5py.File('y_train' + str(counter) + 'catID.h5','r')\n",
    "        cat_arr = h5f['dataset'][0:40000]\n",
    "        h5f.close()\n",
    "        \n",
    "        # Since training size for each dataset is 40,000 and batch_size is 20, loop 400 times because 40000/20 = 2000 \n",
    "        for i in range(1,2001):\n",
    "            if (i == 1):\n",
    "                x_train = pic_arr[0:batch_size]\n",
    "                y_train = cat_arr[0:batch_size]\n",
    "\n",
    "                index = batch_size\n",
    "                yield x_train, y_train\n",
    "            else:\n",
    "                x_train = pic_arr[index:index + batch_size]\n",
    "                y_train = cat_arr[index:index + batch_size]\n",
    "\n",
    "                index += batch_size\n",
    "                yield x_train, y_train\n",
    "                \n",
    "        del pic_arr\n",
    "        del cat_arr\n",
    "        counter += 1\n",
    "        if (counter == 21):\n",
    "            counter = 1\n",
    "\n",
    "# Create validation generator\n",
    "def validation_generator():\n",
    "    counter = 1\n",
    "    batch_size = 20\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # Create arrays to contain x_train and y_train. There are twenty of these files in total, so 10000*20 = 200000 items in entire validation set.\n",
    "        # 200,000 images for each epoch\n",
    "        h5f = h5py.File('x_train' + str(counter) + 'catID.h5','r')\n",
    "        pic_arr = h5f['dataset'][40000:50000]\n",
    "\n",
    "        h5f = h5py.File('y_train' + str(counter) + 'catID.h5','r')\n",
    "        cat_arr = h5f['dataset'][40000:50000]\n",
    "        h5f.close()\n",
    "        \n",
    "        # Since training size for each dataset is 10,000 and batch_size is 20, loop 500 times because 10000/20 = 500 \n",
    "        for i in range(1,501):\n",
    "            if (i == 1):\n",
    "                x_val = pic_arr[0:batch_size]\n",
    "                y_val = cat_arr[0:batch_size]\n",
    "\n",
    "                index = batch_size\n",
    "                yield x_val, y_val\n",
    "            else:\n",
    "                x_val = pic_arr[index:index + batch_size]\n",
    "                y_val = cat_arr[index:index + batch_size]\n",
    "\n",
    "                index += batch_size\n",
    "                yield x_val, y_val\n",
    "                \n",
    "        del pic_arr\n",
    "        del cat_arr\n",
    "        counter += 1\n",
    "        if (counter == 21):\n",
    "            counter = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 1 - Training using Rmsprop optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 177, 177, 100)     4900      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 177, 177, 100)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 88, 88, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 87, 87, 80)        32080     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 87, 87, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 43, 43, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 42, 42, 60)        19260     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 42, 42, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 21, 21, 60)        0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 21, 21, 40)        2440      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 21, 21, 20)        820       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8820)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 5270)              46486670  \n",
      "=================================================================\n",
      "Total params: 46,546,170\n",
      "Trainable params: 46,546,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Initialize neural network\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer of the neural network is the image array which will have the shape of 180px by 180px and have a depth of three.\n",
    "# There will be 100 filters/nodes in the second layer. The kernel size will be four, which means that each filter will cover a 4px\n",
    "# by 4px area to find a pattern. The filter will move across the image array one pixel at a time. Using the 'relu' activation\n",
    "# function.\n",
    "model.add(Conv2D(filters=100, kernel_size=4, strides=1, activation='relu', input_shape=(180, 180, 3)))\n",
    "\n",
    "# Implementing a dropout so that we do not overfit any of the nodes in the second layer. This just means that we randomly turn \n",
    "# off some nodes in the second layer so that others can get trained as well.\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Implementing pooling layer so that we reduce the dimensionality of the array. So that we can focus in on more specific patterns\n",
    "# in the array\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=80, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=60, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 40, activation = 'relu'))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 20, activation = 'relu'))\n",
    "\n",
    "# Flattening out the array so that we can predict into one of the 49 different classes\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the output layer where there are 5270 different possible output nodes that the product can be classified as\n",
    "model.add(Dense(units=5270, activation='softmax'))\n",
    "\n",
    "# Compile the model, loss is categorical crossentropy because we are minimizing the error across more than two classes\n",
    "# The optimizer adam is efficient with quickly minimizing error with momentum. And we want to minimize error due to accuracy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 6.7121 - acc: 0.1531Epoch 00000: val_loss improved from inf to 7.57415, saving model to weights_bestcatIDGenerator.hdf5\n",
      "40000/40000 [==============================] - 10889s - loss: 6.7121 - acc: 0.1531 - val_loss: 7.5741 - val_acc: 0.1101\n",
      "Epoch 2/18\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 7.2933 - acc: 0.1532Epoch 00001: val_loss did not improve\n",
      "40000/40000 [==============================] - 10935s - loss: 7.2933 - acc: 0.1532 - val_loss: 8.3542 - val_acc: 0.1063\n",
      "Epoch 3/18\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 7.4592 - acc: 0.1448Epoch 00002: val_loss did not improve\n",
      "40000/40000 [==============================] - 10980s - loss: 7.4593 - acc: 0.1448 - val_loss: 8.5523 - val_acc: 0.1210\n",
      "Epoch 4/18\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 8.1101 - acc: 0.0953Epoch 00003: val_loss did not improve\n",
      "40000/40000 [==============================] - 10915s - loss: 8.1101 - acc: 0.0953 - val_loss: 9.6641 - val_acc: 0.0117\n",
      "Epoch 5/18\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 9.2983 - acc: 0.0180Epoch 00004: val_loss did not improve\n",
      "40000/40000 [==============================] - 10977s - loss: 9.2982 - acc: 0.0180 - val_loss: 9.5714 - val_acc: 0.0117\n",
      "Epoch 6/18\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 9.2741 - acc: 0.0187- ETA: 2s - loss: 9.2741 - accEpoch 00005: val_loss did not improve\n",
      "40000/40000 [==============================] - 10884s - loss: 9.2741 - acc: 0.0187 - val_loss: 9.5695 - val_acc: 0.0117\n",
      "Epoch 7/18\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 9.2731 - acc: 0.0187Epoch 00006: val_loss did not improve\n",
      "40000/40000 [==============================] - 10838s - loss: 9.2731 - acc: 0.0187 - val_loss: 9.5691 - val_acc: 0.0117\n",
      "Epoch 8/18\n",
      "15683/40000 [==========>...................] - ETA: 5644s - loss: 10.1711 - acc: 0.0155"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# This is used to store the best weights for our trained model.\n",
    "checkpointer = ModelCheckpoint(filepath='weights_bestcatIDGenerator.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# steps_per_epoch=40000 because --> 800,000 (total samples) / 20 (batch size) = 40000\n",
    "# validation_steps=10000 because --> 200,000 (total samples) / 20 (batch size) = 10000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit_generator(generator = train_generator(), steps_per_epoch=40000, epochs=18, \n",
    "                        validation_data = validation_generator(), validation_steps=10000, \n",
    "                        verbose = 1, callbacks=[checkpointer])\n",
    "\n",
    "# with tf.device('/device:GPU:0'):\n",
    "#     model.fit_generator(generator = train_generator(), steps_per_epoch=2000, epochs=2, \n",
    "#                         validation_data = validation_generator(), validation_steps=500, \n",
    "#                         verbose = 1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The first epoch, returned the highest validation accuracy of 15.3%. Although it says I ran this experiment for 18 epochs, I stopped it after 8 because the validation loss kept on increasing; meaning the validation accuracy kept on declining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2 - Training using Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 177, 177, 100)     4900      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 177, 177, 100)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 88, 88, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 87, 87, 80)        32080     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 87, 87, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 42, 42, 60)        19260     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 42, 42, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 60)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21, 21, 40)        2440      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21, 21, 20)        820       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8820)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5270)              46486670  \n",
      "=================================================================\n",
      "Total params: 46,546,170\n",
      "Trainable params: 46,546,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Initialize neural network\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer of the neural network is the image array which will have the shape of 180px by 180px and have a depth of three.\n",
    "# There will be 100 filters/nodes in the second layer. The kernel size will be four, which means that each filter will cover a 4px\n",
    "# by 4px area to find a pattern. The filter will move across the image array one pixels at a time. Using the 'relu' activation\n",
    "# function.\n",
    "model.add(Conv2D(filters=100, kernel_size=4, strides=1, activation='relu', input_shape=(180, 180, 3)))\n",
    "\n",
    "# Implementing a dropout so that we do not overfit any of the nodes in the second layer. This just means that we randomly turn \n",
    "# off some nodes in the second layer so that others can get trained as well.\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Implementing pooling layer so that we reduce the dimensionality of the array. So that we can focus in on more specific patterns\n",
    "# in the array\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=80, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=60, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 40, activation = 'relu'))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 20, activation = 'relu'))\n",
    "\n",
    "# Flattening out the array so that we can predict into one of the 49 different classes\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the output layer where there are 5270 different possible output nodes that the product can be classified as\n",
    "model.add(Dense(units=5270, activation='softmax'))\n",
    "\n",
    "# Compile the model, loss is categorical crossentropy because we are minimizing the error across more than two classes\n",
    "# The optimizer adam is efficient with quickly minimizing error with momentum. And we want to minimize error due to accuracy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.9056 - acc: 0.2920Epoch 00000: val_loss improved from inf to 6.38304, saving model to weights_bestcatIDGeneratorAdam.hdf5\n",
      "40000/40000 [==============================] - 11274s - loss: 4.9055 - acc: 0.2920 - val_loss: 6.3830 - val_acc: 0.2190\n",
      "Epoch 2/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.2730 - acc: 0.3525- ETA: 2s - loss: 4.2729 - accEpoch 00001: val_loss improved from 6.38304 to 6.15673, saving model to weights_bestcatIDGeneratorAdam.hdf5\n",
      "40000/40000 [==============================] - 11249s - loss: 4.2730 - acc: 0.3525 - val_loss: 6.1567 - val_acc: 0.2402\n",
      "Epoch 3/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.9527 - acc: 0.3737- ETA: 2s - loss: 3.9526 - accEpoch 00002: val_loss improved from 6.15673 to 5.90290, saving model to weights_bestcatIDGeneratorAdam.hdf5\n",
      "40000/40000 [==============================] - 11214s - loss: 3.9527 - acc: 0.3737 - val_loss: 5.9029 - val_acc: 0.2452\n",
      "Epoch 4/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.7453 - acc: 0.3878Epoch 00003: val_loss improved from 5.90290 to 5.87724, saving model to weights_bestcatIDGeneratorAdam.hdf5\n",
      "40000/40000 [==============================] - 11238s - loss: 3.7453 - acc: 0.3878 - val_loss: 5.8772 - val_acc: 0.2424\n",
      "Epoch 5/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.6570 - acc: 0.3945- ETA: 2s - loss: 3.6569 - accEpoch 00004: val_loss did not improve\n",
      "40000/40000 [==============================] - 11316s - loss: 3.6570 - acc: 0.3945 - val_loss: 5.9776 - val_acc: 0.2301\n",
      "Epoch 6/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.5928 - acc: 0.3989Epoch 00005: val_loss did not improve\n",
      "40000/40000 [==============================] - 11235s - loss: 3.5928 - acc: 0.3989 - val_loss: 6.1621 - val_acc: 0.2146\n",
      "Epoch 7/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.6027 - acc: 0.3974- ETA: 2s - loss: 3.6026 - accEpoch 00006: val_loss did not improve\n",
      "40000/40000 [==============================] - 11241s - loss: 3.6027 - acc: 0.3974 - val_loss: 6.1439 - val_acc: 0.2183\n",
      "Epoch 8/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.5799 - acc: 0.3987- ETA: 2s - loss: 3.5798 - accEpoch 00007: val_loss did not improve\n",
      "40000/40000 [==============================] - 11245s - loss: 3.5799 - acc: 0.3987 - val_loss: 6.1963 - val_acc: 0.2161\n",
      "Epoch 9/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.4739 - acc: 0.4107Epoch 00008: val_loss did not improve\n",
      "40000/40000 [==============================] - 11247s - loss: 3.4739 - acc: 0.4107 - val_loss: 6.0396 - val_acc: 0.2362\n",
      "Epoch 10/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.6339 - acc: 0.3962Epoch 00009: val_loss did not improve\n",
      "40000/40000 [==============================] - 11255s - loss: 3.6339 - acc: 0.3962 - val_loss: 6.9703 - val_acc: 0.1719\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# This is used to store the best weights for our trained model.\n",
    "checkpointer = ModelCheckpoint(filepath='weights_bestcatIDGeneratorAdam.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# steps_per_epoch=40000 because --> 800,000 (total samples) / 20 (batch size) = 40000\n",
    "# validation_steps=10000 because --> 200,000 (total samples) / 20 (batch size) = 10000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit_generator(generator = train_generator(), steps_per_epoch=40000, epochs=10, \n",
    "                        validation_data = validation_generator(), validation_steps=10000, \n",
    "                        verbose = 1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Due to the first experiment and the training time it took, I felt that it was best to keep this to 10 epochs. And I thought 10 epochs would be enough to minimize the loss in the error function.\n",
    "\n",
    "Experiment 2 saw a significant increase in accuracy as it jumped to 24.2% accuracy. I believe this is because the Adam optimizer uses momentum when trying to minimize the error function. What momentum for the Adam optimizer does is that it uses a more complicated exponential decay for the learning rate that consists of not just considering the average (first moment), but also the variance (second moment) of the previous steps. Each step is performed when a batch of 20 images have backpropagated through our neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 3 - Training using Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 177, 177, 100)     4900      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 177, 177, 100)     0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 88, 88, 100)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 87, 87, 80)        32080     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 87, 87, 80)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 80)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 42, 42, 60)        19260     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 42, 42, 60)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 21, 21, 60)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 21, 21, 40)        2440      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 21, 21, 20)        820       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8820)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5270)              46486670  \n",
      "=================================================================\n",
      "Total params: 46,546,170\n",
      "Trainable params: 46,546,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Initialize neural network\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer of the neural network is the image array which will have the shape of 180px by 180px and have a depth of three.\n",
    "# There will be 100 filters/nodes in the second layer. The kernel size will be four, which means that each filter will cover a 4px\n",
    "# by 4px area to find a pattern. The filter will move across the image array one pixels at a time. Using the 'relu' activation\n",
    "# function.\n",
    "model.add(Conv2D(filters=100, kernel_size=4, strides=1, activation='relu', input_shape=(180, 180, 3)))\n",
    "\n",
    "# Implementing a dropout so that we do not overfit any of the nodes in the second layer. This just means that we randomly turn \n",
    "# off some nodes in the second layer so that others can get trained as well.\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Implementing pooling layer so that we reduce the dimensionality of the array. So that we can focus in on more specific patterns\n",
    "# in the array\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=80, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=60, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 40, activation = 'relu'))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 20, activation = 'relu'))\n",
    "\n",
    "# Flattening out the array so that we can predict into one of the 49 different classes\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the output layer where there are 5270 different possible output nodes that the product can be classified as\n",
    "model.add(Dense(units=5270, activation='softmax'))\n",
    "\n",
    "# Compile the model, loss is categorical crossentropy because we are minimizing the error across more than two classes\n",
    "# The optimizer adam is efficient with quickly minimizing error with momentum. And we want to minimize error due to accuracy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adadelta\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.8301 - acc: 0.3208Epoch 00000: val_loss improved from inf to 6.12848, saving model to weights_bestcatIDGeneratorAdaDelta.hdf5\n",
      "40000/40000 [==============================] - 11845s - loss: 4.8301 - acc: 0.3208 - val_loss: 6.1285 - val_acc: 0.2045\n",
      "Epoch 2/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.5452 - acc: 0.3619- ETA: 2s - loss: 4.5451 - accEpoch 00001: val_loss did not improve\n",
      "40000/40000 [==============================] - 11853s - loss: 4.5452 - acc: 0.3619 - val_loss: 6.2759 - val_acc: 0.1758\n",
      "Epoch 3/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.4901 - acc: 0.3733Epoch 00002: val_loss did not improve\n",
      "40000/40000 [==============================] - 11805s - loss: 4.4901 - acc: 0.3733 - val_loss: 6.1845 - val_acc: 0.1613\n",
      "Epoch 4/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.4835 - acc: 0.3787Epoch 00003: val_loss did not improve\n",
      "40000/40000 [==============================] - 11820s - loss: 4.4835 - acc: 0.3787 - val_loss: 6.1857 - val_acc: 0.1728\n",
      "Epoch 5/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.4896 - acc: 0.3809Epoch 00004: val_loss did not improve\n",
      "40000/40000 [==============================] - 11812s - loss: 4.4896 - acc: 0.3809 - val_loss: 7.3414 - val_acc: 0.1337\n",
      "Epoch 6/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.5216 - acc: 0.3811Epoch 00005: val_loss did not improve\n",
      "40000/40000 [==============================] - 11749s - loss: 4.5216 - acc: 0.3811 - val_loss: 6.2974 - val_acc: 0.1589\n",
      "Epoch 7/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.5420 - acc: 0.3808Epoch 00006: val_loss did not improve\n",
      "40000/40000 [==============================] - 11766s - loss: 4.5420 - acc: 0.3808 - val_loss: 6.1297 - val_acc: 0.1965\n",
      "Epoch 8/10\n",
      "    3/40000 [..............................] - ETA: 26173s - loss: 6.4342 - acc: 0.2000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\callbacks.py:120: UserWarning: Method on_batch_end() is slow compared to the batch update (0.223631). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.5696 - acc: 0.3793Epoch 00007: val_loss improved from 6.12848 to 6.06524, saving model to weights_bestcatIDGeneratorAdaDelta.hdf5\n",
      "40000/40000 [==============================] - 11822s - loss: 4.5696 - acc: 0.3793 - val_loss: 6.0652 - val_acc: 0.1965\n",
      "Epoch 9/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.6106 - acc: 0.3757- ETA: 2s - loss: 4.6105 - accEpoch 00008: val_loss improved from 6.06524 to 5.93814, saving model to weights_bestcatIDGeneratorAdaDelta.hdf5\n",
      "40000/40000 [==============================] - 11713s - loss: 4.6106 - acc: 0.3757 - val_loss: 5.9381 - val_acc: 0.2257\n",
      "Epoch 10/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.6166 - acc: 0.3709Epoch 00009: val_loss did not improve\n",
      "40000/40000 [==============================] - 11833s - loss: 4.6166 - acc: 0.3709 - val_loss: 6.4589 - val_acc: 0.1527\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# This is used to store the best weights for our trained model.\n",
    "checkpointer = ModelCheckpoint(filepath='weights_bestcatIDGeneratorAdaDelta.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# steps_per_epoch=40000 because --> 800,000 (total samples) / 20 (batch size) = 40000\n",
    "# validation_steps=10000 because --> 200,000 (total samples) / 20 (batch size) = 10000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model.fit_generator(generator = train_generator(), steps_per_epoch=40000, epochs=10, \n",
    "                        validation_data = validation_generator(), validation_steps=10000, \n",
    "                        verbose = 1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "The first epoch managed to get the highest validation accuracy of 20.5%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 4 - Using the Adam optimizer, and constructing a new simpler neural network structure\n",
    "\n",
    "Since the highest validation accuracy received came from using the Adam optimizer, I will use that optimizer for experiment 4. \n",
    "\n",
    "Based on the verbose results of experiment 2 (which used the Adam optimizer), it appeared that the neural network overfit the data. This was because the highest accuracy we achieved was 41%, on the ninth epoch, but the validation accuracy was at 23.6%.\n",
    "\n",
    "Because it seemed like my model overfit the data, I reconstructed the model for experiment 4 to make the model simpler. Below is the model summary for experiment 4, and as you can see, it has less layers and nodes than the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 177, 177, 64)      3136      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 177, 177, 64)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 88, 88, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 87, 87, 32)        8224      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 87, 87, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 43, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 43, 43, 20)        660       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 43, 43, 10)        210       \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 18490)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5270)              97447570  \n",
      "=================================================================\n",
      "Total params: 97,459,800\n",
      "Trainable params: 97,459,800\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "\n",
    "# Initialize neural network\n",
    "model = Sequential()\n",
    "\n",
    "# The input layer of the neural network is the image array which will have the shape of 180px by 180px and have a depth of three.\n",
    "# There will be 64 filters/nodes in the second layer. The kernel size will be two, which means that each filter will cover a 2px\n",
    "# by 2px area to find a pattern. The filter will move across the image array two pixels at a time. Using the 'relu' activation\n",
    "# function.\n",
    "model.add(Conv2D(filters=64, kernel_size=4, strides=1, activation='relu', input_shape=(180, 180, 3)))\n",
    "\n",
    "# Implementing a dropout so that we do not overfit any of the nodes in the second layer. This just means that we randomly turn \n",
    "# off some nodes in the second layer so that others can get trained as well.\n",
    "model.add(Dropout(rate = 0.2))\n",
    "\n",
    "# Implementing pooling layer so that we reduce the dimensionality of the array. So that we can focus in on more specific patterns\n",
    "# in the array\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Repeating similar steps for these\n",
    "model.add(Conv2D(filters=32, kernel_size=2, strides=1, activation='relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(MaxPooling2D(pool_size=2, strides=2))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 20, activation = 'relu'))\n",
    "\n",
    "# Fully connected layer\n",
    "model.add(Dense(units = 10, activation = 'relu'))\n",
    "\n",
    "# Flattening out the array so that we can predict into one of the 49 different classes\n",
    "model.add(Flatten())\n",
    "\n",
    "# This is the output layer where there are 5270 different possible output nodes that the product can be classified as\n",
    "model.add(Dense(units=5270, activation='softmax'))\n",
    "\n",
    "# Compile the model, loss is categorical crossentropy because we are minimizing the error across more than two classes\n",
    "# The optimizer adam is efficient with quickly minimizing error with momentum. And we want to minimize error due to accuracy\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 5.3377 - acc: 0.2588Epoch 00000: val_loss improved from inf to 6.77269, saving model to weights_bestcatIDGeneratorAdamV2.hdf5\n",
      "40000/40000 [==============================] - 11256s - loss: 5.3377 - acc: 0.2588 - val_loss: 6.7727 - val_acc: 0.1781\n",
      "Epoch 2/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.5861 - acc: 0.3168Epoch 00001: val_loss improved from 6.77269 to 6.53595, saving model to weights_bestcatIDGeneratorAdamV2.hdf5\n",
      "40000/40000 [==============================] - 11235s - loss: 4.5861 - acc: 0.3168 - val_loss: 6.5359 - val_acc: 0.2033\n",
      "Epoch 3/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.3562 - acc: 0.3368Epoch 00002: val_loss improved from 6.53595 to 6.51178, saving model to weights_bestcatIDGeneratorAdamV2.hdf5\n",
      "40000/40000 [==============================] - 11244s - loss: 4.3562 - acc: 0.3368 - val_loss: 6.5118 - val_acc: 0.2067\n",
      "Epoch 4/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.2848 - acc: 0.3426Epoch 00003: val_loss did not improve\n",
      "40000/40000 [==============================] - 11263s - loss: 4.2848 - acc: 0.3426 - val_loss: 6.6869 - val_acc: 0.1944\n",
      "Epoch 5/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.2255 - acc: 0.3505Epoch 00004: val_loss improved from 6.51178 to 6.45012, saving model to weights_bestcatIDGeneratorAdamV2.hdf5\n",
      "40000/40000 [==============================] - 11186s - loss: 4.2254 - acc: 0.3505 - val_loss: 6.4501 - val_acc: 0.2069\n",
      "Epoch 6/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.1145 - acc: 0.3597Epoch 00005: val_loss did not improve\n",
      "40000/40000 [==============================] - 11161s - loss: 4.1145 - acc: 0.3597 - val_loss: 6.5273 - val_acc: 0.2030\n",
      "Epoch 7/10\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.0479 - acc: 0.3649Epoch 00006: val_loss did not improve\n",
      "40000/40000 [==============================] - 11289s - loss: 4.0479 - acc: 0.3649 - val_loss: 6.6541 - val_acc: 0.1950\n",
      "Epoch 8/10\n",
      "17623/40000 [============>.................] - ETA: 5456s - loss: 3.7498 - acc: 0.4039"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-8dc89697c8c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     m = model.fit_generator(generator = train_generator(), steps_per_epoch=40000, epochs=10, \n\u001b[0;32m     12\u001b[0m                         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                         verbose = 1, callbacks=[checkpointer])\n\u001b[0m",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[0;32m   1119\u001b[0m                                         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m                                         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 87\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   2040\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   2041\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2042\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   2043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2044\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1760\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1761\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1762\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1763\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1764\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2271\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2272\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2273\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2274\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\james\\appdata\\local\\continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "# This is used to store the best weights for our trained model.\n",
    "checkpointer = ModelCheckpoint(filepath='weights_bestcatIDGeneratorAdamV2.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# steps_per_epoch=40000 because --> 800,000 (total samples) / 20 (batch size) = 40000\n",
    "# validation_steps=10000 because --> 200,000 (total samples) / 20 (batch size) = 10000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    m = model.fit_generator(generator = train_generator(), steps_per_epoch=40000, epochs=10, \n",
    "                        validation_data = validation_generator(), validation_steps=10000, \n",
    "                        verbose = 1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "With my new reconstructed model, the validation accuracy was even worse by a whole 4 percentage points. I stopped training in the middle of the eighth epoch because the validation accuracy appeared to be very stagnant across the epochs and that the model was heavily overfitting the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 5 - Using the Inception Model to train the data (transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "model = applications.InceptionV3(weights = \"imagenet\", include_top=False, input_shape = (180, 180, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_4 (InputLayer)             (None, 180, 180, 3)   0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_195 (Conv2D)              (None, 89, 89, 32)    864         input_4[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_189 (BatchNo (None, 89, 89, 32)    96          conv2d_195[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_189 (Activation)      (None, 89, 89, 32)    0           batch_normalization_189[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_196 (Conv2D)              (None, 87, 87, 32)    9216        activation_189[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_190 (BatchNo (None, 87, 87, 32)    96          conv2d_196[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_190 (Activation)      (None, 87, 87, 32)    0           batch_normalization_190[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_197 (Conv2D)              (None, 87, 87, 64)    18432       activation_190[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_191 (BatchNo (None, 87, 87, 64)    192         conv2d_197[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_191 (Activation)      (None, 87, 87, 64)    0           batch_normalization_191[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_15 (MaxPooling2D)  (None, 43, 43, 64)    0           activation_191[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_198 (Conv2D)              (None, 43, 43, 80)    5120        max_pooling2d_15[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_192 (BatchNo (None, 43, 43, 80)    240         conv2d_198[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_192 (Activation)      (None, 43, 43, 80)    0           batch_normalization_192[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_199 (Conv2D)              (None, 41, 41, 192)   138240      activation_192[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_193 (BatchNo (None, 41, 41, 192)   576         conv2d_199[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_193 (Activation)      (None, 41, 41, 192)   0           batch_normalization_193[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling2D)  (None, 20, 20, 192)   0           activation_193[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_203 (Conv2D)              (None, 20, 20, 64)    12288       max_pooling2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_197 (BatchNo (None, 20, 20, 64)    192         conv2d_203[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_197 (Activation)      (None, 20, 20, 64)    0           batch_normalization_197[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_201 (Conv2D)              (None, 20, 20, 48)    9216        max_pooling2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_204 (Conv2D)              (None, 20, 20, 96)    55296       activation_197[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_195 (BatchNo (None, 20, 20, 48)    144         conv2d_201[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_198 (BatchNo (None, 20, 20, 96)    288         conv2d_204[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_195 (Activation)      (None, 20, 20, 48)    0           batch_normalization_195[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_198 (Activation)      (None, 20, 20, 96)    0           batch_normalization_198[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_19 (AveragePoo (None, 20, 20, 192)   0           max_pooling2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_200 (Conv2D)              (None, 20, 20, 64)    12288       max_pooling2d_16[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_202 (Conv2D)              (None, 20, 20, 64)    76800       activation_195[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_205 (Conv2D)              (None, 20, 20, 96)    82944       activation_198[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_206 (Conv2D)              (None, 20, 20, 32)    6144        average_pooling2d_19[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_194 (BatchNo (None, 20, 20, 64)    192         conv2d_200[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_196 (BatchNo (None, 20, 20, 64)    192         conv2d_202[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_199 (BatchNo (None, 20, 20, 96)    288         conv2d_205[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_200 (BatchNo (None, 20, 20, 32)    96          conv2d_206[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_194 (Activation)      (None, 20, 20, 64)    0           batch_normalization_194[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_196 (Activation)      (None, 20, 20, 64)    0           batch_normalization_196[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_199 (Activation)      (None, 20, 20, 96)    0           batch_normalization_199[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_200 (Activation)      (None, 20, 20, 32)    0           batch_normalization_200[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)             (None, 20, 20, 256)   0           activation_194[0][0]             \n",
      "                                                                   activation_196[0][0]             \n",
      "                                                                   activation_199[0][0]             \n",
      "                                                                   activation_200[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_210 (Conv2D)              (None, 20, 20, 64)    16384       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_204 (BatchNo (None, 20, 20, 64)    192         conv2d_210[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_204 (Activation)      (None, 20, 20, 64)    0           batch_normalization_204[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_208 (Conv2D)              (None, 20, 20, 48)    12288       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_211 (Conv2D)              (None, 20, 20, 96)    55296       activation_204[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_202 (BatchNo (None, 20, 20, 48)    144         conv2d_208[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_205 (BatchNo (None, 20, 20, 96)    288         conv2d_211[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_202 (Activation)      (None, 20, 20, 48)    0           batch_normalization_202[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_205 (Activation)      (None, 20, 20, 96)    0           batch_normalization_205[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_20 (AveragePoo (None, 20, 20, 256)   0           mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_207 (Conv2D)              (None, 20, 20, 64)    16384       mixed0[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_209 (Conv2D)              (None, 20, 20, 64)    76800       activation_202[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_212 (Conv2D)              (None, 20, 20, 96)    82944       activation_205[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_213 (Conv2D)              (None, 20, 20, 64)    16384       average_pooling2d_20[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_201 (BatchNo (None, 20, 20, 64)    192         conv2d_207[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_203 (BatchNo (None, 20, 20, 64)    192         conv2d_209[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_206 (BatchNo (None, 20, 20, 96)    288         conv2d_212[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_207 (BatchNo (None, 20, 20, 64)    192         conv2d_213[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_201 (Activation)      (None, 20, 20, 64)    0           batch_normalization_201[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_203 (Activation)      (None, 20, 20, 64)    0           batch_normalization_203[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_206 (Activation)      (None, 20, 20, 96)    0           batch_normalization_206[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_207 (Activation)      (None, 20, 20, 64)    0           batch_normalization_207[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)             (None, 20, 20, 288)   0           activation_201[0][0]             \n",
      "                                                                   activation_203[0][0]             \n",
      "                                                                   activation_206[0][0]             \n",
      "                                                                   activation_207[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_217 (Conv2D)              (None, 20, 20, 64)    18432       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_211 (BatchNo (None, 20, 20, 64)    192         conv2d_217[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_211 (Activation)      (None, 20, 20, 64)    0           batch_normalization_211[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_215 (Conv2D)              (None, 20, 20, 48)    13824       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_218 (Conv2D)              (None, 20, 20, 96)    55296       activation_211[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_209 (BatchNo (None, 20, 20, 48)    144         conv2d_215[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_212 (BatchNo (None, 20, 20, 96)    288         conv2d_218[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_209 (Activation)      (None, 20, 20, 48)    0           batch_normalization_209[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_212 (Activation)      (None, 20, 20, 96)    0           batch_normalization_212[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePoo (None, 20, 20, 288)   0           mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_214 (Conv2D)              (None, 20, 20, 64)    18432       mixed1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_216 (Conv2D)              (None, 20, 20, 64)    76800       activation_209[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_219 (Conv2D)              (None, 20, 20, 96)    82944       activation_212[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_220 (Conv2D)              (None, 20, 20, 64)    18432       average_pooling2d_21[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_208 (BatchNo (None, 20, 20, 64)    192         conv2d_214[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_210 (BatchNo (None, 20, 20, 64)    192         conv2d_216[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_213 (BatchNo (None, 20, 20, 96)    288         conv2d_219[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_214 (BatchNo (None, 20, 20, 64)    192         conv2d_220[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_208 (Activation)      (None, 20, 20, 64)    0           batch_normalization_208[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_210 (Activation)      (None, 20, 20, 64)    0           batch_normalization_210[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_213 (Activation)      (None, 20, 20, 96)    0           batch_normalization_213[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_214 (Activation)      (None, 20, 20, 64)    0           batch_normalization_214[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)             (None, 20, 20, 288)   0           activation_208[0][0]             \n",
      "                                                                   activation_210[0][0]             \n",
      "                                                                   activation_213[0][0]             \n",
      "                                                                   activation_214[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_222 (Conv2D)              (None, 20, 20, 64)    18432       mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_216 (BatchNo (None, 20, 20, 64)    192         conv2d_222[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_216 (Activation)      (None, 20, 20, 64)    0           batch_normalization_216[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_223 (Conv2D)              (None, 20, 20, 96)    55296       activation_216[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_217 (BatchNo (None, 20, 20, 96)    288         conv2d_223[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_217 (Activation)      (None, 20, 20, 96)    0           batch_normalization_217[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_221 (Conv2D)              (None, 9, 9, 384)     995328      mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_224 (Conv2D)              (None, 9, 9, 96)      82944       activation_217[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_215 (BatchNo (None, 9, 9, 384)     1152        conv2d_221[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_218 (BatchNo (None, 9, 9, 96)      288         conv2d_224[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_215 (Activation)      (None, 9, 9, 384)     0           batch_normalization_215[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_218 (Activation)      (None, 9, 9, 96)      0           batch_normalization_218[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling2D)  (None, 9, 9, 288)     0           mixed2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)             (None, 9, 9, 768)     0           activation_215[0][0]             \n",
      "                                                                   activation_218[0][0]             \n",
      "                                                                   max_pooling2d_17[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_229 (Conv2D)              (None, 9, 9, 128)     98304       mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_223 (BatchNo (None, 9, 9, 128)     384         conv2d_229[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_223 (Activation)      (None, 9, 9, 128)     0           batch_normalization_223[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_230 (Conv2D)              (None, 9, 9, 128)     114688      activation_223[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_224 (BatchNo (None, 9, 9, 128)     384         conv2d_230[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_224 (Activation)      (None, 9, 9, 128)     0           batch_normalization_224[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_226 (Conv2D)              (None, 9, 9, 128)     98304       mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_231 (Conv2D)              (None, 9, 9, 128)     114688      activation_224[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_220 (BatchNo (None, 9, 9, 128)     384         conv2d_226[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_225 (BatchNo (None, 9, 9, 128)     384         conv2d_231[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_220 (Activation)      (None, 9, 9, 128)     0           batch_normalization_220[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_225 (Activation)      (None, 9, 9, 128)     0           batch_normalization_225[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_227 (Conv2D)              (None, 9, 9, 128)     114688      activation_220[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_232 (Conv2D)              (None, 9, 9, 128)     114688      activation_225[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_221 (BatchNo (None, 9, 9, 128)     384         conv2d_227[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_226 (BatchNo (None, 9, 9, 128)     384         conv2d_232[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_221 (Activation)      (None, 9, 9, 128)     0           batch_normalization_221[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_226 (Activation)      (None, 9, 9, 128)     0           batch_normalization_226[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_22 (AveragePoo (None, 9, 9, 768)     0           mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_225 (Conv2D)              (None, 9, 9, 192)     147456      mixed3[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_228 (Conv2D)              (None, 9, 9, 192)     172032      activation_221[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_233 (Conv2D)              (None, 9, 9, 192)     172032      activation_226[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_234 (Conv2D)              (None, 9, 9, 192)     147456      average_pooling2d_22[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_219 (BatchNo (None, 9, 9, 192)     576         conv2d_225[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_222 (BatchNo (None, 9, 9, 192)     576         conv2d_228[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_227 (BatchNo (None, 9, 9, 192)     576         conv2d_233[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_228 (BatchNo (None, 9, 9, 192)     576         conv2d_234[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_219 (Activation)      (None, 9, 9, 192)     0           batch_normalization_219[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_222 (Activation)      (None, 9, 9, 192)     0           batch_normalization_222[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_227 (Activation)      (None, 9, 9, 192)     0           batch_normalization_227[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_228 (Activation)      (None, 9, 9, 192)     0           batch_normalization_228[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)             (None, 9, 9, 768)     0           activation_219[0][0]             \n",
      "                                                                   activation_222[0][0]             \n",
      "                                                                   activation_227[0][0]             \n",
      "                                                                   activation_228[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_239 (Conv2D)              (None, 9, 9, 160)     122880      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_233 (BatchNo (None, 9, 9, 160)     480         conv2d_239[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_233 (Activation)      (None, 9, 9, 160)     0           batch_normalization_233[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_240 (Conv2D)              (None, 9, 9, 160)     179200      activation_233[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_234 (BatchNo (None, 9, 9, 160)     480         conv2d_240[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_234 (Activation)      (None, 9, 9, 160)     0           batch_normalization_234[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_236 (Conv2D)              (None, 9, 9, 160)     122880      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_241 (Conv2D)              (None, 9, 9, 160)     179200      activation_234[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_230 (BatchNo (None, 9, 9, 160)     480         conv2d_236[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_235 (BatchNo (None, 9, 9, 160)     480         conv2d_241[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_230 (Activation)      (None, 9, 9, 160)     0           batch_normalization_230[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_235 (Activation)      (None, 9, 9, 160)     0           batch_normalization_235[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_237 (Conv2D)              (None, 9, 9, 160)     179200      activation_230[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_242 (Conv2D)              (None, 9, 9, 160)     179200      activation_235[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_231 (BatchNo (None, 9, 9, 160)     480         conv2d_237[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_236 (BatchNo (None, 9, 9, 160)     480         conv2d_242[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_231 (Activation)      (None, 9, 9, 160)     0           batch_normalization_231[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_236 (Activation)      (None, 9, 9, 160)     0           batch_normalization_236[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_23 (AveragePoo (None, 9, 9, 768)     0           mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_235 (Conv2D)              (None, 9, 9, 192)     147456      mixed4[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_238 (Conv2D)              (None, 9, 9, 192)     215040      activation_231[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_243 (Conv2D)              (None, 9, 9, 192)     215040      activation_236[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_244 (Conv2D)              (None, 9, 9, 192)     147456      average_pooling2d_23[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_229 (BatchNo (None, 9, 9, 192)     576         conv2d_235[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_232 (BatchNo (None, 9, 9, 192)     576         conv2d_238[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_237 (BatchNo (None, 9, 9, 192)     576         conv2d_243[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_238 (BatchNo (None, 9, 9, 192)     576         conv2d_244[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_229 (Activation)      (None, 9, 9, 192)     0           batch_normalization_229[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_232 (Activation)      (None, 9, 9, 192)     0           batch_normalization_232[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_237 (Activation)      (None, 9, 9, 192)     0           batch_normalization_237[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_238 (Activation)      (None, 9, 9, 192)     0           batch_normalization_238[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)             (None, 9, 9, 768)     0           activation_229[0][0]             \n",
      "                                                                   activation_232[0][0]             \n",
      "                                                                   activation_237[0][0]             \n",
      "                                                                   activation_238[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_249 (Conv2D)              (None, 9, 9, 160)     122880      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_243 (BatchNo (None, 9, 9, 160)     480         conv2d_249[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_243 (Activation)      (None, 9, 9, 160)     0           batch_normalization_243[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_250 (Conv2D)              (None, 9, 9, 160)     179200      activation_243[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_244 (BatchNo (None, 9, 9, 160)     480         conv2d_250[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_244 (Activation)      (None, 9, 9, 160)     0           batch_normalization_244[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_246 (Conv2D)              (None, 9, 9, 160)     122880      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_251 (Conv2D)              (None, 9, 9, 160)     179200      activation_244[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_240 (BatchNo (None, 9, 9, 160)     480         conv2d_246[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_245 (BatchNo (None, 9, 9, 160)     480         conv2d_251[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_240 (Activation)      (None, 9, 9, 160)     0           batch_normalization_240[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_245 (Activation)      (None, 9, 9, 160)     0           batch_normalization_245[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_247 (Conv2D)              (None, 9, 9, 160)     179200      activation_240[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_252 (Conv2D)              (None, 9, 9, 160)     179200      activation_245[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_241 (BatchNo (None, 9, 9, 160)     480         conv2d_247[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_246 (BatchNo (None, 9, 9, 160)     480         conv2d_252[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_241 (Activation)      (None, 9, 9, 160)     0           batch_normalization_241[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_246 (Activation)      (None, 9, 9, 160)     0           batch_normalization_246[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_24 (AveragePoo (None, 9, 9, 768)     0           mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_245 (Conv2D)              (None, 9, 9, 192)     147456      mixed5[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_248 (Conv2D)              (None, 9, 9, 192)     215040      activation_241[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_253 (Conv2D)              (None, 9, 9, 192)     215040      activation_246[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_254 (Conv2D)              (None, 9, 9, 192)     147456      average_pooling2d_24[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_239 (BatchNo (None, 9, 9, 192)     576         conv2d_245[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_242 (BatchNo (None, 9, 9, 192)     576         conv2d_248[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_247 (BatchNo (None, 9, 9, 192)     576         conv2d_253[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_248 (BatchNo (None, 9, 9, 192)     576         conv2d_254[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_239 (Activation)      (None, 9, 9, 192)     0           batch_normalization_239[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_242 (Activation)      (None, 9, 9, 192)     0           batch_normalization_242[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_247 (Activation)      (None, 9, 9, 192)     0           batch_normalization_247[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_248 (Activation)      (None, 9, 9, 192)     0           batch_normalization_248[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)             (None, 9, 9, 768)     0           activation_239[0][0]             \n",
      "                                                                   activation_242[0][0]             \n",
      "                                                                   activation_247[0][0]             \n",
      "                                                                   activation_248[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_259 (Conv2D)              (None, 9, 9, 192)     147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_253 (BatchNo (None, 9, 9, 192)     576         conv2d_259[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_253 (Activation)      (None, 9, 9, 192)     0           batch_normalization_253[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_260 (Conv2D)              (None, 9, 9, 192)     258048      activation_253[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_254 (BatchNo (None, 9, 9, 192)     576         conv2d_260[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_254 (Activation)      (None, 9, 9, 192)     0           batch_normalization_254[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_256 (Conv2D)              (None, 9, 9, 192)     147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_261 (Conv2D)              (None, 9, 9, 192)     258048      activation_254[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_250 (BatchNo (None, 9, 9, 192)     576         conv2d_256[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_255 (BatchNo (None, 9, 9, 192)     576         conv2d_261[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_250 (Activation)      (None, 9, 9, 192)     0           batch_normalization_250[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_255 (Activation)      (None, 9, 9, 192)     0           batch_normalization_255[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_257 (Conv2D)              (None, 9, 9, 192)     258048      activation_250[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_262 (Conv2D)              (None, 9, 9, 192)     258048      activation_255[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_251 (BatchNo (None, 9, 9, 192)     576         conv2d_257[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_256 (BatchNo (None, 9, 9, 192)     576         conv2d_262[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_251 (Activation)      (None, 9, 9, 192)     0           batch_normalization_251[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_256 (Activation)      (None, 9, 9, 192)     0           batch_normalization_256[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_25 (AveragePoo (None, 9, 9, 768)     0           mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_255 (Conv2D)              (None, 9, 9, 192)     147456      mixed6[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_258 (Conv2D)              (None, 9, 9, 192)     258048      activation_251[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_263 (Conv2D)              (None, 9, 9, 192)     258048      activation_256[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_264 (Conv2D)              (None, 9, 9, 192)     147456      average_pooling2d_25[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_249 (BatchNo (None, 9, 9, 192)     576         conv2d_255[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_252 (BatchNo (None, 9, 9, 192)     576         conv2d_258[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_257 (BatchNo (None, 9, 9, 192)     576         conv2d_263[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_258 (BatchNo (None, 9, 9, 192)     576         conv2d_264[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_249 (Activation)      (None, 9, 9, 192)     0           batch_normalization_249[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_252 (Activation)      (None, 9, 9, 192)     0           batch_normalization_252[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_257 (Activation)      (None, 9, 9, 192)     0           batch_normalization_257[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_258 (Activation)      (None, 9, 9, 192)     0           batch_normalization_258[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)             (None, 9, 9, 768)     0           activation_249[0][0]             \n",
      "                                                                   activation_252[0][0]             \n",
      "                                                                   activation_257[0][0]             \n",
      "                                                                   activation_258[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_267 (Conv2D)              (None, 9, 9, 192)     147456      mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_261 (BatchNo (None, 9, 9, 192)     576         conv2d_267[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_261 (Activation)      (None, 9, 9, 192)     0           batch_normalization_261[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_268 (Conv2D)              (None, 9, 9, 192)     258048      activation_261[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_262 (BatchNo (None, 9, 9, 192)     576         conv2d_268[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_262 (Activation)      (None, 9, 9, 192)     0           batch_normalization_262[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_265 (Conv2D)              (None, 9, 9, 192)     147456      mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_269 (Conv2D)              (None, 9, 9, 192)     258048      activation_262[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_259 (BatchNo (None, 9, 9, 192)     576         conv2d_265[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_263 (BatchNo (None, 9, 9, 192)     576         conv2d_269[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_259 (Activation)      (None, 9, 9, 192)     0           batch_normalization_259[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_263 (Activation)      (None, 9, 9, 192)     0           batch_normalization_263[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_266 (Conv2D)              (None, 4, 4, 320)     552960      activation_259[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_270 (Conv2D)              (None, 4, 4, 192)     331776      activation_263[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_260 (BatchNo (None, 4, 4, 320)     960         conv2d_266[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_264 (BatchNo (None, 4, 4, 192)     576         conv2d_270[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_260 (Activation)      (None, 4, 4, 320)     0           batch_normalization_260[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_264 (Activation)      (None, 4, 4, 192)     0           batch_normalization_264[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling2D)  (None, 4, 4, 768)     0           mixed7[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)             (None, 4, 4, 1280)    0           activation_260[0][0]             \n",
      "                                                                   activation_264[0][0]             \n",
      "                                                                   max_pooling2d_18[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_275 (Conv2D)              (None, 4, 4, 448)     573440      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_269 (BatchNo (None, 4, 4, 448)     1344        conv2d_275[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_269 (Activation)      (None, 4, 4, 448)     0           batch_normalization_269[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_272 (Conv2D)              (None, 4, 4, 384)     491520      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_276 (Conv2D)              (None, 4, 4, 384)     1548288     activation_269[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_266 (BatchNo (None, 4, 4, 384)     1152        conv2d_272[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_270 (BatchNo (None, 4, 4, 384)     1152        conv2d_276[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_266 (Activation)      (None, 4, 4, 384)     0           batch_normalization_266[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_270 (Activation)      (None, 4, 4, 384)     0           batch_normalization_270[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_273 (Conv2D)              (None, 4, 4, 384)     442368      activation_266[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_274 (Conv2D)              (None, 4, 4, 384)     442368      activation_266[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_277 (Conv2D)              (None, 4, 4, 384)     442368      activation_270[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_278 (Conv2D)              (None, 4, 4, 384)     442368      activation_270[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_26 (AveragePoo (None, 4, 4, 1280)    0           mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_271 (Conv2D)              (None, 4, 4, 320)     409600      mixed8[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_267 (BatchNo (None, 4, 4, 384)     1152        conv2d_273[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_268 (BatchNo (None, 4, 4, 384)     1152        conv2d_274[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_271 (BatchNo (None, 4, 4, 384)     1152        conv2d_277[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_272 (BatchNo (None, 4, 4, 384)     1152        conv2d_278[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_279 (Conv2D)              (None, 4, 4, 192)     245760      average_pooling2d_26[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_265 (BatchNo (None, 4, 4, 320)     960         conv2d_271[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_267 (Activation)      (None, 4, 4, 384)     0           batch_normalization_267[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_268 (Activation)      (None, 4, 4, 384)     0           batch_normalization_268[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_271 (Activation)      (None, 4, 4, 384)     0           batch_normalization_271[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_272 (Activation)      (None, 4, 4, 384)     0           batch_normalization_272[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_273 (BatchNo (None, 4, 4, 192)     576         conv2d_279[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_265 (Activation)      (None, 4, 4, 320)     0           batch_normalization_265[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)           (None, 4, 4, 768)     0           activation_267[0][0]             \n",
      "                                                                   activation_268[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)      (None, 4, 4, 768)     0           activation_271[0][0]             \n",
      "                                                                   activation_272[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_273 (Activation)      (None, 4, 4, 192)     0           batch_normalization_273[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)             (None, 4, 4, 2048)    0           activation_265[0][0]             \n",
      "                                                                   mixed9_0[0][0]                   \n",
      "                                                                   concatenate_5[0][0]              \n",
      "                                                                   activation_273[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_284 (Conv2D)              (None, 4, 4, 448)     917504      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_278 (BatchNo (None, 4, 4, 448)     1344        conv2d_284[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_278 (Activation)      (None, 4, 4, 448)     0           batch_normalization_278[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_281 (Conv2D)              (None, 4, 4, 384)     786432      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_285 (Conv2D)              (None, 4, 4, 384)     1548288     activation_278[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_275 (BatchNo (None, 4, 4, 384)     1152        conv2d_281[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_279 (BatchNo (None, 4, 4, 384)     1152        conv2d_285[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_275 (Activation)      (None, 4, 4, 384)     0           batch_normalization_275[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_279 (Activation)      (None, 4, 4, 384)     0           batch_normalization_279[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_282 (Conv2D)              (None, 4, 4, 384)     442368      activation_275[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_283 (Conv2D)              (None, 4, 4, 384)     442368      activation_275[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_286 (Conv2D)              (None, 4, 4, 384)     442368      activation_279[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_287 (Conv2D)              (None, 4, 4, 384)     442368      activation_279[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "average_pooling2d_27 (AveragePoo (None, 4, 4, 2048)    0           mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_280 (Conv2D)              (None, 4, 4, 320)     655360      mixed9[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_276 (BatchNo (None, 4, 4, 384)     1152        conv2d_282[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_277 (BatchNo (None, 4, 4, 384)     1152        conv2d_283[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_280 (BatchNo (None, 4, 4, 384)     1152        conv2d_286[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_281 (BatchNo (None, 4, 4, 384)     1152        conv2d_287[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_288 (Conv2D)              (None, 4, 4, 192)     393216      average_pooling2d_27[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_274 (BatchNo (None, 4, 4, 320)     960         conv2d_280[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_276 (Activation)      (None, 4, 4, 384)     0           batch_normalization_276[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_277 (Activation)      (None, 4, 4, 384)     0           batch_normalization_277[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_280 (Activation)      (None, 4, 4, 384)     0           batch_normalization_280[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "activation_281 (Activation)      (None, 4, 4, 384)     0           batch_normalization_281[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_282 (BatchNo (None, 4, 4, 192)     576         conv2d_288[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "activation_274 (Activation)      (None, 4, 4, 320)     0           batch_normalization_274[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)           (None, 4, 4, 768)     0           activation_276[0][0]             \n",
      "                                                                   activation_277[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)      (None, 4, 4, 768)     0           activation_280[0][0]             \n",
      "                                                                   activation_281[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "activation_282 (Activation)      (None, 4, 4, 192)     0           batch_normalization_282[0][0]    \n",
      "____________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)            (None, 4, 4, 2048)    0           activation_274[0][0]             \n",
      "                                                                   mixed9_1[0][0]                   \n",
      "                                                                   concatenate_6[0][0]              \n",
      "                                                                   activation_282[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glob (None, 2048)          0           mixed10[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 1024)          2098176     global_average_pooling2d_4[0][0] \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 1024)          0           dense_15[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 5270)          5401750     dropout_10[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 29,302,710\n",
      "Trainable params: 29,268,278\n",
      "Non-trainable params: 34,432\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Add layers to the pre-structured model\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(5270, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 5.5561 - acc: 0.1660Epoch 00000: val_loss improved from inf to 7.81527, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 15282s - loss: 5.5561 - acc: 0.1660 - val_loss: 7.8153 - val_acc: 0.1016\n",
      "Epoch 2/2\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.8690 - acc: 0.2641Epoch 00001: val_loss improved from 7.81527 to 5.86239, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 14880s - loss: 4.8690 - acc: 0.2641 - val_loss: 5.8624 - val_acc: 0.2107\n",
      "Epoch 1/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.4966 - acc: 0.3119Epoch 00000: val_loss improved from 5.86239 to 5.65579, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9611s - loss: 4.4966 - acc: 0.3119 - val_loss: 5.6558 - val_acc: 0.2350\n",
      "Epoch 2/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.2258 - acc: 0.3389Epoch 00001: val_loss improved from 5.65579 to 5.48976, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9505s - loss: 4.2257 - acc: 0.3389 - val_loss: 5.4898 - val_acc: 0.2544\n",
      "Epoch 3/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 4.0726 - acc: 0.3553Epoch 00002: val_loss improved from 5.48976 to 5.32825, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9592s - loss: 4.0726 - acc: 0.3553 - val_loss: 5.3282 - val_acc: 0.2650\n",
      "Epoch 4/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.9554 - acc: 0.3677Epoch 00003: val_loss improved from 5.32825 to 5.24372, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9552s - loss: 3.9554 - acc: 0.3677 - val_loss: 5.2437 - val_acc: 0.2770\n",
      "Epoch 5/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.8609 - acc: 0.3766Epoch 00004: val_loss improved from 5.24372 to 5.16239, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9620s - loss: 3.8609 - acc: 0.3766 - val_loss: 5.1624 - val_acc: 0.2839\n",
      "Epoch 6/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.7759 - acc: 0.3851Epoch 00005: val_loss improved from 5.16239 to 5.16109, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9623s - loss: 3.7759 - acc: 0.3851 - val_loss: 5.1611 - val_acc: 0.2892\n",
      "Epoch 7/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.7021 - acc: 0.3926Epoch 00006: val_loss improved from 5.16109 to 5.15022, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9541s - loss: 3.7020 - acc: 0.3926 - val_loss: 5.1502 - val_acc: 0.2966\n",
      "Epoch 8/8\n",
      "39999/40000 [============================>.] - ETA: 0s - loss: 3.6347 - acc: 0.3989Epoch 00007: val_loss improved from 5.15022 to 5.13357, saving model to weights_bestcatIDGeneratorInception.hdf5\n",
      "40000/40000 [==============================] - 9576s - loss: 3.6347 - acc: 0.3989 - val_loss: 5.1336 - val_acc: 0.2987\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights_bestcatIDGeneratorInception.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "# steps_per_epoch=40000 because --> 800,000 (total samples) / 20 (batch size) = 40000\n",
    "# validation_steps=10000 because --> 200,000 (total samples) / 20 (batch size) = 10000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    m = model.fit_generator(generator = train_generator(), steps_per_epoch=40000, epochs=2, \n",
    "                        validation_data = validation_generator(), validation_steps=10000, \n",
    "                        verbose = 1, callbacks=[checkpointer])\n",
    "    \n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# steps_per_epoch=40000 because --> 800,000 (total samples) / 20 (batch size) = 40000\n",
    "# validation_steps=10000 because --> 200,000 (total samples) / 20 (batch size) = 10000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    m = model.fit_generator(generator = train_generator(), steps_per_epoch=40000, epochs=8, \n",
    "                        validation_data = validation_generator(), validation_steps=10000, \n",
    "                        verbose = 1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Using transfer learning it appeared that the validation accuracy improved by a little over 5 percentage points from experiment 2! I used the code found here https://keras.io/applications/ to implement the inception model with my own dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 6 - Inception model with more data!\n",
    "\n",
    "It has been said many times throughout the Udacity course that the more data, the more accurate the prediction. Since the Inception model provided the most accurate model for predicting the classes of these products, I am going to double the size of my training data from 800,000 to 1,600,000 images. My validation data size will also be doubled from 200,000 to 400,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Contruct the train generator\n",
    "def train_generator():\n",
    "    counter = 1\n",
    "    batch_size = 20\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # Create arrays to contain x_train and y_train. There are fourty of these files in total, so 40000*40 = 1,600,000 items in the entire training set.\n",
    "        # 1,600,000 images for each epoch\n",
    "        h5f = h5py.File('x_train' + str(counter) + 'catID.h5','r')\n",
    "        pic_arr = h5f['dataset'][0:40000]\n",
    "\n",
    "        h5f = h5py.File('y_train' + str(counter) + 'catID.h5','r')\n",
    "        cat_arr = h5f['dataset'][0:40000]\n",
    "        h5f.close()\n",
    "        \n",
    "        # Since training size for each dataset is 40,000 and batch_size is 20, loop 400 times because 40000/20 = 2000 \n",
    "        for i in range(1,2001):\n",
    "            if (i == 1):\n",
    "                x_train = pic_arr[0:batch_size]\n",
    "                y_train = cat_arr[0:batch_size]\n",
    "\n",
    "                index = batch_size\n",
    "                yield x_train, y_train\n",
    "            else:\n",
    "                x_train = pic_arr[index:index + batch_size]\n",
    "                y_train = cat_arr[index:index + batch_size]\n",
    "\n",
    "                index += batch_size\n",
    "                yield x_train, y_train\n",
    "                \n",
    "        del pic_arr\n",
    "        del cat_arr\n",
    "        counter += 1\n",
    "        if (counter == 41):\n",
    "            counter = 1\n",
    "\n",
    "# Construct the validation generator            \n",
    "def validation_generator():\n",
    "    counter = 1\n",
    "    batch_size = 20\n",
    "    \n",
    "    while True:\n",
    "    \n",
    "        # Create arrays to contain x_train and y_train. There are fourty of these files in total, so 10000*40 = 400,000 items in entire validation set.\n",
    "        # 400,000 images for each epoch\n",
    "        h5f = h5py.File('x_train' + str(counter) + 'catID.h5','r')\n",
    "        pic_arr = h5f['dataset'][40000:50000]\n",
    "\n",
    "        h5f = h5py.File('y_train' + str(counter) + 'catID.h5','r')\n",
    "        cat_arr = h5f['dataset'][40000:50000]\n",
    "        h5f.close()\n",
    "        \n",
    "        # Since training size for each dataset is 10,000 and batch_size is 20, loop 500 times because 10000/20 = 500 \n",
    "        for i in range(1,501):\n",
    "            if (i == 1):\n",
    "                x_val = pic_arr[0:batch_size]\n",
    "                y_val = cat_arr[0:batch_size]\n",
    "\n",
    "                index = batch_size\n",
    "                yield x_val, y_val\n",
    "            else:\n",
    "                x_val = pic_arr[index:index + batch_size]\n",
    "                y_val = cat_arr[index:index + batch_size]\n",
    "\n",
    "                index += batch_size\n",
    "                yield x_val, y_val\n",
    "                \n",
    "        del pic_arr\n",
    "        del cat_arr\n",
    "        counter += 1\n",
    "        if (counter == 41):\n",
    "            counter = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import applications\n",
    "\n",
    "# Import the Inception model\n",
    "model = applications.InceptionV3(weights = \"imagenet\", include_top=False, input_shape = (180, 180, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 180, 180, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 89, 89, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 89, 89, 32)   96          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 89, 89, 32)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 87, 87, 32)   9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 87, 87, 32)   96          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 87, 87, 32)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 87, 87, 64)   18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 87, 87, 64)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 87, 87, 64)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 43, 43, 64)   0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 43, 43, 80)   5120        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 43, 43, 80)   240         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 43, 43, 80)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 41, 41, 192)  138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 41, 41, 192)  576         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 41, 41, 192)  0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 20, 20, 192)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 20, 20, 64)   12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 20, 20, 64)   192         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 20, 20, 64)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 20, 20, 48)   9216        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 20, 20, 96)   55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 20, 20, 48)   144         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 20, 20, 96)   288         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 20, 20, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 20, 20, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 20, 20, 192)  0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 20, 20, 64)   12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 20, 20, 64)   76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 20, 20, 96)   82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 20, 20, 32)   6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20, 20, 64)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 20, 20, 64)   192         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 20, 20, 96)   288         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 20, 20, 32)   96          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 20, 20, 64)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 20, 20, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 20, 20, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 20, 20, 32)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 20, 20, 256)  0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 20, 20, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 20, 20, 64)   192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 20, 20, 64)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 20, 20, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 20, 20, 96)   55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 20, 20, 48)   144         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 20, 20, 96)   288         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 20, 20, 48)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 20, 20, 96)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 20, 20, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 20, 20, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 20, 20, 64)   76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 20, 20, 96)   82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 20, 20, 64)   16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 20, 20, 64)   192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 20, 20, 64)   192         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 20, 20, 96)   288         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 20, 20, 64)   192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 20, 20, 64)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 20, 20, 64)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 20, 20, 96)   0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 20, 20, 64)   0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 20, 20, 288)  0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 20, 20, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 20, 20, 64)   192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 20, 20, 64)   0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 20, 20, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 20, 20, 96)   55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 20, 20, 48)   144         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 20, 20, 96)   288         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 20, 20, 48)   0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 20, 20, 96)   0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 20, 20, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 20, 20, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 20, 20, 64)   76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 20, 20, 96)   82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 20, 20, 64)   18432       average_pooling2d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 20, 20, 64)   192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 20, 20, 64)   192         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 20, 20, 96)   288         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 20, 20, 64)   192         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 20, 20, 64)   0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 20, 20, 64)   0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 20, 20, 96)   0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 20, 20, 64)   0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 20, 20, 288)  0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 20, 20, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 20, 20, 64)   192         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 20, 20, 64)   0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 20, 20, 96)   55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 20, 20, 96)   288         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 20, 20, 96)   0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 9, 9, 384)    995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 9, 9, 96)     82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 9, 9, 384)    1152        conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 9, 9, 96)     288         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 9, 9, 384)    0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 9, 9, 96)     0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 9, 9, 288)    0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 9, 9, 768)    0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 9, 9, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 9, 9, 128)    384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 9, 9, 128)    0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 9, 9, 128)    114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 9, 9, 128)    384         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 9, 9, 128)    0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 9, 9, 128)    98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 9, 9, 128)    114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 9, 9, 128)    384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 9, 9, 128)    384         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 9, 9, 128)    0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 9, 9, 128)    0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 9, 9, 128)    114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 9, 9, 128)    114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 9, 9, 128)    384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_38 (BatchNo (None, 9, 9, 128)    384         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 9, 9, 128)    0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 9, 9, 128)    0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 9, 9, 768)    0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 9, 9, 192)    147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 9, 9, 192)    172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 9, 9, 192)    172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 9, 9, 192)    147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 9, 9, 192)    576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 9, 9, 192)    576         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 9, 9, 192)    576         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 9, 9, 192)    576         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 9, 9, 192)    0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 9, 9, 192)    0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 9, 9, 192)    0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 9, 9, 192)    0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 9, 9, 768)    0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 9, 9, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 9, 9, 160)    480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 9, 9, 160)    0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 9, 9, 160)    179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 9, 9, 160)    480         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 9, 9, 160)    0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 9, 9, 160)    122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 9, 9, 160)    179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 9, 9, 160)    480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 9, 9, 160)    480         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 9, 9, 160)    0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 9, 9, 160)    0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 9, 9, 160)    179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 9, 9, 160)    179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 9, 9, 160)    480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 9, 9, 160)    480         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 9, 9, 160)    0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 9, 9, 160)    0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 9, 9, 768)    0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 9, 9, 192)    147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 9, 9, 192)    215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 9, 9, 192)    215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 9, 9, 192)    147456      average_pooling2d_5[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 9, 9, 192)    576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 9, 9, 192)    576         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 9, 9, 192)    576         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 9, 9, 192)    576         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 9, 9, 192)    0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 9, 9, 192)    0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 9, 9, 192)    0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 9, 9, 192)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 9, 9, 768)    0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 9, 9, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 9, 9, 160)    480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 9, 9, 160)    0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 9, 9, 160)    179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 9, 9, 160)    480         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 9, 9, 160)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 9, 9, 160)    122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 9, 9, 160)    179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 9, 9, 160)    480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 9, 9, 160)    480         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 9, 9, 160)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 9, 9, 160)    0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 9, 9, 160)    179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 9, 9, 160)    179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 9, 9, 160)    480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 9, 9, 160)    480         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 9, 9, 160)    0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 9, 9, 160)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 9, 9, 768)    0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 9, 9, 192)    147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 9, 9, 192)    215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 9, 9, 192)    215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 9, 9, 192)    147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 9, 9, 192)    576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 9, 9, 192)    576         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 9, 9, 192)    576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 9, 9, 192)    576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 9, 9, 192)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 9, 9, 192)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 9, 9, 192)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_60 (Activation)      (None, 9, 9, 192)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 9, 9, 768)    0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 9, 9, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 9, 9, 192)    576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 9, 9, 192)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 9, 9, 192)    258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 9, 9, 192)    576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 9, 9, 192)    0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 9, 9, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 9, 9, 192)    258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 9, 9, 192)    576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 9, 9, 192)    576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 9, 9, 192)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 9, 9, 192)    0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 9, 9, 192)    258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 9, 9, 192)    258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 9, 9, 192)    576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 9, 9, 192)    576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 9, 9, 192)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 9, 9, 192)    0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 9, 9, 768)    0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 9, 9, 192)    147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 9, 9, 192)    258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 9, 9, 192)    258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 9, 9, 192)    147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 9, 9, 192)    576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 9, 9, 192)    576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 9, 9, 192)    576         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 9, 9, 192)    576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 9, 9, 192)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 9, 9, 192)    0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 9, 9, 192)    0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 9, 9, 192)    0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 9, 9, 768)    0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 9, 9, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 9, 9, 192)    576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 9, 9, 192)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 9, 9, 192)    258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 9, 9, 192)    576         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 9, 9, 192)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 9, 9, 192)    147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 9, 9, 192)    258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 9, 9, 192)    576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 9, 9, 192)    576         conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 9, 9, 192)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 9, 9, 192)    0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 4, 4, 320)    552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 4, 4, 192)    331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 4, 4, 320)    960         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 4, 4, 192)    576         conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 4, 4, 320)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 4, 4, 192)    0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 4, 4, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 4, 4, 1280)   0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 4, 4, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 4, 4, 448)    1344        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 4, 4, 448)    0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 4, 4, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 4, 4, 384)    1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 4, 4, 384)    1152        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 4, 4, 384)    1152        conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 4, 4, 384)    0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 4, 4, 384)    0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 4, 4, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 4, 4, 384)    442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 4, 4, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 4, 4, 384)    442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 4, 4, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 4, 4, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 4, 4, 384)    1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 4, 4, 384)    1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 4, 4, 384)    1152        conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 4, 4, 384)    1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 4, 4, 192)    245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 4, 4, 320)    960         conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 4, 4, 384)    0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 4, 4, 384)    0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 4, 4, 384)    0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_84 (Activation)      (None, 4, 4, 384)    0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 4, 4, 192)    576         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 4, 4, 320)    0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 4, 4, 768)    0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 4, 4, 768)    0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 4, 4, 192)    0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 4, 4, 2048)   0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 4, 4, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 4, 4, 448)    1344        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 4, 4, 448)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 4, 4, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 4, 4, 384)    1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 4, 4, 384)    1152        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 4, 4, 384)    1152        conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 4, 4, 384)    0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 4, 4, 384)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 4, 4, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 4, 4, 384)    442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 4, 4, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 4, 4, 384)    442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 4, 4, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 4, 4, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 4, 4, 384)    1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 4, 4, 384)    1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 4, 4, 384)    1152        conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 4, 4, 384)    1152        conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 4, 4, 192)    393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 4, 4, 320)    960         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 4, 4, 384)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 4, 4, 384)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 4, 4, 384)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 4, 4, 384)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 4, 4, 192)    576         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 4, 4, 320)    0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 4, 4, 768)    0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 4, 4, 768)    0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 4, 4, 192)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 4, 4, 2048)   0           activation_86[0][0]              \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 1024)         0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 5270)         5401750     dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 29,302,710\n",
      "Trainable params: 29,268,278\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, GlobalAveragePooling2D, Dropout, Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Add layers to the pre-structured model\n",
    "x = model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(5270, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 8.6027 - acc: 3.1813e-04Epoch 00001: val_loss improved from inf to 8.60362, saving model to weights_bestcatIDGeneratorInceptionV2.hdf5\n",
      "80000/80000 [==============================] - 16233s 203ms/step - loss: 8.6027 - acc: 3.1813e-04 - val_loss: 8.6036 - val_acc: 3.0250e-04\n",
      "Epoch 2/2\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 8.6026 - acc: 2.9813e-04Epoch 00002: val_loss did not improve\n",
      "80000/80000 [==============================] - 16099s 201ms/step - loss: 8.6026 - acc: 2.9813e-04 - val_loss: 8.6036 - val_acc: 3.0250e-04\n",
      "Epoch 1/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 3.9095 - acc: 0.3718Epoch 00001: val_loss improved from 8.60362 to 3.59375, saving model to weights_bestcatIDGeneratorInceptionV2.hdf5\n",
      "80000/80000 [==============================] - 19079s 238ms/step - loss: 3.9095 - acc: 0.3718 - val_loss: 3.5938 - val_acc: 0.4178\n",
      "Epoch 2/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 3.2654 - acc: 0.4397Epoch 00002: val_loss improved from 3.59375 to 3.39309, saving model to weights_bestcatIDGeneratorInceptionV2.hdf5\n",
      "80000/80000 [==============================] - 19067s 238ms/step - loss: 3.2654 - acc: 0.4397 - val_loss: 3.3931 - val_acc: 0.4526\n",
      "Epoch 3/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 3.0499 - acc: 0.4639Epoch 00003: val_loss improved from 3.39309 to 3.38398, saving model to weights_bestcatIDGeneratorInceptionV2.hdf5\n",
      "80000/80000 [==============================] - 19113s 239ms/step - loss: 3.0499 - acc: 0.4639 - val_loss: 3.3840 - val_acc: 0.4619\n",
      "Epoch 4/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 2.9087 - acc: 0.4789Epoch 00004: val_loss improved from 3.38398 to 3.38299, saving model to weights_bestcatIDGeneratorInceptionV2.hdf5\n",
      "80000/80000 [==============================] - 19102s 239ms/step - loss: 2.9087 - acc: 0.4789 - val_loss: 3.3830 - val_acc: 0.4671\n",
      "Epoch 5/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 2.8036 - acc: 0.4905Epoch 00005: val_loss did not improve\n",
      "80000/80000 [==============================] - 19097s 239ms/step - loss: 2.8035 - acc: 0.4905 - val_loss: 3.4969 - val_acc: 0.4675\n",
      "Epoch 6/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 2.7208 - acc: 0.4995Epoch 00006: val_loss did not improve\n",
      "80000/80000 [==============================] - 19057s 238ms/step - loss: 2.7208 - acc: 0.4995 - val_loss: 3.4696 - val_acc: 0.4706\n",
      "Epoch 7/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 2.6560 - acc: 0.5071Epoch 00007: val_loss did not improve\n",
      "80000/80000 [==============================] - 19082s 239ms/step - loss: 2.6560 - acc: 0.5071 - val_loss: 3.5260 - val_acc: 0.4707\n",
      "Epoch 8/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 2.6039 - acc: 0.5128Epoch 00008: val_loss did not improve\n",
      "80000/80000 [==============================] - 18999s 237ms/step - loss: 2.6038 - acc: 0.5128 - val_loss: 3.6262 - val_acc: 0.4690\n",
      "Epoch 9/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 2.5606 - acc: 0.5183Epoch 00009: val_loss did not improve\n",
      "80000/80000 [==============================] - 18963s 237ms/step - loss: 2.5605 - acc: 0.5183 - val_loss: 3.6366 - val_acc: 0.4704\n",
      "Epoch 10/10\n",
      "79999/80000 [============================>.] - ETA: 0s - loss: 2.5217 - acc: 0.5239Epoch 00010: val_loss did not improve\n",
      "80000/80000 [==============================] - 19090s 239ms/step - loss: 2.5217 - acc: 0.5239 - val_loss: 3.7100 - val_acc: 0.4699\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights_bestcatIDGeneratorInceptionV2.hdf5', \n",
    "                           verbose=1, save_best_only=True)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "# steps_per_epoch=80000 because --> 1,600,000 (total samples) / 20 (batch size) = 80000\n",
    "# validation_steps=20000 because --> 400,000 (total samples) / 20 (batch size) = 20000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    m = model.fit_generator(generator = train_generator(), steps_per_epoch=80000, epochs=2, \n",
    "                        validation_data = validation_generator(), validation_steps=20000, \n",
    "                        verbose = 1, callbacks=[checkpointer])\n",
    "    \n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=['accuracy'])\n",
    "\n",
    "# steps_per_epoch=80000 because --> 1,600,000 (total samples) / 20 (batch size) = 80000\n",
    "# validation_steps=20000 because --> 400,000 (total samples) / 20 (batch size) = 20000\n",
    "with tf.device('/device:GPU:0'):\n",
    "    m = model.fit_generator(generator = train_generator(), steps_per_epoch=80000, epochs=10, \n",
    "                        validation_data = validation_generator(), validation_steps=20000, \n",
    "                        verbose = 1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "By simply doubling my training set, the validation accuracy increased by around 16% points from 29.87% from the last experiment to 46.71%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outputting predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50,000 images predicted, iteration 1\n",
      "50,000 images predicted, iteration 2\n",
      "50,000 images predicted, iteration 3\n",
      "50,000 images predicted, iteration 4\n",
      "50,000 images predicted, iteration 5\n",
      "50,000 images predicted, iteration 6\n",
      "50,000 images predicted, iteration 7\n",
      "50,000 images predicted, iteration 8\n",
      "50,000 images predicted, iteration 9\n",
      "50,000 images predicted, iteration 10\n",
      "50,000 images predicted, iteration 11\n",
      "50,000 images predicted, iteration 12\n",
      "50,000 images predicted, iteration 13\n",
      "50,000 images predicted, iteration 14\n",
      "50,000 images predicted, iteration 15\n",
      "50,000 images predicted, iteration 16\n",
      "50,000 images predicted, iteration 17\n",
      "50,000 images predicted, iteration 18\n",
      "50,000 images predicted, iteration 19\n",
      "50,000 images predicted, iteration 20\n",
      "50,000 images predicted, iteration 21\n",
      "50,000 images predicted, iteration 22\n",
      "50,000 images predicted, iteration 23\n",
      "50,000 images predicted, iteration 24\n",
      "50,000 images predicted, iteration 25\n",
      "50,000 images predicted, iteration 26\n",
      "50,000 images predicted, iteration 27\n",
      "50,000 images predicted, iteration 28\n",
      "50,000 images predicted, iteration 29\n",
      "50,000 images predicted, iteration 30\n",
      "50,000 images predicted, iteration 31\n",
      "50,000 images predicted, iteration 32\n",
      "50,000 images predicted, iteration 33\n",
      "50,000 images predicted, iteration 34\n",
      "50,000 images predicted, iteration 35\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "## Load in the model weights and structure which obtained highest accuracy \n",
    "model = load_model('weights_bestcatIDGeneratorInceptionV2.hdf5')\n",
    "categories_df = pd.read_csv('category_names.csv')\n",
    "\n",
    "# Declare an empty data frame to load in the dataset\n",
    "temp3_df = pd.DataFrame()\n",
    "for i in range(1,37):\n",
    "    h5f = h5py.File('./test/Product_Imagetest' + str(i) + '.h5','r')\n",
    "    pic_arr = h5f['dataset'][:]\n",
    "    \n",
    "    h5f = h5py.File('./test/Product_IDtest' + str(i) + '.h5','r')\n",
    "    prod_arr = h5f['dataset'][:]\n",
    "    h5f.close()\n",
    "    \n",
    "    if (i < 36):\n",
    "        for k in range(0,50000):\n",
    "            picture = model.predict(np.array(np.reshape(pic_arr[k],(1,180,180,3))/255, dtype='float32'), batch_size=1, verbose=0)\n",
    "            prediction = np.argmax(picture)\n",
    "            prediction = categories_df[categories_df.catID_encode == prediction].iloc[0]['category_id']\n",
    "            prodId = prod_arr[k]\n",
    "\n",
    "            if (k == 0):\n",
    "                temp_df = pd.DataFrame({'_id': [prodId], 'category_id': [prediction]})\n",
    "                temp3_df = temp3_df.append(temp_df)\n",
    "            else:\n",
    "                temp2_df = pd.DataFrame({'_id': [prodId], 'category_id': [prediction]})\n",
    "                temp3_df = temp3_df.append(temp2_df)\n",
    "                if (k == 49999):\n",
    "                    print ('50,000 images predicted, iteration ' + str(i))\n",
    "    \n",
    "    else:\n",
    "        for k in range(0,18182):\n",
    "            picture = model.predict(np.array(np.reshape(pic_arr[k],(1,180,180,3))/255, dtype='float32'), batch_size=1, verbose=0)\n",
    "            prediction = np.argmax(picture)\n",
    "            prediction = categories_df[categories_df.catID_encode == prediction].iloc[0]['category_id']\n",
    "            prodId = prod_arr[k]\n",
    "\n",
    "            if (k == 0):\n",
    "                temp_df = pd.DataFrame({'_id': [prodId], 'category_id': [prediction]})\n",
    "                temp3_df = temp3_df.append(temp_df)\n",
    "            else:\n",
    "                temp2_df = pd.DataFrame({'_id': [prodId], 'category_id': [prediction]})\n",
    "                temp3_df = temp3_df.append(temp2_df)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Writing out predictions to a CSV file. This is the file to submit to \n",
    "## the Kaggle competition\n",
    "temp4_df = temp3_df\n",
    "temp4_df.to_csv('submissionsv2Adam.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
